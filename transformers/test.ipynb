{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "h*dv = model_dim\n",
    "\n",
    "- x : (batch_size, max_length) \n",
    "\n",
    "- tokens_id in x between 0 and vocab_size\n",
    "\n",
    "- Embedd(x) : (batch_size, max_length, model_dim)\n",
    "\n",
    "- K : (model_dim, dk)\n",
    "- Kx : (batch_size, max_length, dk)\n",
    "\n",
    "- Q : (model_dim, dk)\n",
    "- Qx : (batch_size, max_length, dk)\n",
    "\n",
    "- Qx*Kx^T : (batch_size, max_length, max_length)\n",
    "- V : (model_dim, dv)\n",
    "- Vx : (batch_size, max_length, dv)\n",
    "\n",
    "\n",
    "\n",
    "For the mixed attention: (can have â‰  max_lengths for encoder and decoder)\n",
    "\n",
    "- Kx : (batch_size, max_length_encoder, dk)\n",
    "\n",
    "- Qx : (batch_size, max_length_decoder, dk)\n",
    "\n",
    "- Qx*Kx^T : (batch_size, max_length_decoder, max_length_encoder)\n",
    "\n",
    "- Vx : (batch_size, max_length_encoder, dv)\n",
    "\n",
    "- SingleHeadAttention : (batch_size, max_length_decoder, dv)\n",
    "\n",
    "- MultiHeadAttention : (batch_size, max_length_decoder, h*dv)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_length):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.max_length = max_length\n",
    "        self.compute()\n",
    "\n",
    "    def SinPos(self, i: int, pos: int):\n",
    "        if i % 2 == 0:\n",
    "            return np.sin(pos / 10000 ** (2 * i / self.d_model))\n",
    "        else:\n",
    "            return np.cos(pos / 10000 ** (2 * i / self.d_model))\n",
    "\n",
    "    def compute(self):\n",
    "        self.Mat = torch.Tensor([[self.SinPos(i, pos) for i in range(self.d_model)] for pos in range(self.max_length)])\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.Mat[:x.shape[-1], :]\n",
    "\n",
    "\n",
    "class Embedding(nn.Module):\n",
    "    def __init__(self, d_model, max_length, n_embedding, dropout):\n",
    "        super().__init__()\n",
    "        self.max_length = max_length\n",
    "        self.d_model = d_model\n",
    "        self.n_embedding = n_embedding\n",
    "        self.embedding = nn.Embedding(num_embeddings=n_embedding, embedding_dim=d_model)\n",
    "        self.pos_encoding = PositionalEncoding(d_model=d_model, max_length=max_length)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        pass\n",
    "\n",
    "    def forward(self, x):\n",
    "        device = x.device\n",
    "\n",
    "        x = self.embedding(x) + self.pos_encoding(x).to(device)\n",
    "        return self.dropout(x)\n",
    "\n",
    "\n",
    "class SingleHeadAttention(nn.Module):\n",
    "    def __init__(self, dk: int, dv: int, d_model: int):\n",
    "        super().__init__()\n",
    "        self.dk = dk\n",
    "        self.dv = dv\n",
    "        self.d_model = d_model\n",
    "        self.K = nn.Linear(in_features=d_model, out_features=dk)\n",
    "        self.Q = nn.Linear(in_features=d_model, out_features=dk)\n",
    "        self.V = nn.Linear(in_features=d_model, out_features=dv)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, x_encoder: torch.Tensor = None, mask=None):\n",
    "        Kx = self.K(x_encoder) if x_encoder is not None else self.K(x)\n",
    "        Vx = self.V(x_encoder) if x_encoder is not None else self.V(x)\n",
    "        Qx = self.Q(x)\n",
    "        QK = torch.matmul(Qx, Kx.transpose(-2, -1)) / np.sqrt(self.dk)\n",
    "        if mask is not None:\n",
    "            QK = QK + mask\n",
    "        QK = torch.softmax(QK, dim=-1)\n",
    "        return torch.matmul(QK, Vx)\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, num_heads: int, dk: int, dv: int, d_model: int):\n",
    "        super().__init__()\n",
    "        assert num_heads * dv == d_model, \"num_heads * dv should be equal to the model dim\"\n",
    "        self.attention_heads = nn.ModuleList([SingleHeadAttention(dk=dk, dv=dv, d_model=d_model) for _ in range(num_heads)])\n",
    "        self.WO = nn.Linear(in_features=num_heads * dv, out_features=d_model)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, x_encoder: torch.Tensor = None, mask=None):\n",
    "        outputs = [head(x, x_encoder, mask) for head in self.attention_heads]\n",
    "        x = torch.cat(outputs, dim=-1)\n",
    "        x = self.WO(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "class SublayerConnection(nn.Module): #Dropout, Add and Norm\n",
    "    def __init__(self, features, dropout, eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.a_2 = nn.Parameter(torch.ones(features))\n",
    "        self.b_2 = nn.Parameter(torch.zeros(features))\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x, sublayer_output):\n",
    "        x = x + self.dropout(sublayer_output) #dropout and add\n",
    "        mean = x.mean(-1, keepdim=True)\n",
    "        std = x.std(-1, keepdim=True)\n",
    "        x = self.a_2 * (x - mean) / (std + self.eps) + self.b_2 #norm\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, num_heads, dk, dv, d_ff, d_model, dropout):\n",
    "        super().__init__()\n",
    "        self.attention = MultiHeadAttention(num_heads=num_heads, dk=dk, dv=dv, d_model=d_model)\n",
    "        self.sublayer1 = SublayerConnection(features=d_model, dropout=dropout)\n",
    "        self.sublayer2 = SublayerConnection(features=d_model, dropout=dropout)\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(in_features=d_model, out_features=d_ff),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(in_features=d_ff, out_features=d_model),\n",
    "        )\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        attention = self.attention(x=x, mask=mask)\n",
    "        x = self.sublayer1(x, attention)\n",
    "        feedforward = self.ff(x)\n",
    "        x = self.sublayer2(x, feedforward)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, num_heads, dk, dv, d_ff, d_model, dropout, num_encoders):\n",
    "        super().__init__()\n",
    "        self.encoders_list = [EncoderBlock(num_heads=num_heads, dk=dk, dv=dv, d_ff=d_ff, d_model=d_model, dropout=dropout) for _ in range(num_encoders)]\n",
    "        self.encoders = nn.ModuleList(self.encoders_list)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        for encoder in self.encoders_list:\n",
    "            x = encoder(x, mask)\n",
    "        return x\n",
    "    \n",
    "\n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, num_heads, dk, dv, d_ff, d_model, dropout):\n",
    "        super().__init__()\n",
    "        self.masked_attention = MultiHeadAttention(num_heads=num_heads, dk=dk, dv=dv, d_model=d_model)\n",
    "        self.mixed_attention = MultiHeadAttention(num_heads=num_heads, dk=dk, dv=dv, d_model=d_model)\n",
    "        self.sublayer1 = SublayerConnection(features=d_model, dropout=dropout)\n",
    "        self.sublayer2 = SublayerConnection(features=d_model, dropout=dropout)\n",
    "        self.sublayer3 = SublayerConnection(features=d_model, dropout=dropout)\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(in_features=d_model, out_features=d_ff),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(in_features=d_ff, out_features=d_model),\n",
    "        )\n",
    "\n",
    "    def forward(self, x, x_encoder, causal_mask=None, mixed_mask=None):\n",
    "        attention = self.masked_attention(x, mask=causal_mask)\n",
    "        x = self.sublayer1(x, attention)\n",
    "        attention = self.mixed_attention(x, x_encoder=x_encoder, mask=mixed_mask)\n",
    "        x = self.sublayer2(x, attention)\n",
    "        feedforward = self.ff(x)\n",
    "        x = self.sublayer3(x, feedforward)\n",
    "        return x\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, num_heads, dk, dv, d_ff, d_model, dropout, num_decoders):\n",
    "        super().__init__()\n",
    "        decoders_list = [DecoderBlock(num_heads=num_heads, dk=dk, dv=dv, d_ff=d_ff, d_model=d_model, dropout=dropout) for _ in range(num_decoders)]\n",
    "        self.decoders = nn.ModuleList(decoders_list)\n",
    "\n",
    "    def forward(self, x, x_encoder, causal_mask=None, mixed_mask=None):\n",
    "        for decoder in self.decoders:\n",
    "            x = decoder(x, x_encoder, causal_mask, mixed_mask)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, d_model, max_length_encoder, vocab_size_encoder, max_length_decoder, vocab_size_decoder, num_out, num_heads, dv, dk, d_ff, dropout, num_encoders, num_decoders):\n",
    "        super().__init__()\n",
    "        self.embedding_encoder = Embedding(d_model, max_length_encoder, n_embedding=vocab_size_encoder, dropout=dropout)\n",
    "        self.embedding_decoder = Embedding(d_model, max_length_decoder, n_embedding=vocab_size_decoder, dropout=dropout)\n",
    "        self.encoder = Encoder(num_heads, dk, dv, d_ff, d_model, dropout, num_encoders)\n",
    "        self.decoder = Decoder(num_heads, dk, dv, d_ff, d_model, dropout, num_decoders)\n",
    "        self.linear = nn.Linear(in_features=d_model, out_features=num_out)\n",
    "        self.ff_mask = torch.zeros(max_length_decoder, max_length_decoder) + torch.triu(torch.full((max_length_decoder, max_length_decoder), float(-1e9)), diagonal=1)\n",
    "        self.max_length_encoder = max_length_encoder\n",
    "        self.max_length_decoder = max_length_decoder\n",
    "\n",
    "    def forward(self, input, output, padding_mask_encoder, padding_mask_decoder):\n",
    "        device = input.device\n",
    "\n",
    "        padding_mask_decoder[padding_mask_decoder == 1] = -1e9\n",
    "        padding_mask_decoder[padding_mask_decoder == 0] = 0.0\n",
    "\n",
    "        padding_mask_encoder[padding_mask_encoder == 1] = -1e9\n",
    "        padding_mask_encoder[padding_mask_encoder == 0] = 0.0\n",
    "\n",
    "        encoder_mask = padding_mask_encoder.unsqueeze(1).expand(-1, self.max_length_encoder, -1)\n",
    "\n",
    "        decoder_mask = self.ff_mask.to(device) + padding_mask_decoder.unsqueeze(1).expand(-1, self.max_length_decoder, -1) #Causal mask\n",
    "        mixed_mask = padding_mask_decoder.unsqueeze(1).expand(-1, self.max_length_encoder, -1).transpose(-1, -2) + padding_mask_encoder.unsqueeze(1).expand(-1, self.max_length_decoder,-1)\n",
    "        # In reality we don't care about the padded rows of our attention matrix, at the end the loss won't take them into account and we won't update the weights during the backward pass.\n",
    "        input_embed = self.embedding_encoder(input)\n",
    "        output_embed = self.embedding_decoder(output)\n",
    "\n",
    "        x_encoder = self.encoder(input_embed, mask=encoder_mask)\n",
    "        x = self.decoder(output_embed, x_encoder, causal_mask=decoder_mask, mixed_mask=mixed_mask)\n",
    "        x = self.linear(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "    def generate(self, input, max_gen_length, start_token, end_token, padding_mask_encoder): #greed decoding\n",
    "        self.eval()\n",
    "        input_embed = self.embedding_encoder(input)\n",
    "        x_encoder = self.encoder(input_embed, mask=padding_mask_encoder)\n",
    "\n",
    "        generated_tokens = [start_token]\n",
    "        generated_tokens_probas = [1]\n",
    "\n",
    "        for _ in range(max_gen_length):\n",
    "            output = torch.tensor(generated_tokens).unsqueeze(0) # size [1, sequence_length]\n",
    "            out_embed = self.embedding_decoder(output)\n",
    "            causal_mask = self.ff_mask[:out_embed.size(1), :out_embed.size(1)]\n",
    "            x = self.decoder(out_embed, x_encoder, causal_mask=causal_mask)\n",
    "            x = self.linear(x)\n",
    "            probas = torch.softmax(x, dim=-1)\n",
    "            max_proba, next_token = torch.max(probas[:, -1, :], dim=-1) #greedy decoding : only max_proba\n",
    "            generated_tokens.append(next_token.item())\n",
    "            generated_tokens_probas.append(max_proba.item())\n",
    "            if next_token == end_token:\n",
    "                break\n",
    "            \n",
    "        return generated_tokens, generated_tokens_probas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parameters\n",
    "batch_size = 2\n",
    "d_model = 128\n",
    "max_length_encoder = 100\n",
    "max_length_decoder = 80\n",
    "vocab_size_encoder = 10000\n",
    "vocab_size_decoder = 25000\n",
    "num_out = vocab_size_decoder\n",
    "num_heads = 8\n",
    "dv = 16\n",
    "dk = 16\n",
    "d_ff = 512\n",
    "dropout = 0.1\n",
    "num_encoders = 2\n",
    "num_decoders = 2\n",
    "dropout = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 100])\n",
      "torch.Size([2, 100, 128])\n",
      "torch.Size([2, 100, 128])\n",
      "torch.Size([2, 80, 128])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randint(0, vocab_size_encoder, (batch_size, max_length_encoder))\n",
    "print(x.shape)\n",
    "MyEmbeddingEncoder = Embedding(d_model, max_length_encoder, n_embedding=vocab_size_encoder, dropout=dropout)\n",
    "x= MyEmbeddingEncoder(x)\n",
    "print(x.shape)\n",
    "\n",
    "MyEncoder = Encoder(num_heads, dk, dv, d_ff, d_model, dropout, num_encoders)\n",
    "x_encoder = MyEncoder(x)\n",
    "print(x.shape)\n",
    "\n",
    "y = torch.randint(0, vocab_size_decoder, (batch_size, max_length_decoder))\n",
    "MyEmbeddingDecoder = Embedding(d_model, max_length_decoder, n_embedding=vocab_size_decoder, dropout=dropout)\n",
    "y = MyEmbeddingDecoder(y)\n",
    "\n",
    "MyDecoder = Decoder(num_heads, dk, dv, d_ff, d_model, dropout, num_decoders)\n",
    "y = MyDecoder(y, x_encoder) \n",
    "print(y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[7, 9, 2, 2, 7, 5, 2, 3, 4, 5, 6, 4, 3, 0, 0, 0],\n",
      "        [6, 8, 1, 2, 3, 4, 9, 8, 1, 2, 4, 4, 0, 0, 0, 0]])\n",
      "tensor([[14,  5, 11,  3,  3,  9,  4,  2, 12,  4,  5,  7,  0,  0],\n",
      "        [ 5,  8, 10,  8,  6,  7, 11, 14, 10,  0,  0,  0,  0,  0]])\n",
      "torch.Size([2, 16])\n",
      "torch.Size([2, 14])\n",
      "tensor([[[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00,  0.0000e+00, -1.0000e+09, -1.0000e+09,\n",
      "          -1.0000e+09],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00,  0.0000e+00, -1.0000e+09, -1.0000e+09,\n",
      "          -1.0000e+09],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00,  0.0000e+00, -1.0000e+09, -1.0000e+09,\n",
      "          -1.0000e+09],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00,  0.0000e+00, -1.0000e+09, -1.0000e+09,\n",
      "          -1.0000e+09],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00,  0.0000e+00, -1.0000e+09, -1.0000e+09,\n",
      "          -1.0000e+09],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00,  0.0000e+00, -1.0000e+09, -1.0000e+09,\n",
      "          -1.0000e+09],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00,  0.0000e+00, -1.0000e+09, -1.0000e+09,\n",
      "          -1.0000e+09],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00,  0.0000e+00, -1.0000e+09, -1.0000e+09,\n",
      "          -1.0000e+09],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00,  0.0000e+00, -1.0000e+09, -1.0000e+09,\n",
      "          -1.0000e+09],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00,  0.0000e+00, -1.0000e+09, -1.0000e+09,\n",
      "          -1.0000e+09],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00,  0.0000e+00, -1.0000e+09, -1.0000e+09,\n",
      "          -1.0000e+09],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00,  0.0000e+00, -1.0000e+09, -1.0000e+09,\n",
      "          -1.0000e+09],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00,  0.0000e+00, -1.0000e+09, -1.0000e+09,\n",
      "          -1.0000e+09],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00,  0.0000e+00, -1.0000e+09, -1.0000e+09,\n",
      "          -1.0000e+09],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00,  0.0000e+00, -1.0000e+09, -1.0000e+09,\n",
      "          -1.0000e+09],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00,  0.0000e+00, -1.0000e+09, -1.0000e+09,\n",
      "          -1.0000e+09]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00, -1.0000e+09, -1.0000e+09, -1.0000e+09,\n",
      "          -1.0000e+09],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00, -1.0000e+09, -1.0000e+09, -1.0000e+09,\n",
      "          -1.0000e+09],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00, -1.0000e+09, -1.0000e+09, -1.0000e+09,\n",
      "          -1.0000e+09],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00, -1.0000e+09, -1.0000e+09, -1.0000e+09,\n",
      "          -1.0000e+09],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00, -1.0000e+09, -1.0000e+09, -1.0000e+09,\n",
      "          -1.0000e+09],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00, -1.0000e+09, -1.0000e+09, -1.0000e+09,\n",
      "          -1.0000e+09],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00, -1.0000e+09, -1.0000e+09, -1.0000e+09,\n",
      "          -1.0000e+09],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00, -1.0000e+09, -1.0000e+09, -1.0000e+09,\n",
      "          -1.0000e+09],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00, -1.0000e+09, -1.0000e+09, -1.0000e+09,\n",
      "          -1.0000e+09],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00, -1.0000e+09, -1.0000e+09, -1.0000e+09,\n",
      "          -1.0000e+09],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00, -1.0000e+09, -1.0000e+09, -1.0000e+09,\n",
      "          -1.0000e+09],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00, -1.0000e+09, -1.0000e+09, -1.0000e+09,\n",
      "          -1.0000e+09],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00, -1.0000e+09, -1.0000e+09, -1.0000e+09,\n",
      "          -1.0000e+09],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00, -1.0000e+09, -1.0000e+09, -1.0000e+09,\n",
      "          -1.0000e+09],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00, -1.0000e+09, -1.0000e+09, -1.0000e+09,\n",
      "          -1.0000e+09],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00, -1.0000e+09, -1.0000e+09, -1.0000e+09,\n",
      "          -1.0000e+09]]])\n",
      "tensor([[[ 0.0000e+00, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09,\n",
      "          -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09,\n",
      "          -1.0000e+09, -1.0000e+09, -2.0000e+09, -2.0000e+09],\n",
      "         [ 0.0000e+00,  0.0000e+00, -1.0000e+09, -1.0000e+09, -1.0000e+09,\n",
      "          -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09,\n",
      "          -1.0000e+09, -1.0000e+09, -2.0000e+09, -2.0000e+09],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00, -1.0000e+09, -1.0000e+09,\n",
      "          -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09,\n",
      "          -1.0000e+09, -1.0000e+09, -2.0000e+09, -2.0000e+09],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00, -1.0000e+09,\n",
      "          -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09,\n",
      "          -1.0000e+09, -1.0000e+09, -2.0000e+09, -2.0000e+09],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09,\n",
      "          -1.0000e+09, -1.0000e+09, -2.0000e+09, -2.0000e+09],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "           0.0000e+00, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09,\n",
      "          -1.0000e+09, -1.0000e+09, -2.0000e+09, -2.0000e+09],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00, -1.0000e+09, -1.0000e+09, -1.0000e+09,\n",
      "          -1.0000e+09, -1.0000e+09, -2.0000e+09, -2.0000e+09],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00,  0.0000e+00, -1.0000e+09, -1.0000e+09,\n",
      "          -1.0000e+09, -1.0000e+09, -2.0000e+09, -2.0000e+09],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00, -1.0000e+09,\n",
      "          -1.0000e+09, -1.0000e+09, -2.0000e+09, -2.0000e+09],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          -1.0000e+09, -1.0000e+09, -2.0000e+09, -2.0000e+09],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "           0.0000e+00, -1.0000e+09, -2.0000e+09, -2.0000e+09],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00, -2.0000e+09, -2.0000e+09],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00, -1.0000e+09, -2.0000e+09],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00, -1.0000e+09, -1.0000e+09]],\n",
      "\n",
      "        [[ 0.0000e+00, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09,\n",
      "          -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -2.0000e+09,\n",
      "          -2.0000e+09, -2.0000e+09, -2.0000e+09, -2.0000e+09],\n",
      "         [ 0.0000e+00,  0.0000e+00, -1.0000e+09, -1.0000e+09, -1.0000e+09,\n",
      "          -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -2.0000e+09,\n",
      "          -2.0000e+09, -2.0000e+09, -2.0000e+09, -2.0000e+09],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00, -1.0000e+09, -1.0000e+09,\n",
      "          -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -2.0000e+09,\n",
      "          -2.0000e+09, -2.0000e+09, -2.0000e+09, -2.0000e+09],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00, -1.0000e+09,\n",
      "          -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -2.0000e+09,\n",
      "          -2.0000e+09, -2.0000e+09, -2.0000e+09, -2.0000e+09],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -2.0000e+09,\n",
      "          -2.0000e+09, -2.0000e+09, -2.0000e+09, -2.0000e+09],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "           0.0000e+00, -1.0000e+09, -1.0000e+09, -1.0000e+09, -2.0000e+09,\n",
      "          -2.0000e+09, -2.0000e+09, -2.0000e+09, -2.0000e+09],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00, -1.0000e+09, -1.0000e+09, -2.0000e+09,\n",
      "          -2.0000e+09, -2.0000e+09, -2.0000e+09, -2.0000e+09],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00,  0.0000e+00, -1.0000e+09, -2.0000e+09,\n",
      "          -2.0000e+09, -2.0000e+09, -2.0000e+09, -2.0000e+09],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00, -2.0000e+09,\n",
      "          -2.0000e+09, -2.0000e+09, -2.0000e+09, -2.0000e+09],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00, -1.0000e+09,\n",
      "          -2.0000e+09, -2.0000e+09, -2.0000e+09, -2.0000e+09],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00, -1.0000e+09,\n",
      "          -1.0000e+09, -2.0000e+09, -2.0000e+09, -2.0000e+09],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00, -1.0000e+09,\n",
      "          -1.0000e+09, -1.0000e+09, -2.0000e+09, -2.0000e+09],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00, -1.0000e+09,\n",
      "          -1.0000e+09, -1.0000e+09, -1.0000e+09, -2.0000e+09],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00, -1.0000e+09,\n",
      "          -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09]]])\n",
      "tensor([[[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00,  0.0000e+00, -1.0000e+09, -1.0000e+09,\n",
      "          -1.0000e+09],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00,  0.0000e+00, -1.0000e+09, -1.0000e+09,\n",
      "          -1.0000e+09],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00,  0.0000e+00, -1.0000e+09, -1.0000e+09,\n",
      "          -1.0000e+09],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00,  0.0000e+00, -1.0000e+09, -1.0000e+09,\n",
      "          -1.0000e+09],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00,  0.0000e+00, -1.0000e+09, -1.0000e+09,\n",
      "          -1.0000e+09],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00,  0.0000e+00, -1.0000e+09, -1.0000e+09,\n",
      "          -1.0000e+09],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00,  0.0000e+00, -1.0000e+09, -1.0000e+09,\n",
      "          -1.0000e+09],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00,  0.0000e+00, -1.0000e+09, -1.0000e+09,\n",
      "          -1.0000e+09],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00,  0.0000e+00, -1.0000e+09, -1.0000e+09,\n",
      "          -1.0000e+09],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00,  0.0000e+00, -1.0000e+09, -1.0000e+09,\n",
      "          -1.0000e+09],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00,  0.0000e+00, -1.0000e+09, -1.0000e+09,\n",
      "          -1.0000e+09],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00,  0.0000e+00, -1.0000e+09, -1.0000e+09,\n",
      "          -1.0000e+09],\n",
      "         [-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09,\n",
      "          -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09,\n",
      "          -1.0000e+09, -1.0000e+09, -1.0000e+09, -2.0000e+09, -2.0000e+09,\n",
      "          -2.0000e+09],\n",
      "         [-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09,\n",
      "          -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09,\n",
      "          -1.0000e+09, -1.0000e+09, -1.0000e+09, -2.0000e+09, -2.0000e+09,\n",
      "          -2.0000e+09]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00, -1.0000e+09, -1.0000e+09, -1.0000e+09,\n",
      "          -1.0000e+09],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00, -1.0000e+09, -1.0000e+09, -1.0000e+09,\n",
      "          -1.0000e+09],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00, -1.0000e+09, -1.0000e+09, -1.0000e+09,\n",
      "          -1.0000e+09],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00, -1.0000e+09, -1.0000e+09, -1.0000e+09,\n",
      "          -1.0000e+09],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00, -1.0000e+09, -1.0000e+09, -1.0000e+09,\n",
      "          -1.0000e+09],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00, -1.0000e+09, -1.0000e+09, -1.0000e+09,\n",
      "          -1.0000e+09],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00, -1.0000e+09, -1.0000e+09, -1.0000e+09,\n",
      "          -1.0000e+09],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00, -1.0000e+09, -1.0000e+09, -1.0000e+09,\n",
      "          -1.0000e+09],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00, -1.0000e+09, -1.0000e+09, -1.0000e+09,\n",
      "          -1.0000e+09],\n",
      "         [-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09,\n",
      "          -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09,\n",
      "          -1.0000e+09, -1.0000e+09, -2.0000e+09, -2.0000e+09, -2.0000e+09,\n",
      "          -2.0000e+09],\n",
      "         [-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09,\n",
      "          -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09,\n",
      "          -1.0000e+09, -1.0000e+09, -2.0000e+09, -2.0000e+09, -2.0000e+09,\n",
      "          -2.0000e+09],\n",
      "         [-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09,\n",
      "          -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09,\n",
      "          -1.0000e+09, -1.0000e+09, -2.0000e+09, -2.0000e+09, -2.0000e+09,\n",
      "          -2.0000e+09],\n",
      "         [-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09,\n",
      "          -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09,\n",
      "          -1.0000e+09, -1.0000e+09, -2.0000e+09, -2.0000e+09, -2.0000e+09,\n",
      "          -2.0000e+09],\n",
      "         [-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09,\n",
      "          -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09,\n",
      "          -1.0000e+09, -1.0000e+09, -2.0000e+09, -2.0000e+09, -2.0000e+09,\n",
      "          -2.0000e+09]]])\n",
      "tensor([[[0.0769, 0.0769, 0.0769, 0.0769, 0.0769, 0.0769, 0.0769, 0.0769,\n",
      "          0.0769, 0.0769, 0.0769, 0.0769, 0.0769, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0769, 0.0769, 0.0769, 0.0769, 0.0769, 0.0769, 0.0769, 0.0769,\n",
      "          0.0769, 0.0769, 0.0769, 0.0769, 0.0769, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0769, 0.0769, 0.0769, 0.0769, 0.0769, 0.0769, 0.0769, 0.0769,\n",
      "          0.0769, 0.0769, 0.0769, 0.0769, 0.0769, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0769, 0.0769, 0.0769, 0.0769, 0.0769, 0.0769, 0.0769, 0.0769,\n",
      "          0.0769, 0.0769, 0.0769, 0.0769, 0.0769, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0769, 0.0769, 0.0769, 0.0769, 0.0769, 0.0769, 0.0769, 0.0769,\n",
      "          0.0769, 0.0769, 0.0769, 0.0769, 0.0769, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0769, 0.0769, 0.0769, 0.0769, 0.0769, 0.0769, 0.0769, 0.0769,\n",
      "          0.0769, 0.0769, 0.0769, 0.0769, 0.0769, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0769, 0.0769, 0.0769, 0.0769, 0.0769, 0.0769, 0.0769, 0.0769,\n",
      "          0.0769, 0.0769, 0.0769, 0.0769, 0.0769, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0769, 0.0769, 0.0769, 0.0769, 0.0769, 0.0769, 0.0769, 0.0769,\n",
      "          0.0769, 0.0769, 0.0769, 0.0769, 0.0769, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0769, 0.0769, 0.0769, 0.0769, 0.0769, 0.0769, 0.0769, 0.0769,\n",
      "          0.0769, 0.0769, 0.0769, 0.0769, 0.0769, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0769, 0.0769, 0.0769, 0.0769, 0.0769, 0.0769, 0.0769, 0.0769,\n",
      "          0.0769, 0.0769, 0.0769, 0.0769, 0.0769, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0769, 0.0769, 0.0769, 0.0769, 0.0769, 0.0769, 0.0769, 0.0769,\n",
      "          0.0769, 0.0769, 0.0769, 0.0769, 0.0769, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0769, 0.0769, 0.0769, 0.0769, 0.0769, 0.0769, 0.0769, 0.0769,\n",
      "          0.0769, 0.0769, 0.0769, 0.0769, 0.0769, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0769, 0.0769, 0.0769, 0.0769, 0.0769, 0.0769, 0.0769, 0.0769,\n",
      "          0.0769, 0.0769, 0.0769, 0.0769, 0.0769, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0769, 0.0769, 0.0769, 0.0769, 0.0769, 0.0769, 0.0769, 0.0769,\n",
      "          0.0769, 0.0769, 0.0769, 0.0769, 0.0769, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0833, 0.0833, 0.0833, 0.0833, 0.0833, 0.0833, 0.0833, 0.0833,\n",
      "          0.0833, 0.0833, 0.0833, 0.0833, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0833, 0.0833, 0.0833, 0.0833, 0.0833, 0.0833, 0.0833, 0.0833,\n",
      "          0.0833, 0.0833, 0.0833, 0.0833, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0833, 0.0833, 0.0833, 0.0833, 0.0833, 0.0833, 0.0833, 0.0833,\n",
      "          0.0833, 0.0833, 0.0833, 0.0833, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0833, 0.0833, 0.0833, 0.0833, 0.0833, 0.0833, 0.0833, 0.0833,\n",
      "          0.0833, 0.0833, 0.0833, 0.0833, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0833, 0.0833, 0.0833, 0.0833, 0.0833, 0.0833, 0.0833, 0.0833,\n",
      "          0.0833, 0.0833, 0.0833, 0.0833, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0833, 0.0833, 0.0833, 0.0833, 0.0833, 0.0833, 0.0833, 0.0833,\n",
      "          0.0833, 0.0833, 0.0833, 0.0833, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0833, 0.0833, 0.0833, 0.0833, 0.0833, 0.0833, 0.0833, 0.0833,\n",
      "          0.0833, 0.0833, 0.0833, 0.0833, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0833, 0.0833, 0.0833, 0.0833, 0.0833, 0.0833, 0.0833, 0.0833,\n",
      "          0.0833, 0.0833, 0.0833, 0.0833, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0833, 0.0833, 0.0833, 0.0833, 0.0833, 0.0833, 0.0833, 0.0833,\n",
      "          0.0833, 0.0833, 0.0833, 0.0833, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0833, 0.0833, 0.0833, 0.0833, 0.0833, 0.0833, 0.0833, 0.0833,\n",
      "          0.0833, 0.0833, 0.0833, 0.0833, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0833, 0.0833, 0.0833, 0.0833, 0.0833, 0.0833, 0.0833, 0.0833,\n",
      "          0.0833, 0.0833, 0.0833, 0.0833, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0833, 0.0833, 0.0833, 0.0833, 0.0833, 0.0833, 0.0833, 0.0833,\n",
      "          0.0833, 0.0833, 0.0833, 0.0833, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0833, 0.0833, 0.0833, 0.0833, 0.0833, 0.0833, 0.0833, 0.0833,\n",
      "          0.0833, 0.0833, 0.0833, 0.0833, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0833, 0.0833, 0.0833, 0.0833, 0.0833, 0.0833, 0.0833, 0.0833,\n",
      "          0.0833, 0.0833, 0.0833, 0.0833, 0.0000, 0.0000, 0.0000, 0.0000]]])\n",
      "tensor([[[-0.4271,  0.0358, -0.2896,  ..., -1.0270, -0.0972,  0.2962],\n",
      "         [-0.2056,  0.0295,  0.4405,  ...,  0.2386, -0.0748,  0.2969],\n",
      "         [-0.0308, -0.0469, -0.4178,  ..., -0.9780, -0.6695, -0.7357],\n",
      "         ...,\n",
      "         [-0.2515,  0.0048, -0.1626,  ..., -0.8812,  0.0604, -0.4896],\n",
      "         [-0.4270, -0.4830,  0.0916,  ..., -1.0462,  0.7276,  0.1032],\n",
      "         [-0.3602, -0.4774, -0.1392,  ..., -0.9218,  0.6088,  0.2303]],\n",
      "\n",
      "        [[-0.0778, -0.7210,  0.8196,  ...,  0.0352,  0.1361, -0.1825],\n",
      "         [-0.6159, -0.4020,  0.1391,  ..., -1.3447, -0.4275,  0.2709],\n",
      "         [ 0.0637, -0.3440, -0.5783,  ...,  0.0037, -0.2558, -1.3703],\n",
      "         ...,\n",
      "         [-0.3268, -0.2993,  0.2205,  ..., -0.9895,  0.4909, -0.0955],\n",
      "         [-0.0384, -0.7090,  0.0996,  ..., -1.0322,  0.4502, -0.5848],\n",
      "         [-0.1884, -0.6979,  0.0871,  ..., -1.1550,  0.1978, -0.2376]]],\n",
      "       grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "def init_transformer():\n",
    "    model = Transformer(\n",
    "        d_model=d_model,\n",
    "        max_length_encoder=16,\n",
    "        max_length_decoder=14,\n",
    "        vocab_size_encoder=10,\n",
    "        vocab_size_decoder=20,\n",
    "        num_out=num_out,\n",
    "        num_heads=num_heads,\n",
    "        dv=dv,\n",
    "        dk=dk,\n",
    "        d_ff=d_ff,\n",
    "        dropout=dropout,\n",
    "        num_encoders=num_encoders,\n",
    "        num_decoders=num_decoders,\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "#######  Forward Test  #############\n",
    "input = torch.tensor([[7, 9, 2, 2, 7, 5, 2, 3, 4, 5, 6, 4, 3, 0, 0, 0],\n",
    "        [6, 8, 1, 2, 3, 4, 9, 8, 1, 2, 4, 4, 0, 0, 0, 0]])\n",
    "output = torch.tensor([[14,  5, 11,  3,  3,  9,  4,  2, 12,  4,  5,  7, 0,  0],\n",
    "        [ 5,  8, 10,  8,  6,  7, 11, 14, 10,  0,  0,  0,  0,  0]])\n",
    "print(input)\n",
    "print(output)\n",
    "padding_mask_encoder = (input == 0)\n",
    "padding_mask_encoder = padding_mask_encoder.to(torch.float32)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "padding_mask_decoder = (output == 0)\n",
    "padding_mask_decoder = padding_mask_decoder.to(torch.float32)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(padding_mask_encoder.shape)\n",
    "print(padding_mask_decoder.shape)\n",
    "\n",
    "\n",
    "MyTransformer = init_transformer()\n",
    "out = MyTransformer(input, output, padding_mask_encoder, padding_mask_decoder)\n",
    "\n",
    "print(out)\n",
    "# ####### Generation Test ######\n",
    "# input_seq = torch.tensor([[0, 2, 3, 4, 5, 1]])\n",
    "# input_seq_mask = torch.tensor([[0, 0, 0, 0, -torch.inf, -torch.inf]])\n",
    "# start_token = 0 \n",
    "# end_token = 1\n",
    "\n",
    "# generated_seq, corresponding_probas = MyTransformer.generate(input_seq, max_gen_length=10, start_token=start_token, end_token=end_token, padding_mask_encoder=input_seq_mask)\n",
    "# print(generated_seq)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Character Level Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "\n",
    "class MyTokenizer:\n",
    "    def __init__(self, text):\n",
    "        self.bos_token = \"<\"\n",
    "        self.eos_token = \">\"\n",
    "        self.pad_token = \"_\"\n",
    "        self.chars = sorted(list(set(text)))\n",
    "        self.chars = [self.eos_token, self.bos_token, self.pad_token] + self.chars\n",
    "        self.char_to_idx = {char: idx for idx, char in enumerate(self.chars)}\n",
    "        self.idx_to_char = {idx: char for idx, char in enumerate(self.chars)}\n",
    "        self.bos_token_id = self.char_to_idx[self.bos_token]\n",
    "        self.eos_token_id = self.char_to_idx[self.eos_token]\n",
    "        self.pad_token_id = self.char_to_idx[self.pad_token]\n",
    "\n",
    "    def encode(self, text):\n",
    "        return [self.char_to_idx[char] for char in text]\n",
    "\n",
    "    def decode(self, indices):\n",
    "        return ''.join([self.idx_to_char[idx] for idx in indices if idx not in {self.bos_token_id, self.eos_token_id, self.pad_token_id}])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can only concatenate str (not \"list\") to str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 37\u001b[0m\n\u001b[1;32m     35\u001b[0m max_length \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m20\u001b[39m\n\u001b[1;32m     36\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m MyTokenizer(text)\n\u001b[0;32m---> 37\u001b[0m dataset \u001b[38;5;241m=\u001b[39m TextDataset(text, tokenizer, max_length\u001b[38;5;241m=\u001b[39mmax_length)\n\u001b[1;32m     38\u001b[0m dataloader \u001b[38;5;241m=\u001b[39m DataLoader(dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# Print some examples\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[7], line 9\u001b[0m, in \u001b[0;36mTextDataset.__init__\u001b[0;34m(self, text, tokenizer, max_length)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer \u001b[38;5;241m=\u001b[39m tokenizer\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_length \u001b[38;5;241m=\u001b[39m max_length\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocess(text)\n",
      "Cell \u001b[0;32mIn[7], line 16\u001b[0m, in \u001b[0;36mTextDataset.preprocess\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m     14\u001b[0m input_seq \u001b[38;5;241m=\u001b[39m text[:i]\n\u001b[1;32m     15\u001b[0m target_seq \u001b[38;5;241m=\u001b[39m text[i:i \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_length \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m---> 16\u001b[0m input_seq \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mbos_token \u001b[38;5;241m+\u001b[39m input_seq\n\u001b[1;32m     17\u001b[0m target_seq \u001b[38;5;241m=\u001b[39m target_seq \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39meos_token\n\u001b[1;32m     18\u001b[0m input_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mencode(input_seq)\n",
      "\u001b[0;31mTypeError\u001b[0m: can only concatenate str (not \"list\") to str"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, text, tokenizer, max_length=100):\n",
    "        self.text = text\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.data = self.preprocess(text)\n",
    "\n",
    "    def preprocess(self, text):\n",
    "        data = []\n",
    "        for i in range(1, len(text)):\n",
    "            input_seq = text[:i]\n",
    "            target_seq = text[i:i + self.max_length - 1]\n",
    "            input_seq = self.tokenizer.bos_token + input_seq\n",
    "            target_seq = target_seq + self.tokenizer.eos_token\n",
    "            input_idx = self.tokenizer.encode(input_seq)\n",
    "            target_idx = self.tokenizer.encode(target_seq)\n",
    "            # Pad sequences to the max_length\n",
    "            input_idx += [self.tokenizer.pad_token_id] * (self.max_length - len(input_idx))\n",
    "            target_idx += [self.tokenizer.pad_token_id] * (self.max_length - len(target_idx))\n",
    "            data.append((input_idx, target_idx))\n",
    "        return data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_idx, target_idx = self.data[idx]\n",
    "        return torch.tensor(input_idx), torch.tensor(target_idx)\n",
    "\n",
    "# Example usage\n",
    "text = [\"hello world.\", \"this is a simple text dataset for training a transformer.\"]\n",
    "max_length = 20\n",
    "tokenizer = MyTokenizer(text)\n",
    "dataset = TextDataset(text, tokenizer, max_length=max_length)\n",
    "dataloader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
    "\n",
    "# Print some examples\n",
    "for input_seq, target_seq in dataloader:\n",
    "    print(\"Input:\", tokenizer.decode(input_seq[0].tolist()))\n",
    "    print(\"Target:\", tokenizer.decode(target_seq[0].tolist()))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for +: 'int' and 'MyTokenizer'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m seq_length \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m\n\u001b[1;32m      4\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m MyTokenizer(text)\n\u001b[0;32m----> 5\u001b[0m dataset \u001b[38;5;241m=\u001b[39m TextDataset(text, \u001b[38;5;241m50\u001b[39m, tokenizer)\n\u001b[1;32m      6\u001b[0m dataloader \u001b[38;5;241m=\u001b[39m DataLoader(dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[7], line 9\u001b[0m, in \u001b[0;36mTextDataset.__init__\u001b[0;34m(self, text, tokenizer, max_length)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer \u001b[38;5;241m=\u001b[39m tokenizer\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_length \u001b[38;5;241m=\u001b[39m max_length\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocess(text)\n",
      "Cell \u001b[0;32mIn[7], line 15\u001b[0m, in \u001b[0;36mTextDataset.preprocess\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mlen\u001b[39m(text)):\n\u001b[1;32m     14\u001b[0m     input_seq \u001b[38;5;241m=\u001b[39m text[:i]\n\u001b[0;32m---> 15\u001b[0m     target_seq \u001b[38;5;241m=\u001b[39m text[i:i \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_length \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     16\u001b[0m     input_seq \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mbos_token \u001b[38;5;241m+\u001b[39m input_seq\n\u001b[1;32m     17\u001b[0m     target_seq \u001b[38;5;241m=\u001b[39m target_seq \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39meos_token\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for +: 'int' and 'MyTokenizer'"
     ]
    }
   ],
   "source": [
    "\n",
    "# Example usage\n",
    "text = \"hello world. this is a simple toy text dataset for training a transformer.\"\n",
    "seq_length = 10\n",
    "tokenizer = MyTokenizer(text)\n",
    "dataset = TextDataset(text, 50, tokenizer)\n",
    "dataloader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([ 1,  3,  5,  3, 18, 11, 13, 16, 12,  7,  3, 19, 15, 22,  3, 19,  7, 21,\n",
       "         19,  3,  6,  5, 19,  5, 18,  7, 19,  3,  8, 15, 17,  3, 19, 17,  5, 11,\n",
       "         14, 11, 14,  9,  3,  5,  3, 19, 17,  5, 14, 18,  8, 15, 17]),\n",
       " tensor([ 5,  3, 18, 11, 13, 16, 12,  7,  3, 19, 15, 22,  3, 19,  7, 21, 19,  3,\n",
       "          6,  5, 19,  5, 18,  7, 19,  3,  8, 15, 17,  3, 19, 17,  5, 11, 14, 11,\n",
       "         14,  9,  3,  5,  3, 19, 17,  5, 14, 18,  8, 15, 17, 13,  0]))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(tokenizer.pad_token_id)\n",
    "dataset.__getitem__(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Transformer.__init__() missing 2 required positional arguments: 'num_encoders' and 'num_decoders'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m num_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m\n\u001b[1;32m     14\u001b[0m learning_rate \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.001\u001b[39m\n\u001b[0;32m---> 16\u001b[0m model \u001b[38;5;241m=\u001b[39m Transformer(d_model, \u001b[38;5;241m100\u001b[39m, vocab_size, num_out, num_heads, dv, dk, d_ff, dropout, num_encoders, num_decoders)\n\u001b[1;32m     17\u001b[0m criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()\n\u001b[1;32m     18\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mlearning_rate)\n",
      "\u001b[0;31mTypeError\u001b[0m: Transformer.__init__() missing 2 required positional arguments: 'num_encoders' and 'num_decoders'"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define the Transformer model (assuming the Transformer class is already defined)\n",
    "d_model = 128\n",
    "max_length = seq_length + 1  # Adjust for start token\n",
    "vocab_size = len(tokenizer.chars)\n",
    "num_out = vocab_size\n",
    "num_heads = 8\n",
    "dv = 16\n",
    "dk = 16\n",
    "d_ff = 512\n",
    "dropout = 0.1\n",
    "num_encoders = 2\n",
    "num_decoders = 2\n",
    "num_epochs = 10\n",
    "learning_rate = 0.001\n",
    "\n",
    "model = Transformer(d_model, 100, vocab_size, num_out, num_heads, dv, dk, d_ff, dropout, num_encoders, num_decoders)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for input_seq, target_seq in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(input_seq, input_seq)\n",
    "        loss = criterion(output.view(-1, vocab_size), target_seq.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {total_loss / len(dataloader)}\")\n",
    "\n",
    "# Save the model\n",
    "torch.save(model.state_dict(), \"transformer_model.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Load the model\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m model\u001b[38;5;241m.\u001b[39mload_state_dict(torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransformer_model.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m, weights_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m))\n\u001b[1;32m      3\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Generate text\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load the model\n",
    "model.load_state_dict(torch.load(\"transformer_model.pth\", weights_only=True))\n",
    "model.eval()\n",
    "\n",
    "# Generate text\n",
    "\n",
    "input_seq = tokenizer.encode(tokenizer.bos_token + \"hello\")\n",
    "\n",
    "\n",
    "generated_text = model.generate(torch.tensor(input_seq), max_gen_length=50, start_token=tokenizer.bos_token_id, end_token=tokenizer.eos_token_id)\n",
    "print(\"Input:\", tokenizer.decode(input_seq))\n",
    "print(\"Generated Text:\", tokenizer.decode(generated_text[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: set for tr\n",
      "Target: et for tra\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "\n",
    "class MyTokenizer:\n",
    "    def __init__(self, text):\n",
    "        self.chars = sorted(list(set(text)))\n",
    "        self.char_to_idx = {char: idx for idx, char in enumerate(self.chars)}\n",
    "        self.idx_to_char = {idx: char for idx, char in enumerate(self.chars)}\n",
    "\n",
    "    def encode(self, text):\n",
    "        return [self.char_to_idx[char] for char in text]\n",
    "\n",
    "    def decode(self, indices):\n",
    "        return ''.join([self.idx_to_char[idx] for idx in indices])\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, text, seq_length, tokenizer):\n",
    "        self.text = text\n",
    "        self.seq_length = seq_length\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = self.preprocess(text)\n",
    "\n",
    "    def preprocess(self, text):\n",
    "        data = []\n",
    "        for i in range(0, len(text) - self.seq_length):\n",
    "            input_seq = text[i:i + self.seq_length]\n",
    "            target_seq = text[i + 1:i + self.seq_length + 1]\n",
    "            input_idx = self.tokenizer.encode(input_seq)\n",
    "            target_idx = self.tokenizer.encode(target_seq)\n",
    "            data.append((input_idx, target_idx))\n",
    "        return data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_idx, target_idx = self.data[idx]\n",
    "        return torch.tensor(input_idx), torch.tensor(target_idx)\n",
    "\n",
    "# Example usage\n",
    "text = \"hello world. this is a simple text dataset for training a transformer.\"\n",
    "seq_length = 10\n",
    "tokenizer = MyTokenizer(text)\n",
    "dataset = TextDataset(text, seq_length, tokenizer)\n",
    "dataloader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
    "\n",
    "# Print some examples\n",
    "for input_seq, target_seq in dataloader:\n",
    "    print(\"Input:\", tokenizer.decode(input_seq[0].tolist()))\n",
    "    print(\"Target:\", tokenizer.decode(target_seq[0].tolist()))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Transformer.__init__() missing 2 required positional arguments: 'num_encoders' and 'num_decoders'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 20\u001b[0m\n\u001b[1;32m     17\u001b[0m learning_rate \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.001\u001b[39m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Initialize model, loss function, and optimizer\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m model \u001b[38;5;241m=\u001b[39m Transformer(d_model, max_length, vocab_size, num_out, num_heads, dv, dk, d_ff, dropout, num_encoders, num_decoders)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# for p in model.parameters():\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m#     if p.dim() > 1:\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m#         nn.init.xavier_uniform_(p)\u001b[39;00m\n\u001b[1;32m     25\u001b[0m criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()\n",
      "\u001b[0;31mTypeError\u001b[0m: Transformer.__init__() missing 2 required positional arguments: 'num_encoders' and 'num_decoders'"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Hyperparameters\n",
    "d_model = 128\n",
    "max_length = seq_length\n",
    "vocab_size = len(tokenizer.chars)\n",
    "num_out = vocab_size\n",
    "num_heads = 8\n",
    "dv = 16\n",
    "dk = 16\n",
    "d_ff = 512\n",
    "dropout = 0.1\n",
    "num_encoders = 2\n",
    "num_decoders = 2\n",
    "num_epochs = 20\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Initialize model, loss function, and optimizer\n",
    "model = Transformer(d_model, max_length, vocab_size, num_out, num_heads, dv, dk, d_ff, dropout, num_encoders, num_decoders)\n",
    "# for p in model.parameters():\n",
    "#     if p.dim() > 1:\n",
    "#         nn.init.xavier_uniform_(p)\n",
    "        \n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for input_seq, target_seq in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(input_seq, input_seq)\n",
    "        loss = criterion(output.view(-1, vocab_size), target_seq.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {total_loss / len(dataloader)}\")\n",
    "\n",
    "# Save the model\n",
    "torch.save(model.state_dict(), \"transformer_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Load the model\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m model\u001b[38;5;241m.\u001b[39mload_state_dict(torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransformer_model.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m      3\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Generate text\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "# Load the model\n",
    "model.load_state_dict(torch.load(\"transformer_model.pth\"))\n",
    "model.eval()\n",
    "\n",
    "# Generate text\n",
    "\n",
    "\n",
    "start_text = \"hello\"\n",
    "input_seq = tokenizer.encode(\"hello\")\n",
    "print(input_seq)\n",
    "\n",
    "\n",
    "# generated_text = model.generate(input, start_token=0, 50, dataset)\n",
    "# print(\"Generated Text:\", generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "No such file or directory (os error 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m special_tokens \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[PAD]\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[UNK]\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[CLS]\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[SEP]\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[MASK]\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     10\u001b[0m trainer \u001b[38;5;241m=\u001b[39m BpeTrainer(special_tokens\u001b[38;5;241m=\u001b[39mspecial_tokens)\n\u001b[0;32m---> 12\u001b[0m tokenizer\u001b[38;5;241m.\u001b[39mtrain([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m], trainer)\n\u001b[1;32m     13\u001b[0m tokenizer\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbpe_tokenizer.json\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     15\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m Tokenizer\u001b[38;5;241m.\u001b[39mfrom_file(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbpe_tokenizer.json\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mException\u001b[0m: No such file or directory (os error 2)"
     ]
    }
   ],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "\n",
    "tokenizer = Tokenizer(BPE())\n",
    "\n",
    "tokenizer.pre_tokenizer = Whitespace()\n",
    "special_tokens = [\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"]\n",
    "trainer = BpeTrainer(special_tokens=special_tokens)\n",
    "\n",
    "tokenizer.train([\"data.txt\"], trainer)\n",
    "tokenizer.save(\"bpe_tokenizer.json\")\n",
    "\n",
    "tokenizer = Tokenizer.from_file(\"bpe_tokenizer.json\")\n",
    "\n",
    "with open('/users/eleves-b/2021/valentin.dorseuil/Desktop/ScratchML/transformers/data.txt', 'r') as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "\n",
    "chunk_size = 10\n",
    "overlap_size = 5\n",
    "chunks = [''.join(lines[i:i+chunk_size]) for i in range(0, len(lines) - chunk_size + 1, chunk_size - overlap_size)]\n",
    "\n",
    "print(f\"Vocabulary size: {tokenizer.get_vocab_size()}\")\n",
    "\n",
    "encoded_text = tokenizer.encode_batch(chunks)\n",
    "for token in special_tokens:\n",
    "    token_id = tokenizer.token_to_id(token)\n",
    "    print(f\"Token: {token}, ID: {token_id}\")\n",
    "\n",
    "print(f\"Number of chunks: {len(encoded_text)}\")\n",
    "print(f\"Average chunk length (num tokens): {sum([len(chunk) for chunk in encoded_text])/len(encoded_text):.2f}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'chunks' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrandom\u001b[39;00m\n\u001b[1;32m      3\u001b[0m random\u001b[38;5;241m.\u001b[39mseed(\u001b[38;5;241m123\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m random\u001b[38;5;241m.\u001b[39mshuffle(chunks)\n\u001b[1;32m      6\u001b[0m train_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;241m0.8\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(chunks))\n\u001b[1;32m      7\u001b[0m val_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;241m0.1\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(chunks))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'chunks' is not defined"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "random.seed(123)\n",
    "random.shuffle(chunks)\n",
    "\n",
    "train_size = int(0.8 * len(chunks))\n",
    "val_size = int(0.1 * len(chunks))\n",
    "test_size = len(chunks) - train_size - val_size\n",
    "\n",
    "train_chunks = chunks[:train_size]\n",
    "val_chunks = chunks[train_size:train_size + val_size]\n",
    "test_chunks = chunks[train_size + val_size:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "/users/eleves-b/2021/valentin.dorseuil/.local/lib/python3.11/site-packages/torchtext/lib/libtorchtext.so: undefined symbol: _ZN5torch3jit21setUTF8DecodingIgnoreEb",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptim\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01moptim\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchtext\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AG_NEWS\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchtext\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Field, BucketIterator\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmodel\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Transformer\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torchtext/__init__.py:18\u001b[0m\n\u001b[1;32m     15\u001b[0m     _WARN \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# the following import has to happen first in order to load the torchtext C++ library\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchtext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _extension  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[1;32m     20\u001b[0m _TEXT_BUCKET \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://download.pytorch.org/models/text/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     22\u001b[0m _CACHE_DIR \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexpanduser(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(_get_torch_home(), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torchtext/_extension.py:64\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;66;03m# This import is for initializing the methods registered via PyBind11\u001b[39;00m\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;66;03m# This has to happen after the base library is loaded\u001b[39;00m\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchtext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _torchtext  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[0;32m---> 64\u001b[0m _init_extension()\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torchtext/_extension.py:58\u001b[0m, in \u001b[0;36m_init_extension\u001b[0;34m()\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _mod_utils\u001b[38;5;241m.\u001b[39mis_module_available(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorchtext._torchtext\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorchtext C++ Extension is not found.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 58\u001b[0m _load_lib(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlibtorchtext\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     59\u001b[0m \u001b[38;5;66;03m# This import is for initializing the methods registered via PyBind11\u001b[39;00m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;66;03m# This has to happen after the base library is loaded\u001b[39;00m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchtext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _torchtext\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torchtext/_extension.py:50\u001b[0m, in \u001b[0;36m_load_lib\u001b[0;34m(lib)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m path\u001b[38;5;241m.\u001b[39mexists():\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m---> 50\u001b[0m torch\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39mload_library(path)\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/anaconda3/lib/python3.11/site-packages/torch/_ops.py:643\u001b[0m, in \u001b[0;36m_Ops.load_library\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m    638\u001b[0m path \u001b[38;5;241m=\u001b[39m _utils_internal\u001b[38;5;241m.\u001b[39mresolve_library_path(path)\n\u001b[1;32m    639\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m dl_open_guard():\n\u001b[1;32m    640\u001b[0m     \u001b[38;5;66;03m# Import the shared library into the process, thus running its\u001b[39;00m\n\u001b[1;32m    641\u001b[0m     \u001b[38;5;66;03m# static (global) initialization code in order to register custom\u001b[39;00m\n\u001b[1;32m    642\u001b[0m     \u001b[38;5;66;03m# operators with the JIT.\u001b[39;00m\n\u001b[0;32m--> 643\u001b[0m     ctypes\u001b[38;5;241m.\u001b[39mCDLL(path)\n\u001b[1;32m    644\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloaded_libraries\u001b[38;5;241m.\u001b[39madd(path)\n",
      "File \u001b[0;32m/usr/local/anaconda3/lib/python3.11/ctypes/__init__.py:376\u001b[0m, in \u001b[0;36mCDLL.__init__\u001b[0;34m(self, name, mode, handle, use_errno, use_last_error, winmode)\u001b[0m\n\u001b[1;32m    373\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_FuncPtr \u001b[38;5;241m=\u001b[39m _FuncPtr\n\u001b[1;32m    375\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m handle \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 376\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle \u001b[38;5;241m=\u001b[39m _dlopen(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_name, mode)\n\u001b[1;32m    377\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    378\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle \u001b[38;5;241m=\u001b[39m handle\n",
      "\u001b[0;31mOSError\u001b[0m: /users/eleves-b/2021/valentin.dorseuil/.local/lib/python3.11/site-packages/torchtext/lib/libtorchtext.so: undefined symbol: _ZN5torch3jit21setUTF8DecodingIgnoreEb"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Hyperparameters\n",
    "batch_size = 16\n",
    "model_dim = 512\n",
    "max_length = 100\n",
    "vocab_size = 32000\n",
    "num_out = 4  # Four classes for AG News\n",
    "num_heads = 8\n",
    "dv = 64\n",
    "dk = 64\n",
    "d_ff = 2048\n",
    "dropout = 0.1\n",
    "num_encoders = 6\n",
    "num_decoders = 6\n",
    "num_epochs = 10\n",
    "learning_rate = 0.001\n",
    "\n",
    "\n",
    "\n",
    "# Prepare the AG News dataset\n",
    "def prepare_data():\n",
    "    TEXT = Field(tokenize='spacy', tokenizer_language='en_core_web_sm', include_lengths=True)\n",
    "    LABEL = Field(sequential=False, use_vocab=False)\n",
    "    \n",
    "    train_data, test_data = AG_NEWS.splits(TEXT, LABEL)\n",
    "    TEXT.build_vocab(train_data, max_size=vocab_size)\n",
    "    \n",
    "    train_iterator, test_iterator = BucketIterator.splits(\n",
    "        (train_data, test_data),\n",
    "        batch_size=batch_size,\n",
    "        sort_within_batch=True,\n",
    "        device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    )\n",
    "    \n",
    "    return train_iterator, test_iterator, TEXT.vocab\n",
    "\n",
    "# Training loop\n",
    "def train(model, train_iterator, num_epochs, learning_rate):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        \n",
    "        for batch in train_iterator:\n",
    "            input_data, input_lengths = batch.text\n",
    "            target_data = batch.label\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            output = model(input_data, target_data)\n",
    "            \n",
    "            loss = criterion(output, target_data)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss/len(train_iterator):.4f}\")\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     MyTransformer = init_transformer()\n",
    "#     train_iterator, test_iterator, vocab = prepare_data()\n",
    "#     train(MyTransformer, train_iterator, num_epochs, learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__For an English to French translation__\n",
    "\n",
    "- Encoder Input : [tokenA, tokenB, tokenC] #English\n",
    "- Target Sequence: [BOS, token1, token2, token3, EOS] #French\n",
    "- Decoder Input: [BOS, token1, token2, token3] #French\n",
    "- Labels: [token1, token2, token3, EOS] # French"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "import zipfile\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "# Download and extract the dataset\n",
    "filename = \"fra-eng.zip\"\n",
    "\n",
    "with zipfile.ZipFile(filename, 'r') as zip_ref:\n",
    "    zip_ref.extractall(\".\")\n",
    "\n",
    "# Read the dataset\n",
    "with open(\"fra.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "# Prepare the dataset\n",
    "pairs = [line.strip().split('\\t') for line in lines]\n",
    "english_sentences = [pair[0] for pair in pairs]\n",
    "french_sentences = [pair[1] for pair in pairs]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizer class\n",
    "class Tokenizer:\n",
    "    def __init__(self, sentences):\n",
    "        self.bos_token = \"<BOS>\"\n",
    "        self.eos_token = \"<EOS>\"\n",
    "        self.pad_token = \"<PAD>\"\n",
    "        self.special_tokens = [self.bos_token, self.eos_token, self.pad_token]\n",
    "        self.build_vocab(sentences)\n",
    "\n",
    "    def build_vocab(self, sentences):\n",
    "        self.vocab = set()\n",
    "        for sentence in sentences:\n",
    "            self.vocab.update(sentence.split())\n",
    "        self.vocab = sorted(list(self.vocab))\n",
    "        self.vocab = self.special_tokens + self.vocab\n",
    "        self.word_to_idx = {word: idx for idx, word in enumerate(self.vocab)}\n",
    "        self.idx_to_word = {idx: word for idx, word in enumerate(self.vocab)}\n",
    "        self.sos_token_id = self.word_to_idx[self.bos_token]\n",
    "        self.eos_token_id = self.word_to_idx[self.eos_token]\n",
    "        self.pad_token_id = self.word_to_idx[self.pad_token]\n",
    "\n",
    "    def encode(self, sentence):\n",
    "        return [self.word_to_idx[word] for word in sentence.split()]\n",
    "\n",
    "    def decode(self, indices):\n",
    "        return ' '.join([self.idx_to_word[idx] for idx in indices if idx not in {self.sos_token_id, self.eos_token_id, self.pad_token_id}])\n",
    "\n",
    "# Create tokenizers for English and French\n",
    "english_tokenizer = Tokenizer(english_sentences)\n",
    "french_tokenizer = Tokenizer(french_sentences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset class\n",
    "class TranslationDataset(Dataset):\n",
    "    \"\"\"English to French translation Dataset\n",
    "\n",
    "    Args:\n",
    "        Dataset (class): The base pytorch class for datasets\n",
    "\n",
    "    Comments:\n",
    "        - Encoder Input : [tokenA, tokenB, tokenC] #English\n",
    "        - Target Sequence: [BOS, token1, token2, token3, EOS] #French\n",
    "        - Decoder Input: [BOS, token1, token2, token3] #French\n",
    "        - Labels: [token1, token2, token3, EOS] # French\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, english_sentences, french_sentences, english_tokenizer, french_tokenizer, max_length_french, max_length_english):\n",
    "        self.english_sentences = english_sentences\n",
    "        self.french_sentences = french_sentences\n",
    "        self.english_tokenizer = english_tokenizer\n",
    "        self.french_tokenizer = french_tokenizer\n",
    "        self.max_length_french = max_length_french\n",
    "        self.max_length_english = max_length_english\n",
    "        self.data = self.preprocess()\n",
    "\n",
    "    def preprocess(self): #\n",
    "        data = []\n",
    "        for eng, fra in zip(self.english_sentences, self.french_sentences):\n",
    "            if len(eng.split()) > self.max_length_english or len(fra.split()) > self.max_length_french-1:\n",
    "                continue #We remove the sequences that are too long (only 800 and 150 of each -> not a lot)\n",
    "            \n",
    "            eng_tokens = eng.split() \n",
    "            fra_tokens =[self.french_tokenizer.bos_token] + fra.split() + [self.french_tokenizer.eos_token]\n",
    "\n",
    "            eng_idx = self.english_tokenizer.encode(' '.join(eng_tokens))\n",
    "            fra_idx = self.french_tokenizer.encode(' '.join(fra_tokens))\n",
    "\n",
    "            eng_idx += [self.english_tokenizer.pad_token_id] * (self.max_length_english - len(eng_idx))\n",
    "            fra_idx += [self.french_tokenizer.pad_token_id] * (self.max_length_french - len(fra_idx) + 1) #+1 because then we will shift outputs to the right\n",
    "\n",
    "            encoder_input = torch.tensor(eng_idx)\n",
    "            decoder_input = torch.tensor(fra_idx[:-1])\n",
    "            label = torch.tensor(fra_idx[1:])\n",
    "\n",
    "            encoder_mask = (encoder_input == self.english_tokenizer.pad_token_id).to(torch.float32)\n",
    "\n",
    "\n",
    "            decoder_mask = (decoder_input == self.french_tokenizer.pad_token_id).to(torch.float32)\n",
    "\n",
    "\n",
    "            data.append({\"encoder_input\": encoder_input, \"decoder_input\":decoder_input, \"label\":label, \"encoder_mask\":encoder_mask, \"decoder_mask\":decoder_mask})\n",
    "        return data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        return item[\"encoder_input\"], item[\"decoder_input\"], item[\"label\"], item[\"encoder_mask\"], item[\"decoder_mask\"]\n",
    "    \n",
    "max_length_english = 20\n",
    "max_length_french = 25\n",
    "dataset = TranslationDataset(english_sentences, french_sentences, english_tokenizer, french_tokenizer, max_length_french, max_length_english)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 20 25 25 25\n",
      "20 20 25 25 25\n",
      "20 20 25 25 25\n",
      "20 20 25 25 25\n",
      "20 20 25 25 25\n",
      "20 20 25 25 25\n",
      "20 20 25 25 25\n",
      "20 20 25 25 25\n",
      "20 20 25 25 25\n",
      "20 20 25 25 25\n",
      "20 20 25 25 25\n",
      "20 20 25 25 25\n",
      "20 20 25 25 25\n",
      "20 20 25 25 25\n",
      "20 20 25 25 25\n",
      "20 20 25 25 25\n",
      "20 20 25 25 25\n",
      "20 20 25 25 25\n",
      "20 20 25 25 25\n",
      "20 20 25 25 25\n",
      "20 20 25 25 25\n",
      "20 20 25 25 25\n",
      "20 20 25 25 25\n",
      "20 20 25 25 25\n",
      "20 20 25 25 25\n",
      "20 20 25 25 25\n",
      "20 20 25 25 25\n",
      "20 20 25 25 25\n",
      "20 20 25 25 25\n",
      "20 20 25 25 25\n",
      "20 20 25 25 25\n",
      "20 20 25 25 25\n",
      "20 20 25 25 25\n",
      "20 20 25 25 25\n",
      "20 20 25 25 25\n",
      "20 20 25 25 25\n",
      "20 20 25 25 25\n",
      "20 20 25 25 25\n",
      "20 20 25 25 25\n",
      "20 20 25 25 25\n",
      "20 20 25 25 25\n",
      "20 20 25 25 25\n",
      "20 20 25 25 25\n",
      "20 20 25 25 25\n",
      "20 20 25 25 25\n",
      "20 20 25 25 25\n",
      "20 20 25 25 25\n",
      "20 20 25 25 25\n",
      "20 20 25 25 25\n"
     ]
    }
   ],
   "source": [
    "for k in range(dataset.__len__()):\n",
    "    input, output, label, mask_encoder, mask_decoder = dataset.__getitem__(k)\n",
    "    if len(input) > 20 or len(mask_encoder) > 20 or len(output) > 25 or len(label)>25 or len(mask_decoder)> 25:\n",
    "        print(\"stop : \", k)\n",
    "        break\n",
    "    if k >= 817*16 and k <= 820*16:\n",
    "\n",
    "        print(len(input), len(mask_encoder), len(output), len(label),len(mask_decoder))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into train, validation, and test sets\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = int(0.1 * len(dataset))\n",
    "test_size = len(dataset) - train_size - val_size\n",
    "train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n",
    "\n",
    "# Create DataLoaders for each split\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder Input: torch.Size([32, 20])\n",
      "Decoder Input: torch.Size([32, 25])\n",
      "Label: torch.Size([32, 25])\n",
      "Encoder Mask: torch.Size([32, 20])\n",
      "Decoder Mask: torch.Size([32, 25])\n",
      "Decoced input: I'm the strongest.\n",
      "Decoced input: Je suis le plus fort.\n"
     ]
    }
   ],
   "source": [
    "for batch in train_loader:\n",
    "    encoder_input, decoder_input, label, encoder_mask, decoder_mask = batch\n",
    "    print(\"Encoder Input:\", encoder_input.shape)\n",
    "    print(\"Decoder Input:\", decoder_input.shape)\n",
    "    print(\"Label:\", label.shape)\n",
    "    print(\"Encoder Mask:\", encoder_mask.shape)\n",
    "    print(\"Decoder Mask:\", decoder_mask.shape)\n",
    "    print(\"Decoced input:\", english_tokenizer.decode(encoder_input[0].tolist()))\n",
    "    print(\"Decoced input:\", french_tokenizer.decode(label[0].tolist()))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#Parameters\n",
    "batch_size = 32\n",
    "d_model = 128\n",
    "max_length_encoder = max_length_english\n",
    "max_length_decoder = max_length_french\n",
    "vocab_size_encoder = len(english_tokenizer.vocab)\n",
    "vocab_size_decoder = len(french_tokenizer.vocab)\n",
    "num_out = vocab_size_decoder\n",
    "num_heads = 8\n",
    "dv = 16\n",
    "dk = 16\n",
    "d_ff = 512\n",
    "dropout = 0.1\n",
    "num_encoders = 4\n",
    "num_decoders = 4\n",
    "dropout = 0.1\n",
    "\n",
    "learning_rate = 0.001\n",
    "num_epochs = 10\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "\n",
    "def init_transformer():\n",
    "    model = Transformer(\n",
    "        d_model=d_model,\n",
    "        max_length_encoder=max_length_encoder,\n",
    "        max_length_decoder=max_length_decoder,\n",
    "        vocab_size_encoder=vocab_size_encoder,\n",
    "        vocab_size_decoder=vocab_size_decoder,\n",
    "        num_out=num_out,\n",
    "        num_heads=num_heads,\n",
    "        dv=dv,\n",
    "        dk=dk,\n",
    "        d_ff=d_ff,\n",
    "        dropout=dropout,\n",
    "        num_encoders=num_encoders,\n",
    "        num_decoders=num_decoders,\n",
    "    )\n",
    "    return model\n",
    "model = init_transformer()\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=english_tokenizer.pad_token_id)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Assuming the model, criterion, optimizer, train_loader, and val_loader are already defined\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "def train(num_epochs):\n",
    "    best_val_loss = float('inf')\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "        for batch_idx, batch in enumerate(train_loader):\n",
    "            encoder_input, decoder_input, label, encoder_mask, decoder_mask = batch\n",
    "            encoder_input, decoder_input, label, encoder_mask, decoder_mask = encoder_input.to(device), decoder_input.to(device), label.to(device), encoder_mask.to(device), decoder_mask.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            output = model(encoder_input, decoder_input, encoder_mask, decoder_mask)\n",
    "            loss = criterion(output.view(-1, vocab_size_decoder), label.view(-1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_train_loss += loss.item()\n",
    "            \n",
    "            if batch_idx %100 == 0: \n",
    "                print(f\"Epoch [{epoch + 1}/{num_epochs}], Batch [{batch_idx + 1}/{len(train_loader)}], Loss: {loss.item():.4f}\")\n",
    "        \n",
    "        avg_train_loss = total_train_loss / len(train_loader)\n",
    "        train_losses.append(avg_train_loss)\n",
    "        \n",
    "        model.eval()\n",
    "        total_val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                encoder_input, decoder_input, label, encoder_mask, decoder_mask = batch\n",
    "                encoder_input, decoder_input, label, encoder_mask, decoder_mask = encoder_input.to(device), decoder_input.to(device), label.to(device), encoder_mask.to(device), decoder_mask.to(device)\n",
    "            \n",
    "                output = model(encoder_input, decoder_input, encoder_mask, decoder_mask)\n",
    "                loss = criterion(output.view(-1, vocab_size_decoder), label.view(-1))\n",
    "                total_val_loss += loss.item()\n",
    "        \n",
    "        avg_val_loss = total_val_loss / len(val_loader)\n",
    "        val_losses.append(avg_val_loss)\n",
    "        \n",
    "        print(f\"Epoch [{epoch + 1}/{num_epochs}], Average Training Loss: {avg_train_loss:.4f}, Average Validation Loss: {avg_val_loss:.4f}\")\n",
    "        \n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            torch.save(model.state_dict(), \"best_transformer_model.pth\")\n",
    "            print(f\"Model saved with validation loss: {best_val_loss:.4f}\")\n",
    "\n",
    "    return train_losses, val_losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters in the Transformer model: 19204991\n"
     ]
    }
   ],
   "source": [
    "num_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Number of parameters in the Transformer model: {num_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Batch [1/5814], Loss: 11.1658\n",
      "Epoch [1/5], Batch [101/5814], Loss: 6.0203\n",
      "Epoch [1/5], Batch [201/5814], Loss: 5.3267\n",
      "Epoch [1/5], Batch [301/5814], Loss: 5.1262\n",
      "Epoch [1/5], Batch [401/5814], Loss: 4.9666\n",
      "Epoch [1/5], Batch [501/5814], Loss: 4.8190\n",
      "Epoch [1/5], Batch [601/5814], Loss: 4.6695\n",
      "Epoch [1/5], Batch [701/5814], Loss: 4.5371\n",
      "Epoch [1/5], Batch [801/5814], Loss: 4.7708\n",
      "Epoch [1/5], Batch [901/5814], Loss: 4.0614\n",
      "Epoch [1/5], Batch [1001/5814], Loss: 4.0212\n",
      "Epoch [1/5], Batch [1101/5814], Loss: 4.3762\n",
      "Epoch [1/5], Batch [1201/5814], Loss: 4.3826\n",
      "Epoch [1/5], Batch [1301/5814], Loss: 4.0596\n",
      "Epoch [1/5], Batch [1401/5814], Loss: 4.1522\n",
      "Epoch [1/5], Batch [1501/5814], Loss: 3.8611\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[70], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m num_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m\n\u001b[0;32m----> 2\u001b[0m train_losses, val_losses \u001b[38;5;241m=\u001b[39m train(num_epochs)\n",
      "Cell \u001b[0;32mIn[68], line 24\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(num_epochs)\u001b[0m\n\u001b[1;32m     22\u001b[0m output \u001b[38;5;241m=\u001b[39m model(encoder_input, decoder_input, encoder_mask, decoder_mask)\n\u001b[1;32m     23\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(output\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, vocab_size_decoder), label\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m---> 24\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     25\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     27\u001b[0m total_train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    580\u001b[0m     )\n\u001b[0;32m--> 581\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[1;32m    582\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[1;32m    583\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m _engine_run_backward(\n\u001b[1;32m    348\u001b[0m     tensors,\n\u001b[1;32m    349\u001b[0m     grad_tensors_,\n\u001b[1;32m    350\u001b[0m     retain_graph,\n\u001b[1;32m    351\u001b[0m     create_graph,\n\u001b[1;32m    352\u001b[0m     inputs,\n\u001b[1;32m    353\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    354\u001b[0m     accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    355\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/autograd/graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    826\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    827\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_epochs = 5\n",
    "train_losses, val_losses = train(num_epochs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
