{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "- x : (batch_size, max_length) \n",
    "\n",
    "- tokens_id in x between 0 and vocab_size\n",
    "\n",
    "- Embedd(x) : (batch_size, max_length, model_dim)\n",
    "\n",
    "- K : (model_dim, dk)\n",
    "- Kx : (batch_size, max_length, dk)\n",
    "\n",
    "- Q : (model_dim, dk)\n",
    "- Qx : (batch_size, max_length, dk)\n",
    "\n",
    "- Qx*Kx^T : (batch_size, max_length, max_length)\n",
    "- V : (model_dim, dv)\n",
    "- Vx : (batch_size, max_length, dv)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, batch_size, model_dim, max_length):\n",
    "        super().__init__()\n",
    "        self.model_dim = model_dim\n",
    "        self.max_length = max_length\n",
    "        self.batch_size = batch_size\n",
    "        self.compute()\n",
    "\n",
    "    def SinPos(self, i: int, pos: int):\n",
    "        if i % 2 == 0:\n",
    "            return np.sin(pos / 10000 ** (2 * i / self.model_dim))\n",
    "        else:\n",
    "            return np.cos(pos / 10000 ** (2 * i / self.model_dim))\n",
    "\n",
    "    def compute(self):\n",
    "        Mat = torch.Tensor([[self.SinPos(i, pos) for i in range(self.model_dim)] for pos in range(self.max_length)])\n",
    "        self.Mat = Mat.expand(self.batch_size, -1, -1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        with torch.no_grad():\n",
    "            return self.Mat\n",
    "\n",
    "\n",
    "class Embedding(nn.Module):\n",
    "    def __init__(self, batch_size, model_dim, max_length, n_embedding):\n",
    "        super().__init__()\n",
    "        self.max_length = max_length\n",
    "        self.batch_size = batch_size\n",
    "        self.model_dim = model_dim\n",
    "        self.n_embedding = n_embedding\n",
    "        self.embedding = torch.nn.Embedding(num_embeddings=n_embedding, embedding_dim=model_dim)\n",
    "        self.pos_encoding = PositionalEncoding(batch_size=batch_size, model_dim=model_dim, max_length=max_length)\n",
    "        pass\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x) + self.pos_encoding(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class SingleHeadAttention(nn.Module):\n",
    "    def __init__(self, dk: int, dv: int, model_dim: int, mask: torch.Tensor = None):\n",
    "        super().__init__()\n",
    "        self.dk = dk\n",
    "        self.dv = dv\n",
    "        self.model_dim = model_dim\n",
    "        self.K = nn.Linear(in_features=model_dim, out_features=dk)\n",
    "        self.Q = nn.Linear(in_features=model_dim, out_features=dk)\n",
    "        self.V = nn.Linear(in_features=model_dim, out_features=dv)\n",
    "        self.mask = mask\n",
    "\n",
    "    def forward(self, x: torch.Tensor, x_encoder: torch.Tensor = None):\n",
    "        Kx = self.K(x_encoder) if x_encoder is not None else self.K(x)\n",
    "        Vx = self.V(x_encoder) if x_encoder is not None else self.V(x)\n",
    "        Qx = self.Q(x)\n",
    "        QK = torch.matmul(Qx, Kx.transpose(-2, -1)) / np.sqrt(self.dk)\n",
    "        if self.mask is not None:\n",
    "            QK += self.mask\n",
    "        QK = torch.softmax(QK, dim=-1)\n",
    "        return torch.matmul(QK, Vx)\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, num_heads: int, dk: int, dv: int, model_dim: int, mask=None):\n",
    "        super().__init__()\n",
    "        assert num_heads * dv == model_dim, \"num_heads * dv should be equal to the model dim\"\n",
    "        self.attention_heads = nn.ModuleList([SingleHeadAttention(dk=dk, dv=dv, model_dim=model_dim, mask=mask) for _ in range(num_heads)])\n",
    "        self.WO = nn.Linear(in_features=num_heads * dv, out_features=model_dim)\n",
    "        self.mask = mask\n",
    "\n",
    "    def forward(self, x: torch.Tensor, x_encoder: torch.Tensor = None):\n",
    "        outputs = [head(x, x_encoder) for head in self.attention_heads]\n",
    "        x = torch.cat(outputs, dim=-1)\n",
    "        x = self.WO(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, num_heads, dk, dv, d_ff, model_dim, dropout):\n",
    "        super().__init__()\n",
    "        self.attention = MultiHeadAttention(num_heads=num_heads, dk=dk, dv=dv, model_dim=model_dim)\n",
    "        self.layerNorm1 = nn.LayerNorm(normalized_shape=model_dim)\n",
    "        self.layerNorm2 = nn.LayerNorm(normalized_shape=model_dim)\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(in_features=model_dim, out_features=d_ff),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(in_features=d_ff, out_features=model_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        attention = self.attention(x)\n",
    "        x = self.layerNorm1(x + attention)\n",
    "        feedforward = self.ff(x)\n",
    "        x = self.layerNorm2(x + feedforward)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, num_heads, dk, dv, d_ff, model_dim, dropout, num_encoders):\n",
    "        super().__init__()\n",
    "        self.encoders_list = [\n",
    "            EncoderBlock(num_heads=num_heads, dk=dk, dv=dv, d_ff=d_ff, model_dim=model_dim, dropout=dropout) for _ in range(num_encoders)\n",
    "        ]\n",
    "        self.encoders = nn.Sequential(*self.encoders_list)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoders(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, num_heads, dk, dv, d_ff, model_dim, dropout, max_length):\n",
    "        super().__init__()\n",
    "        self.mask = torch.zeros(max_length, max_length) + torch.triu(torch.full((max_length, max_length), float(\"-inf\")), diagonal=1)\n",
    "        self.masked_attention = MultiHeadAttention(num_heads=num_heads, dk=dk, dv=dv, model_dim=model_dim, mask=self.mask)\n",
    "        self.mixed_attention = MultiHeadAttention(num_heads=num_heads, dk=dk, dv=dv, model_dim=model_dim)\n",
    "        self.layerNorm1 = nn.LayerNorm(normalized_shape=model_dim)\n",
    "        self.layerNorm2 = nn.LayerNorm(normalized_shape=model_dim)\n",
    "        self.layerNorm3 = nn.LayerNorm(normalized_shape=model_dim)\n",
    "\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(in_features=model_dim, out_features=d_ff),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(in_features=d_ff, out_features=model_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x, x_encoder):\n",
    "        attention = self.masked_attention(x)\n",
    "        x = self.layerNorm1(x + attention)\n",
    "        attention = self.mixed_attention(x, x_encoder)\n",
    "        x = self.layerNorm2(x + attention)\n",
    "        feedforward = self.ff(x)\n",
    "        x = self.layerNorm3(x + feedforward)\n",
    "        return x\n",
    "\n",
    "\n",
    "class CustomSequential(nn.Module):\n",
    "    def __init__(self, *args):\n",
    "        super().__init__()\n",
    "        self.modules_list = nn.ModuleList(args)\n",
    "\n",
    "    def forward(self, x, x_encoder):\n",
    "        for module in self.modules_list:\n",
    "            x = module(x, x_encoder)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, num_heads, dk, dv, d_ff, model_dim, max_length, dropout, num_decoders):\n",
    "        super().__init__()\n",
    "        decoders_list = [\n",
    "            DecoderBlock(\n",
    "                num_heads=num_heads,\n",
    "                dk=dk,\n",
    "                dv=dv,\n",
    "                d_ff=d_ff,\n",
    "                model_dim=model_dim,\n",
    "                dropout=dropout,\n",
    "                max_length=max_length,\n",
    "            )\n",
    "            for _ in range(num_decoders)\n",
    "        ]\n",
    "        self.decoders = CustomSequential(*decoders_list)\n",
    "\n",
    "    def forward(self, x, x_encoder):\n",
    "        x = self.decoders(x, x_encoder)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        batch_size,\n",
    "        model_dim,\n",
    "        max_length,\n",
    "        vocab_size,\n",
    "        num_out,\n",
    "        num_heads,\n",
    "        dv,\n",
    "        dk,\n",
    "        d_ff,\n",
    "        dropout,\n",
    "        num_encoders,\n",
    "        num_decoders,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(\n",
    "            num_heads=num_heads,\n",
    "            dk=dk,\n",
    "            dv=dv,\n",
    "            d_ff=d_ff,\n",
    "            model_dim=model_dim,\n",
    "            dropout=dropout,\n",
    "            num_encoders=num_encoders,\n",
    "        )\n",
    "        self.decoder = Decoder(\n",
    "            num_heads=num_heads,\n",
    "            dk=dk,\n",
    "            dv=dv,\n",
    "            d_ff=d_ff,\n",
    "            model_dim=model_dim,\n",
    "            dropout=dropout,\n",
    "            num_decoders=num_decoders,\n",
    "            max_length=max_length,\n",
    "        )\n",
    "        self.linear = nn.Linear(in_features=model_dim, out_features=num_out)\n",
    "        self.embedding = Embedding(\n",
    "            batch_size=batch_size,\n",
    "            model_dim=model_dim,\n",
    "            max_length=max_length,\n",
    "            n_embedding=vocab_size,\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x_encoder = self.encoder(x)\n",
    "        x = self.decoder(x, x_encoder)\n",
    "        x = self.linear(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 110])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 16\n",
    "model_dim = 512\n",
    "max_length = 100\n",
    "vocab_size = 20000\n",
    "num_out = vocab_size\n",
    "num_heads = 8\n",
    "dv = 64\n",
    "dk = 64\n",
    "d_ff = 2048\n",
    "dropout = 0.1\n",
    "num_encoders = 6\n",
    "num_decoders = 6\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def init_transformer():\n",
    "    model = Transformer(\n",
    "        batch_size=batch_size,\n",
    "        model_dim=model_dim,\n",
    "        max_length=max_length,\n",
    "        vocab_size=vocab_size,\n",
    "        num_out=num_out,\n",
    "        num_heads=num_heads,\n",
    "        dv=dv,\n",
    "        dk=dk,\n",
    "        d_ff=d_ff,\n",
    "        dropout=dropout,\n",
    "        num_encoders=num_encoders,\n",
    "        num_decoders=num_decoders,\n",
    "    )\n",
    "    for p in model.parameters():\n",
    "        if p.dim() > 1:\n",
    "            nn.init.xavier_uniform_(p)\n",
    "    return model\n",
    "\n",
    "def inference(model, input, max_gen_length):\n",
    "    output = input\n",
    "    x = input\n",
    "    for i in range(max_gen_length):\n",
    "        proba = torch.softmax(model(x), dim=-1)\n",
    "        max_proba, next_token = torch.max(proba, dim=-1)\n",
    "        next_token = next_token[:, -1].unsqueeze(-1)\n",
    "        x = torch.cat((x[:, 1:], next_token), dim=1)\n",
    "        output = torch.cat((output, next_token), dim=1)\n",
    "    return output\n",
    "\n",
    "\n",
    "x = torch.randint(0, vocab_size, (batch_size, max_length))\n",
    "MyTransformer = init_transformer()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "inference(MyTransformer, x, 10).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Vocabulary size: 18151\n",
      "Token: [PAD], ID: 0\n",
      "Token: [UNK], ID: 1\n",
      "Token: [CLS], ID: 2\n",
      "Token: [SEP], ID: 3\n",
      "Token: [MASK], ID: 4\n",
      "Number of chunks: 7999\n",
      "Average chunk length (num tokens): 65.50\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "\n",
    "tokenizer = Tokenizer(BPE())\n",
    "\n",
    "tokenizer.pre_tokenizer = Whitespace()\n",
    "special_tokens = [\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"]\n",
    "trainer = BpeTrainer(special_tokens=special_tokens)\n",
    "\n",
    "tokenizer.train([\"data.txt\"], trainer)\n",
    "tokenizer.save(\"bpe_tokenizer.json\")\n",
    "\n",
    "tokenizer = Tokenizer.from_file(\"bpe_tokenizer.json\")\n",
    "\n",
    "with open('/users/eleves-b/2021/valentin.dorseuil/Desktop/ScratchML/transformers/data.txt', 'r') as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "\n",
    "chunk_size = 10\n",
    "overlap_size = 5\n",
    "chunks = [''.join(lines[i:i+chunk_size]) for i in range(0, len(lines) - chunk_size + 1, chunk_size - overlap_size)]\n",
    "\n",
    "print(f\"Vocabulary size: {tokenizer.get_vocab_size()}\")\n",
    "\n",
    "encoded_text = tokenizer.encode_batch(chunks)\n",
    "for token in special_tokens:\n",
    "    token_id = tokenizer.token_to_id(token)\n",
    "    print(f\"Token: {token}, ID: {token_id}\")\n",
    "\n",
    "print(f\"Number of chunks: {len(encoded_text)}\")\n",
    "print(f\"Average chunk length (num tokens): {sum([len(chunk) for chunk in encoded_text])/len(encoded_text):.2f}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'First Citizen : Before we proceed any further , hear me speak . All : Speak , speak . First Citizen : You are all resolved rather to die than to famish ? All :'"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(encoded_text[0].ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "random.seed(123)\n",
    "random.shuffle(chunks)\n",
    "\n",
    "train_size = int(0.8 * len(chunks))\n",
    "val_size = int(0.1 * len(chunks))\n",
    "test_size = len(chunks) - train_size - val_size\n",
    "\n",
    "train_chunks = chunks[:train_size]\n",
    "val_chunks = chunks[train_size:train_size + val_size]\n",
    "test_chunks = chunks[train_size + val_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
