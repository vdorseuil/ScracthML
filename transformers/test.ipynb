{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "- x : (batch_size, max_length) \n",
    "\n",
    "- tokens_id in x between 0 and vocab_size\n",
    "\n",
    "- Embedd(x) : (batch_size, max_length, model_dim)\n",
    "\n",
    "- K : (model_dim, dk)\n",
    "- Kx : (batch_size, max_length, dk)\n",
    "\n",
    "- Q : (model_dim, dk)\n",
    "- Qx : (batch_size, max_length, dk)\n",
    "\n",
    "- Qx*Kx^T : (batch_size, max_length, max_length)\n",
    "- V : (model_dim, dv)\n",
    "- Vx : (batch_size, max_length, dv)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_length):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.max_length = max_length\n",
    "        self.compute()\n",
    "\n",
    "    def SinPos(self, i: int, pos: int):\n",
    "        if i % 2 == 0:\n",
    "            return np.sin(pos / 10000 ** (2 * i / self.d_model))\n",
    "        else:\n",
    "            return np.cos(pos / 10000 ** (2 * i / self.d_model))\n",
    "\n",
    "    def compute(self):\n",
    "        self.Mat = torch.Tensor([[self.SinPos(i, pos) for i in range(self.d_model)] for pos in range(self.max_length)])\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.Mat[:x.shape[-1], :]\n",
    "\n",
    "\n",
    "class Embedding(nn.Module):\n",
    "    def __init__(self, d_model, max_length, n_embedding, dropout):\n",
    "        super().__init__()\n",
    "        self.max_length = max_length\n",
    "        self.d_model = d_model\n",
    "        self.n_embedding = n_embedding\n",
    "        self.embedding = nn.Embedding(num_embeddings=n_embedding, embedding_dim=d_model)\n",
    "        self.pos_encoding = PositionalEncoding(d_model=d_model, max_length=max_length)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        pass\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x) + self.pos_encoding(x)\n",
    "        return self.dropout(x)\n",
    "\n",
    "\n",
    "class SingleHeadAttention(nn.Module):\n",
    "    def __init__(self, dk: int, dv: int, d_model: int):\n",
    "        super().__init__()\n",
    "        self.dk = dk\n",
    "        self.dv = dv\n",
    "        self.d_model = d_model\n",
    "        self.K = nn.Linear(in_features=d_model, out_features=dk)\n",
    "        self.Q = nn.Linear(in_features=d_model, out_features=dk)\n",
    "        self.V = nn.Linear(in_features=d_model, out_features=dv)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, x_encoder: torch.Tensor = None, mask=None):\n",
    "        Kx = self.K(x_encoder) if x_encoder is not None else self.K(x)\n",
    "        Vx = self.V(x_encoder) if x_encoder is not None else self.V(x)\n",
    "        Qx = self.Q(x)\n",
    "        QK = torch.matmul(Qx, Kx.transpose(-2, -1)) / np.sqrt(self.dk)\n",
    "        if mask is not None:\n",
    "            QK = QK + mask\n",
    "        QK = torch.softmax(QK, dim=-1)\n",
    "        return torch.matmul(QK, Vx)\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, num_heads: int, dk: int, dv: int, d_model: int):\n",
    "        super().__init__()\n",
    "        assert num_heads * dv == d_model, \"num_heads * dv should be equal to the model dim\"\n",
    "        self.attention_heads = nn.ModuleList([SingleHeadAttention(dk=dk, dv=dv, d_model=d_model) for _ in range(num_heads)])\n",
    "        self.WO = nn.Linear(in_features=num_heads * dv, out_features=d_model)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, x_encoder: torch.Tensor = None, mask=None):\n",
    "        outputs = [head(x, x_encoder, mask) for head in self.attention_heads]\n",
    "        x = torch.cat(outputs, dim=-1)\n",
    "        x = self.WO(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "class SublayerConnection(nn.Module): #Dropout, Add and Norm\n",
    "    def __init__(self, features, dropout, eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.a_2 = nn.Parameter(torch.ones(features))\n",
    "        self.b_2 = nn.Parameter(torch.zeros(features))\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x, sublayer_output):\n",
    "        x = x + self.dropout(sublayer_output) #dropout and add\n",
    "        mean = x.mean(-1, keepdim=True)\n",
    "        std = x.std(-1, keepdim=True)\n",
    "        x = self.a_2 * (x - mean) / (std + self.eps) + self.b_2 #norm\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, num_heads, dk, dv, d_ff, d_model, dropout):\n",
    "        super().__init__()\n",
    "        self.attention = MultiHeadAttention(num_heads=num_heads, dk=dk, dv=dv, d_model=d_model)\n",
    "        self.sublayer1 = SublayerConnection(features=d_model, dropout=dropout)\n",
    "        self.sublayer2 = SublayerConnection(features=d_model, dropout=dropout)\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(in_features=d_model, out_features=d_ff),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(in_features=d_ff, out_features=d_model),\n",
    "        )\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        attention = self.attention(x=x, mask=mask)\n",
    "        x = self.sublayer1(x, attention)\n",
    "        feedforward = self.ff(x)\n",
    "        x = self.sublayer2(x, feedforward)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, num_heads, dk, dv, d_ff, d_model, dropout, num_encoders):\n",
    "        super().__init__()\n",
    "        self.encoders_list = [EncoderBlock(num_heads=num_heads, dk=dk, dv=dv, d_ff=d_ff, d_model=d_model, dropout=dropout) for _ in range(num_encoders)]\n",
    "        self.encoders = nn.ModuleList(self.encoders_list)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        for encoder in self.encoders_list:\n",
    "            x = encoder(x, mask)\n",
    "        return x\n",
    "\n",
    "\n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, num_heads, dk, dv, d_ff, d_model, dropout):\n",
    "        super().__init__()\n",
    "        self.masked_attention = MultiHeadAttention(num_heads=num_heads, dk=dk, dv=dv, d_model=d_model)\n",
    "        self.mixed_attention = MultiHeadAttention(num_heads=num_heads, dk=dk, dv=dv, d_model=d_model)\n",
    "        self.sublayer1 = SublayerConnection(features=d_model, dropout=dropout)\n",
    "        self.sublayer2 = SublayerConnection(features=d_model, dropout=dropout)\n",
    "        self.sublayer3 = SublayerConnection(features=d_model, dropout=dropout)\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(in_features=d_model, out_features=d_ff),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(in_features=d_ff, out_features=d_model),\n",
    "        )\n",
    "\n",
    "    def forward(self, x, x_encoder, causal_mask=None, mixed_mask=None):\n",
    "        attention = self.masked_attention(x, mask=causal_mask)\n",
    "        x = self.sublayer1(x, attention)\n",
    "        attention = self.mixed_attention(x, x_encoder=x_encoder, mask=mixed_mask)\n",
    "        x = self.sublayer2(x, attention)\n",
    "        feedforward = self.ff(x)\n",
    "        x = self.sublayer3(x, feedforward)\n",
    "        return x\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, num_heads, dk, dv, d_ff, d_model, dropout, num_decoders):\n",
    "        super().__init__()\n",
    "        decoders_list = [DecoderBlock(num_heads=num_heads, dk=dk, dv=dv, d_ff=d_ff, d_model=d_model, dropout=dropout) for _ in range(num_decoders)]\n",
    "        self.decoders = nn.ModuleList(decoders_list)\n",
    "\n",
    "    def forward(self, x, x_encoder, causal_mask=None, mixed_mask=None):\n",
    "        for decoder in self.decoders:\n",
    "            x = decoder(x, x_encoder, causal_mask, mixed_mask)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, d_model, max_length, vocab_size, num_out, num_heads, dv, dk, d_ff, dropout, num_encoders, num_decoders):\n",
    "        super().__init__()\n",
    "        self.embedding = Embedding(d_model, max_length, n_embedding=vocab_size, dropout=dropout)\n",
    "        self.encoder = Encoder(num_heads, dk, dv, d_ff, d_model, dropout, num_encoders)\n",
    "        self.decoder = Decoder(num_heads, dk, dv, d_ff, d_model, dropout, num_decoders)\n",
    "        self.linear = nn.Linear(in_features=d_model, out_features=num_out)\n",
    "        self.ff_mask = torch.zeros(max_length, max_length) + torch.triu(torch.full((max_length, max_length), float(\"-inf\")), diagonal=1)\n",
    "\n",
    "    def forward(self, input, output):\n",
    "        input_embed = self.embedding(input)\n",
    "        output_embed = self.embedding(output)\n",
    "        x_encoder = self.encoder(input_embed)\n",
    "        x = self.decoder(output_embed, x_encoder, causal_mask=self.ff_mask)\n",
    "        x = self.linear(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "    def generate(self, input, max_gen_length, start_token, end_token): #greed decoding\n",
    "        self.eval()\n",
    "        input_embed = self.embedding(input)\n",
    "        x_encoder = self.encoder(input_embed)\n",
    "\n",
    "        generated_tokens = [start_token]\n",
    "        generated_tokens_probas = [1]\n",
    "\n",
    "        for _ in range(max_gen_length):\n",
    "            output = torch.tensor(generated_tokens).unsqueeze(0) # size [1, sequence_length]\n",
    "            out_embed = self.embedding(output)\n",
    "            causal_mask = self.ff_mask[:out_embed.size(1), :out_embed.size(1)]\n",
    "            x = self.decoder(out_embed, x_encoder, causal_mask=causal_mask)\n",
    "            x = self.linear(x)\n",
    "            probas = torch.softmax(x, dim=-1)\n",
    "            max_proba, next_token = torch.max(probas[:, -1, :], dim=-1) #greedy decoding : only max_proba\n",
    "            generated_tokens.append(next_token.item())\n",
    "            generated_tokens_probas.append(max_proba.item())\n",
    "            if next_token == end_token:\n",
    "                break\n",
    "            \n",
    "        return generated_tokens, generated_tokens_probas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parameters\n",
    "batch_size = 2\n",
    "d_model = 128\n",
    "max_length = 100\n",
    "vocab_size = 20000\n",
    "num_out = vocab_size\n",
    "num_heads = 8\n",
    "dv = 16\n",
    "dk = 16\n",
    "d_ff = 512\n",
    "dropout = 0.1\n",
    "num_encoders = 2\n",
    "num_decoders = 2\n",
    "dropout = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 100])\n",
      "torch.Size([2, 100, 128])\n",
      "torch.Size([2, 100, 128])\n",
      "torch.Size([2, 100, 128])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randint(0, vocab_size, (batch_size, max_length))\n",
    "print(x.shape)\n",
    "MyEmbedding = Embedding(d_model, max_length, n_embedding=vocab_size, dropout=dropout)\n",
    "x= MyEmbedding(x)\n",
    "print(x.shape)\n",
    "\n",
    "MyEncoder = Encoder(num_heads, dk, dv, d_ff, d_model, dropout, num_encoders)\n",
    "x_encoder = MyEncoder(x)\n",
    "print(x.shape)\n",
    "\n",
    "MyDecoder = Decoder(num_heads, dk, dv, d_ff, d_model, dropout, num_decoders)\n",
    "x = MyDecoder(x, x_encoder) \n",
    "print(x.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 2719, 2719, 2719, 2719, 2719, 2719, 2719, 2719, 2719, 2719]\n"
     ]
    }
   ],
   "source": [
    "def init_transformer():\n",
    "    model = Transformer(\n",
    "        d_model=d_model,\n",
    "        max_length=max_length,\n",
    "        vocab_size=vocab_size,\n",
    "        num_out=num_out,\n",
    "        num_heads=num_heads,\n",
    "        dv=dv,\n",
    "        dk=dk,\n",
    "        d_ff=d_ff,\n",
    "        dropout=dropout,\n",
    "        num_encoders=num_encoders,\n",
    "        num_decoders=num_decoders,\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "#######  Forward Test  #############\n",
    "input = torch.randint(0, vocab_size, (batch_size, max_length))\n",
    "output = torch.randint(0, vocab_size, (batch_size, max_length))\n",
    "\n",
    "MyTransformer = init_transformer()\n",
    "out = MyTransformer(input, output)\n",
    "out.shape\n",
    "\n",
    "\n",
    "####### Generation Test ######\n",
    "input_seq = torch.tensor([[0, 2, 3, 4, 5, 1]])\n",
    "start_token = 0 \n",
    "end_token = 1\n",
    "\n",
    "generated_seq, corresponding_probas = MyTransformer.generate(input_seq, max_gen_length=10, start_token=start_token, end_token=end_token)\n",
    "print(generated_seq)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Character Level Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "\n",
    "class MyTokenizer:\n",
    "    def __init__(self, text):\n",
    "        self.bos_token = \"<\"\n",
    "        self.eos_token = \">\"\n",
    "        self.chars = sorted(list(set(text)))\n",
    "        self.chars = [self.bos_token, self.eos_token] + self.chars\n",
    "        self.char_to_idx = {char: idx for idx, char in enumerate(self.chars)}\n",
    "        self.idx_to_char = {idx: char for idx, char in enumerate(self.chars)}\n",
    "        self.bos_token_id = self.char_to_idx[self.bos_token]\n",
    "        self.eos_token_id = self.char_to_idx[self.eos_token]\n",
    "\n",
    "    def encode(self, text):\n",
    "        return [self.char_to_idx[char] for char in text]\n",
    "\n",
    "    def decode(self, indices):\n",
    "        return ''.join([self.idx_to_char[idx] for idx in indices if idx not in {self.bos_token_id, self.eos_token_id}])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, text, seq_length, tokenizer):\n",
    "        self.text = text\n",
    "        self.seq_length = seq_length\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = self.preprocess(text)\n",
    "\n",
    "    def preprocess(self, text):\n",
    "        data = []\n",
    "        for i in range(0, len(text) - self.seq_length):\n",
    "            input_seq = text[i:i + self.seq_length]\n",
    "            target_seq = text[i + 1:i + self.seq_length + 1]\n",
    "            input_seq = self.tokenizer.bos_token + input_seq\n",
    "            target_seq = target_seq + self.tokenizer.eos_token\n",
    "            input_idx = self.tokenizer.encode(input_seq)\n",
    "            target_idx = self.tokenizer.encode(target_seq)\n",
    "            data.append((input_idx, target_idx))\n",
    "        return data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_idx, target_idx = self.data[idx]\n",
    "        return torch.tensor(input_idx), torch.tensor(target_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Example usage\n",
    "text = \"hello world. this is a simple toy text dataset for training a transformer.\"\n",
    "seq_length = 10\n",
    "tokenizer = MyTokenizer(text)\n",
    "dataset = TextDataset(text, seq_length, tokenizer)\n",
    "dataloader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60, Loss: 2.2776170186698437\n",
      "Epoch 2/60, Loss: 1.3597143031656742\n",
      "Epoch 3/60, Loss: 1.034982355311513\n",
      "Epoch 4/60, Loss: 0.7859382070600986\n",
      "Epoch 5/60, Loss: 0.6525621777400374\n",
      "Epoch 6/60, Loss: 0.5793520892038941\n",
      "Epoch 7/60, Loss: 0.48461533430963755\n",
      "Epoch 8/60, Loss: 0.4736202759668231\n",
      "Epoch 9/60, Loss: 0.3786097466945648\n",
      "Epoch 10/60, Loss: 0.35829091630876064\n",
      "Epoch 11/60, Loss: 0.341633356641978\n",
      "Epoch 12/60, Loss: 0.3049099618801847\n",
      "Epoch 13/60, Loss: 0.3518168574664742\n",
      "Epoch 14/60, Loss: 0.35978076606988907\n",
      "Epoch 15/60, Loss: 0.2855427381582558\n",
      "Epoch 16/60, Loss: 0.23374111426528543\n",
      "Epoch 17/60, Loss: 0.20292445935774595\n",
      "Epoch 18/60, Loss: 0.22330564120784402\n",
      "Epoch 19/60, Loss: 0.24253607168793678\n",
      "Epoch 20/60, Loss: 0.22264130611438304\n",
      "Epoch 21/60, Loss: 0.22580913186538965\n",
      "Epoch 22/60, Loss: 0.20121996733359993\n",
      "Epoch 23/60, Loss: 0.31154937646351755\n",
      "Epoch 24/60, Loss: 0.2642110785818659\n",
      "Epoch 25/60, Loss: 0.1764750420115888\n",
      "Epoch 26/60, Loss: 0.20117109356215224\n",
      "Epoch 27/60, Loss: 0.20780534914229065\n",
      "Epoch 28/60, Loss: 0.1806951385224238\n",
      "Epoch 29/60, Loss: 0.2186197459232062\n",
      "Epoch 30/60, Loss: 0.18314437987282872\n",
      "Epoch 31/60, Loss: 0.22482946969103068\n",
      "Epoch 32/60, Loss: 0.25387229141779244\n",
      "Epoch 33/60, Loss: 0.20493062573950738\n",
      "Epoch 34/60, Loss: 0.20471168513176963\n",
      "Epoch 35/60, Loss: 0.17292741872370243\n",
      "Epoch 36/60, Loss: 0.1643310298677534\n",
      "Epoch 37/60, Loss: 0.203150118351914\n",
      "Epoch 38/60, Loss: 0.17183631416992284\n",
      "Epoch 39/60, Loss: 0.1059809465368744\n",
      "Epoch 40/60, Loss: 0.14691377626149915\n",
      "Epoch 41/60, Loss: 0.19298424871522002\n",
      "Epoch 42/60, Loss: 0.2251172218238935\n",
      "Epoch 43/60, Loss: 0.2145397940184921\n",
      "Epoch 44/60, Loss: 0.1533269468927756\n",
      "Epoch 45/60, Loss: 0.19958536740159616\n",
      "Epoch 46/60, Loss: 0.11941052763722837\n",
      "Epoch 47/60, Loss: 0.14197243965463713\n",
      "Epoch 48/60, Loss: 0.11373357515549287\n",
      "Epoch 49/60, Loss: 0.13293461888679303\n",
      "Epoch 50/60, Loss: 0.10293528469628654\n",
      "Epoch 51/60, Loss: 0.09753822282073088\n",
      "Epoch 52/60, Loss: 0.0970019455999136\n",
      "Epoch 53/60, Loss: 0.09901625513157342\n",
      "Epoch 54/60, Loss: 0.1264956082450226\n",
      "Epoch 55/60, Loss: 0.14899535791482776\n",
      "Epoch 56/60, Loss: 0.15791964455274865\n",
      "Epoch 57/60, Loss: 0.18888839392457157\n",
      "Epoch 58/60, Loss: 0.1529884779592976\n",
      "Epoch 59/60, Loss: 0.1874580482544843\n",
      "Epoch 60/60, Loss: 0.3691522650187835\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define the Transformer model (assuming the Transformer class is already defined)\n",
    "d_model = 128\n",
    "max_length = seq_length + 1  # Adjust for start token\n",
    "vocab_size = len(tokenizer.chars)\n",
    "num_out = vocab_size\n",
    "num_heads = 8\n",
    "dv = 16\n",
    "dk = 16\n",
    "d_ff = 512\n",
    "dropout = 0.1\n",
    "num_encoders = 2\n",
    "num_decoders = 2\n",
    "num_epochs = 60\n",
    "learning_rate = 0.001\n",
    "\n",
    "model = Transformer(d_model, max_length, vocab_size, num_out, num_heads, dv, dk, d_ff, dropout, num_encoders, num_decoders)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for input_seq, target_seq in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(input_seq, input_seq)\n",
    "        loss = criterion(output.view(-1, vocab_size), target_seq.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {total_loss / len(dataloader)}\")\n",
    "\n",
    "# Save the model\n",
    "torch.save(model.state_dict(), \"transformer_model.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: hello\n",
      "Generated Text: elowrdol.t\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load the model\n",
    "model.load_state_dict(torch.load(\"transformer_model.pth\", weights_only=True))\n",
    "model.eval()\n",
    "\n",
    "# Generate text\n",
    "\n",
    "input_seq = tokenizer.encode(tokenizer.bos_token + \"hello\")\n",
    "\n",
    "\n",
    "generated_text = model.generate(torch.tensor(input_seq), max_gen_length=50, start_token=tokenizer.bos_token_id, end_token=tokenizer.eos_token_id)\n",
    "print(\"Input:\", tokenizer.decode(input_seq))\n",
    "print(\"Generated Text:\", tokenizer.decode(generated_text[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: for traini\n",
      "Target: or trainin\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "\n",
    "class MyTokenizer:\n",
    "    def __init__(self, text):\n",
    "        self.chars = sorted(list(set(text)))\n",
    "        self.char_to_idx = {char: idx for idx, char in enumerate(self.chars)}\n",
    "        self.idx_to_char = {idx: char for idx, char in enumerate(self.chars)}\n",
    "\n",
    "    def encode(self, text):\n",
    "        return [self.char_to_idx[char] for char in text]\n",
    "\n",
    "    def decode(self, indices):\n",
    "        return ''.join([self.idx_to_char[idx] for idx in indices])\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, text, seq_length, tokenizer):\n",
    "        self.text = text\n",
    "        self.seq_length = seq_length\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = self.preprocess(text)\n",
    "\n",
    "    def preprocess(self, text):\n",
    "        data = []\n",
    "        for i in range(0, len(text) - self.seq_length):\n",
    "            input_seq = text[i:i + self.seq_length]\n",
    "            target_seq = text[i + 1:i + self.seq_length + 1]\n",
    "            input_idx = self.tokenizer.encode(input_seq)\n",
    "            target_idx = self.tokenizer.encode(target_seq)\n",
    "            data.append((input_idx, target_idx))\n",
    "        return data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_idx, target_idx = self.data[idx]\n",
    "        return torch.tensor(input_idx), torch.tensor(target_idx)\n",
    "\n",
    "# Example usage\n",
    "text = \"hello world. this is a simple text dataset for training a transformer.\"\n",
    "seq_length = 10\n",
    "tokenizer = MyTokenizer(text)\n",
    "dataset = TextDataset(text, seq_length, tokenizer)\n",
    "dataloader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
    "\n",
    "# Print some examples\n",
    "for input_seq, target_seq in dataloader:\n",
    "    print(\"Input:\", tokenizer.decode(input_seq[0].tolist()))\n",
    "    print(\"Target:\", tokenizer.decode(target_seq[0].tolist()))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Loss: 2.0158475716908772\n",
      "Epoch 2/20, Loss: 0.9365063230196635\n",
      "Epoch 3/20, Loss: 0.5599679509798686\n",
      "Epoch 4/20, Loss: 0.4209852079550425\n",
      "Epoch 5/20, Loss: 0.3388981262842814\n",
      "Epoch 6/20, Loss: 0.23375614906350772\n",
      "Epoch 7/20, Loss: 0.2032998559375604\n",
      "Epoch 8/20, Loss: 0.2159905731678009\n",
      "Epoch 9/20, Loss: 0.17053566339115303\n",
      "Epoch 10/20, Loss: 0.1532356958836317\n",
      "Epoch 11/20, Loss: 0.1635421346873045\n",
      "Epoch 12/20, Loss: 0.1461115210627516\n",
      "Epoch 13/20, Loss: 0.13610078835239012\n",
      "Epoch 14/20, Loss: 0.1227427354703347\n",
      "Epoch 15/20, Loss: 0.10143420770764351\n",
      "Epoch 16/20, Loss: 0.0678825307947894\n",
      "Epoch 17/20, Loss: 0.08694310352827112\n",
      "Epoch 18/20, Loss: 0.09003200912848115\n",
      "Epoch 19/20, Loss: 0.0856722991913557\n",
      "Epoch 20/20, Loss: 0.12321084660167496\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Hyperparameters\n",
    "d_model = 128\n",
    "max_length = seq_length\n",
    "vocab_size = len(tokenizer.chars)\n",
    "num_out = vocab_size\n",
    "num_heads = 8\n",
    "dv = 16\n",
    "dk = 16\n",
    "d_ff = 512\n",
    "dropout = 0.1\n",
    "num_encoders = 2\n",
    "num_decoders = 2\n",
    "num_epochs = 20\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Initialize model, loss function, and optimizer\n",
    "model = Transformer(d_model, max_length, vocab_size, num_out, num_heads, dv, dk, d_ff, dropout, num_encoders, num_decoders)\n",
    "# for p in model.parameters():\n",
    "#     if p.dim() > 1:\n",
    "#         nn.init.xavier_uniform_(p)\n",
    "        \n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for input_seq, target_seq in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(input_seq, input_seq)\n",
    "        loss = criterion(output.view(-1, vocab_size), target_seq.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {total_loss / len(dataloader)}\")\n",
    "\n",
    "# Save the model\n",
    "torch.save(model.state_dict(), \"transformer_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7, 4, 9, 9, 12]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2448144/3065763057.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"transformer_model.pth\"))\n"
     ]
    }
   ],
   "source": [
    "# Load the model\n",
    "model.load_state_dict(torch.load(\"transformer_model.pth\"))\n",
    "model.eval()\n",
    "\n",
    "# Generate text\n",
    "\n",
    "\n",
    "start_text = \"hello\"\n",
    "input_seq = tokenizer.encode(\"hello\")\n",
    "print(input_seq)\n",
    "\n",
    "\n",
    "# generated_text = model.generate(input, start_token=0, 50, dataset)\n",
    "# print(\"Generated Text:\", generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "No such file or directory (os error 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[128], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m special_tokens \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[PAD]\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[UNK]\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[CLS]\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[SEP]\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[MASK]\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     10\u001b[0m trainer \u001b[38;5;241m=\u001b[39m BpeTrainer(special_tokens\u001b[38;5;241m=\u001b[39mspecial_tokens)\n\u001b[0;32m---> 12\u001b[0m tokenizer\u001b[38;5;241m.\u001b[39mtrain([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m], trainer)\n\u001b[1;32m     13\u001b[0m tokenizer\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbpe_tokenizer.json\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     15\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m Tokenizer\u001b[38;5;241m.\u001b[39mfrom_file(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbpe_tokenizer.json\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mException\u001b[0m: No such file or directory (os error 2)"
     ]
    }
   ],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "\n",
    "tokenizer = Tokenizer(BPE())\n",
    "\n",
    "tokenizer.pre_tokenizer = Whitespace()\n",
    "special_tokens = [\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"]\n",
    "trainer = BpeTrainer(special_tokens=special_tokens)\n",
    "\n",
    "tokenizer.train([\"data.txt\"], trainer)\n",
    "tokenizer.save(\"bpe_tokenizer.json\")\n",
    "\n",
    "tokenizer = Tokenizer.from_file(\"bpe_tokenizer.json\")\n",
    "\n",
    "with open('/users/eleves-b/2021/valentin.dorseuil/Desktop/ScratchML/transformers/data.txt', 'r') as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "\n",
    "chunk_size = 10\n",
    "overlap_size = 5\n",
    "chunks = [''.join(lines[i:i+chunk_size]) for i in range(0, len(lines) - chunk_size + 1, chunk_size - overlap_size)]\n",
    "\n",
    "print(f\"Vocabulary size: {tokenizer.get_vocab_size()}\")\n",
    "\n",
    "encoded_text = tokenizer.encode_batch(chunks)\n",
    "for token in special_tokens:\n",
    "    token_id = tokenizer.token_to_id(token)\n",
    "    print(f\"Token: {token}, ID: {token_id}\")\n",
    "\n",
    "print(f\"Number of chunks: {len(encoded_text)}\")\n",
    "print(f\"Average chunk length (num tokens): {sum([len(chunk) for chunk in encoded_text])/len(encoded_text):.2f}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "random.seed(123)\n",
    "random.shuffle(chunks)\n",
    "\n",
    "train_size = int(0.8 * len(chunks))\n",
    "val_size = int(0.1 * len(chunks))\n",
    "test_size = len(chunks) - train_size - val_size\n",
    "\n",
    "train_chunks = chunks[:train_size]\n",
    "val_chunks = chunks[train_size:train_size + val_size]\n",
    "test_chunks = chunks[train_size + val_size:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "/users/eleves-b/2021/valentin.dorseuil/.local/lib/python3.11/site-packages/torchtext/lib/libtorchtext.so: undefined symbol: _ZN5torch3jit21setUTF8DecodingIgnoreEb",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptim\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01moptim\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchtext\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AG_NEWS\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchtext\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Field, BucketIterator\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmodel\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Transformer\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torchtext/__init__.py:18\u001b[0m\n\u001b[1;32m     15\u001b[0m     _WARN \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# the following import has to happen first in order to load the torchtext C++ library\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchtext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _extension  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[1;32m     20\u001b[0m _TEXT_BUCKET \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://download.pytorch.org/models/text/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     22\u001b[0m _CACHE_DIR \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexpanduser(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(_get_torch_home(), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torchtext/_extension.py:64\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;66;03m# This import is for initializing the methods registered via PyBind11\u001b[39;00m\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;66;03m# This has to happen after the base library is loaded\u001b[39;00m\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchtext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _torchtext  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[0;32m---> 64\u001b[0m _init_extension()\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torchtext/_extension.py:58\u001b[0m, in \u001b[0;36m_init_extension\u001b[0;34m()\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _mod_utils\u001b[38;5;241m.\u001b[39mis_module_available(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorchtext._torchtext\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorchtext C++ Extension is not found.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 58\u001b[0m _load_lib(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlibtorchtext\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     59\u001b[0m \u001b[38;5;66;03m# This import is for initializing the methods registered via PyBind11\u001b[39;00m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;66;03m# This has to happen after the base library is loaded\u001b[39;00m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchtext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _torchtext\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torchtext/_extension.py:50\u001b[0m, in \u001b[0;36m_load_lib\u001b[0;34m(lib)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m path\u001b[38;5;241m.\u001b[39mexists():\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m---> 50\u001b[0m torch\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39mload_library(path)\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/anaconda3/lib/python3.11/site-packages/torch/_ops.py:643\u001b[0m, in \u001b[0;36m_Ops.load_library\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m    638\u001b[0m path \u001b[38;5;241m=\u001b[39m _utils_internal\u001b[38;5;241m.\u001b[39mresolve_library_path(path)\n\u001b[1;32m    639\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m dl_open_guard():\n\u001b[1;32m    640\u001b[0m     \u001b[38;5;66;03m# Import the shared library into the process, thus running its\u001b[39;00m\n\u001b[1;32m    641\u001b[0m     \u001b[38;5;66;03m# static (global) initialization code in order to register custom\u001b[39;00m\n\u001b[1;32m    642\u001b[0m     \u001b[38;5;66;03m# operators with the JIT.\u001b[39;00m\n\u001b[0;32m--> 643\u001b[0m     ctypes\u001b[38;5;241m.\u001b[39mCDLL(path)\n\u001b[1;32m    644\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloaded_libraries\u001b[38;5;241m.\u001b[39madd(path)\n",
      "File \u001b[0;32m/usr/local/anaconda3/lib/python3.11/ctypes/__init__.py:376\u001b[0m, in \u001b[0;36mCDLL.__init__\u001b[0;34m(self, name, mode, handle, use_errno, use_last_error, winmode)\u001b[0m\n\u001b[1;32m    373\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_FuncPtr \u001b[38;5;241m=\u001b[39m _FuncPtr\n\u001b[1;32m    375\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m handle \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 376\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle \u001b[38;5;241m=\u001b[39m _dlopen(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_name, mode)\n\u001b[1;32m    377\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    378\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle \u001b[38;5;241m=\u001b[39m handle\n",
      "\u001b[0;31mOSError\u001b[0m: /users/eleves-b/2021/valentin.dorseuil/.local/lib/python3.11/site-packages/torchtext/lib/libtorchtext.so: undefined symbol: _ZN5torch3jit21setUTF8DecodingIgnoreEb"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Hyperparameters\n",
    "batch_size = 16\n",
    "model_dim = 512\n",
    "max_length = 100\n",
    "vocab_size = 32000\n",
    "num_out = 4  # Four classes for AG News\n",
    "num_heads = 8\n",
    "dv = 64\n",
    "dk = 64\n",
    "d_ff = 2048\n",
    "dropout = 0.1\n",
    "num_encoders = 6\n",
    "num_decoders = 6\n",
    "num_epochs = 10\n",
    "learning_rate = 0.001\n",
    "\n",
    "\n",
    "\n",
    "# Prepare the AG News dataset\n",
    "def prepare_data():\n",
    "    TEXT = Field(tokenize='spacy', tokenizer_language='en_core_web_sm', include_lengths=True)\n",
    "    LABEL = Field(sequential=False, use_vocab=False)\n",
    "    \n",
    "    train_data, test_data = AG_NEWS.splits(TEXT, LABEL)\n",
    "    TEXT.build_vocab(train_data, max_size=vocab_size)\n",
    "    \n",
    "    train_iterator, test_iterator = BucketIterator.splits(\n",
    "        (train_data, test_data),\n",
    "        batch_size=batch_size,\n",
    "        sort_within_batch=True,\n",
    "        device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    )\n",
    "    \n",
    "    return train_iterator, test_iterator, TEXT.vocab\n",
    "\n",
    "# Training loop\n",
    "def train(model, train_iterator, num_epochs, learning_rate):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        \n",
    "        for batch in train_iterator:\n",
    "            input_data, input_lengths = batch.text\n",
    "            target_data = batch.label\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            output = model(input_data, target_data)\n",
    "            \n",
    "            loss = criterion(output, target_data)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss/len(train_iterator):.4f}\")\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     MyTransformer = init_transformer()\n",
    "#     train_iterator, test_iterator, vocab = prepare_data()\n",
    "#     train(MyTransformer, train_iterator, num_epochs, learning_rate)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
