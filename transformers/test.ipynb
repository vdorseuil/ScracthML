{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "- x : (batch_size, max_length) \n",
    "\n",
    "- tokens_id in x between 0 and vocab_size\n",
    "\n",
    "- Embedd(x) : (batch_size, max_length, model_dim)\n",
    "\n",
    "- K : (model_dim, dk)\n",
    "- Kx : (batch_size, max_length, dk)\n",
    "\n",
    "- Q : (model_dim, dk)\n",
    "- Qx : (batch_size, max_length, dk)\n",
    "\n",
    "- Qx*Kx^T : (batch_size, max_length, max_length)\n",
    "- V : (model_dim, dv)\n",
    "- Vx : (batch_size, max_length, dv)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, batch_size, model_dim, max_length):\n",
    "        super().__init__()\n",
    "        self.model_dim = model_dim\n",
    "        self.max_length = max_length\n",
    "        self.batch_size = batch_size\n",
    "        self.compute()\n",
    "\n",
    "    def SinPos(self, i: int, pos: int):\n",
    "        if i % 2 == 0:\n",
    "            return np.sin(pos / 10000 ** (2 * i / self.model_dim))\n",
    "        else:\n",
    "            return np.cos(pos / 10000 ** (2 * i / self.model_dim))\n",
    "\n",
    "    def compute(self):\n",
    "        Mat = torch.Tensor([[self.SinPos(i, pos) for i in range(self.model_dim)] for pos in range(self.max_length)])\n",
    "        self.Mat = Mat.expand(self.batch_size, -1, -1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        with torch.no_grad():\n",
    "            return self.Mat\n",
    "\n",
    "\n",
    "class Embedding(nn.Module):\n",
    "    def __init__(self, batch_size, model_dim, max_length, n_embedding):\n",
    "        super().__init__()\n",
    "        self.max_length = max_length\n",
    "        self.batch_size = batch_size\n",
    "        self.model_dim = model_dim\n",
    "        self.n_embedding = n_embedding\n",
    "        self.embedding = torch.nn.Embedding(num_embeddings=n_embedding, embedding_dim=model_dim)\n",
    "        self.pos_encoding = PositionalEncoding(batch_size=batch_size, model_dim=model_dim, max_length=max_length)\n",
    "        pass\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x) + self.pos_encoding(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class SingleHeadAttention(nn.Module):\n",
    "    def __init__(self, dk: int, dv: int, model_dim: int, mask: torch.Tensor = None):\n",
    "        super().__init__()\n",
    "        self.dk = dk\n",
    "        self.dv = dv\n",
    "        self.model_dim = model_dim\n",
    "        self.K = nn.Linear(in_features=model_dim, out_features=dk)\n",
    "        self.Q = nn.Linear(in_features=model_dim, out_features=dk)\n",
    "        self.V = nn.Linear(in_features=model_dim, out_features=dv)\n",
    "        self.mask = mask\n",
    "\n",
    "    def forward(self, x: torch.Tensor, x_encoder: torch.Tensor = None):\n",
    "        Kx = self.K(x_encoder) if x_encoder is not None else self.K(x)\n",
    "        Vx = self.V(x_encoder) if x_encoder is not None else self.V(x)\n",
    "        Qx = self.Q(x)\n",
    "        QK = torch.matmul(Qx, Kx.transpose(-2, -1)) / np.sqrt(self.dk)\n",
    "        if self.mask is not None:\n",
    "            QK += self.mask\n",
    "        QK = torch.softmax(QK, dim=-1)\n",
    "        return torch.matmul(QK, Vx)\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, num_heads: int, dk: int, dv: int, model_dim: int, mask=None):\n",
    "        super().__init__()\n",
    "        assert num_heads * dv == model_dim, \"num_heads * dv should be equal to the model dim\"\n",
    "        self.attention_heads = nn.ModuleList([SingleHeadAttention(dk, dv, model_dim, mask) for _ in range(num_heads)])\n",
    "        self.WO = nn.Linear(in_features=num_heads * dv, out_features=model_dim)\n",
    "        self.mask = mask\n",
    "\n",
    "    def forward(self, x: torch.Tensor, x_encoder: torch.Tensor = None):\n",
    "        outputs = [head(x, x_encoder) for head in self.attention_heads]\n",
    "        x = torch.cat(outputs, dim=-1)\n",
    "        x = self.WO(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, num_heads, dk, dv, model_dim):\n",
    "        super().__init__()\n",
    "        self.attention = MultiHeadAttention(num_heads=num_heads, dk=dk, dv=dv, model_dim=model_dim)\n",
    "        self.layerNorm1 = nn.LayerNorm(normalized_shape=model_dim)\n",
    "        self.layerNorm2 = nn.LayerNorm(normalized_shape=model_dim)\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(in_features=model_dim, out_features=2048),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=2048, out_features=model_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        attention = self.attention(x)\n",
    "        x = self.layerNorm1(x + attention)\n",
    "        feedforward = self.ff(x)\n",
    "        x = self.layerNorm2(x + feedforward)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, num_heads, dk, dv, model_dim, num_encoders):\n",
    "        super().__init__()\n",
    "        self.encoders_list = [EncoderBlock(num_heads, dk, dv, model_dim) for _ in range(num_encoders)]\n",
    "        self.encoders = nn.Sequential(*self.encoders_list)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoders(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, num_heads, dk, dv, model_dim, max_length):\n",
    "        super().__init__()\n",
    "        self.mask = torch.zeros(max_length, max_length) + torch.triu(\n",
    "            torch.full((max_length, max_length), float(\"-inf\")), diagonal=1\n",
    "        )\n",
    "        self.masked_attention = MultiHeadAttention(\n",
    "            num_heads=num_heads, dk=dk, dv=dv, model_dim=model_dim, mask=self.mask\n",
    "        )\n",
    "        self.mixed_attention = MultiHeadAttention(num_heads=num_heads, dk=dk, dv=dv, model_dim=model_dim)\n",
    "        self.layerNorm1 = nn.LayerNorm(normalized_shape=model_dim)\n",
    "        self.layerNorm2 = nn.LayerNorm(normalized_shape=model_dim)\n",
    "        self.layerNorm3 = nn.LayerNorm(normalized_shape=model_dim)\n",
    "\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(in_features=model_dim, out_features=2048),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=2048, out_features=model_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x, x_encoder):\n",
    "        attention = self.masked_attention(x)\n",
    "        x = self.layerNorm1(x + attention)\n",
    "        attention = self.mixed_attention(x, x_encoder)\n",
    "        x = self.layerNorm2(x + attention)\n",
    "        feedforward = self.ff(x)\n",
    "        x = self.layerNorm3(x + feedforward)\n",
    "        return x\n",
    "\n",
    "\n",
    "class CustomSequential(nn.Module):\n",
    "    def __init__(self, *args):\n",
    "        super().__init__()\n",
    "        self.modules_list = nn.ModuleList(args)\n",
    "\n",
    "    def forward(self, x, x_encoder):\n",
    "        for module in self.modules_list:\n",
    "            x = module(x, x_encoder)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, num_heads, dk, dv, model_dim, max_length, num_decoders):\n",
    "        super().__init__()\n",
    "        decoders_list = [DecoderBlock(num_heads, dk, dv, model_dim, max_length) for _ in range(num_decoders)]\n",
    "        self.decoders = CustomSequential(*decoders_list)\n",
    "\n",
    "    def forward(self, x, x_encoder):\n",
    "        x = self.decoders(x, x_encoder)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        batch_size,\n",
    "        model_dim,\n",
    "        max_length,\n",
    "        vocab_size,\n",
    "        num_out,\n",
    "        num_heads,\n",
    "        dv,\n",
    "        dk,\n",
    "        num_encoders,\n",
    "        num_decoders,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(\n",
    "            num_heads=num_heads,\n",
    "            dk=dk,\n",
    "            dv=dv,\n",
    "            model_dim=model_dim,\n",
    "            num_encoders=num_encoders,\n",
    "        )\n",
    "        self.decoder = Decoder(\n",
    "            num_heads=num_heads,\n",
    "            dk=dk,\n",
    "            dv=dv,\n",
    "            model_dim=model_dim,\n",
    "            num_decoders=num_decoders,\n",
    "            max_length=max_length,\n",
    "        )\n",
    "        self.linear = nn.Linear(in_features=model_dim, out_features=num_out)\n",
    "        self.embedding = Embedding(\n",
    "            batch_size=batch_size,\n",
    "            model_dim=model_dim,\n",
    "            max_length=max_length,\n",
    "            n_embedding=vocab_size,\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x_encoder = self.encoder(x)\n",
    "        x = self.decoder(x, x_encoder)\n",
    "        x = self.linear(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 100, 32000])\n"
     ]
    }
   ],
   "source": [
    "batch_size = 16\n",
    "model_dim = 512\n",
    "max_length = 100\n",
    "vocab_size = 32000\n",
    "num_out = vocab_size\n",
    "num_heads = 8\n",
    "dv = 64\n",
    "dk = 64\n",
    "num_encoders = 6\n",
    "num_decoders = 6\n",
    "\n",
    "x = torch.randint(0, vocab_size, (batch_size, max_length))\n",
    "\n",
    "MyTransformer = Transformer(\n",
    "    batch_size=batch_size,\n",
    "    model_dim=model_dim,\n",
    "    max_length=max_length,\n",
    "    vocab_size=vocab_size,\n",
    "    num_out=num_out,\n",
    "    num_heads=num_heads,\n",
    "    dv=dv,\n",
    "    dk=dk,\n",
    "    num_encoders=num_encoders,\n",
    "    num_decoders=num_decoders,\n",
    ")\n",
    "\n",
    "\n",
    "out = torch.softmax(MyTransformer(x), dim=-1)\n",
    "print(out.shape)\n",
    "\n",
    "\n",
    "∏"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
