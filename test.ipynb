{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "- x : (batch_size, max_length) \n",
    "\n",
    "- tokens_id in x between 0 and vocab_size\n",
    "\n",
    "- Embedd(x) : (batch_size, max_length, model_dim)\n",
    "\n",
    "- K : (model_dim, dk)\n",
    "- Kx : (batch_size, max_length, dk)\n",
    "\n",
    "- Q : (model_dim, dk)\n",
    "- Qx : (batch_size, max_length, dk)\n",
    "\n",
    "- Qx*Kx^T : (batch_size, max_length, max_length)\n",
    "- V : (model_dim, dv)\n",
    "- Vx : (batch_size, max_length, dv)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding(nn.Module):\n",
    "    def __init__(self,  batch_size, model_dim, max_length, n_embedding):\n",
    "        super().__init__()\n",
    "        self.max_length = max_length\n",
    "        self.batch_size = batch_size\n",
    "        self.model_dim = model_dim\n",
    "        self.n_embedding = n_embedding\n",
    "        self.embedding = torch.nn.Embedding(num_embeddings=n_embedding, embedding_dim=model_dim)\n",
    "        self.pos_encoding = PositionalEncoding(batch_size=batch_size, model_dim=model_dim, max_length=max_length)\n",
    "        pass\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x) + self.pos_encoding(x)\n",
    "        return x\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, batch_size, model_dim, max_length):\n",
    "        super().__init__()\n",
    "        self.model_dim = model_dim\n",
    "        self.max_length = max_length\n",
    "        self.batch_size = batch_size\n",
    "        self.compute()\n",
    "\n",
    "    def SinPos(self, i: int, pos: int):\n",
    "        if i % 2 == 0:\n",
    "            return np.sin(pos / 10000 ** (2 * i / self.model_dim))\n",
    "        else:\n",
    "            return np.cos(pos / 10000 ** (2 * i / self.model_dim))\n",
    "\n",
    "    def compute(self):\n",
    "        Mat = torch.Tensor([[self.SinPos(i, pos) for i in range(self.model_dim)] for pos in range(self.max_length)])\n",
    "        self.Mat = Mat\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        with torch.no_grad():\n",
    "            return self.Mat\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SingleHeadAttention(nn.Module):\n",
    "    def __init__(self, dk:int, dv:int, model_dim:int, mask:torch.Tensor=None):\n",
    "        super().__init__()\n",
    "        self.dk = dk\n",
    "        self.dv = dv\n",
    "        self.model_dim = model_dim\n",
    "        self.K = nn.Linear(in_features=model_dim, out_features=dk)\n",
    "        self.Q = nn.Linear(in_features=model_dim, out_features=dk)\n",
    "        self.V = nn.Linear(in_features=model_dim, out_features=dv)\n",
    "        self.mask = mask\n",
    "\n",
    "    def forward(self, x:torch.Tensor, x_encoder:torch.Tensor=None):\n",
    "        if x_encoder is not None:\n",
    "            Kx = self.K(x_encoder)\n",
    "            Vx = self.V(x_encoder)\n",
    "        else:\n",
    "            Kx = self.K(x)\n",
    "            Vx = self.V(x)\n",
    "        Qx = self.Q(x)\n",
    "        QK = torch.matmul(Qx, Kx.transpose(-2, -1)) / np.sqrt(self.dk)\n",
    "        if self.mask is not None:\n",
    "            QK += self.mask\n",
    "        QK = torch.softmax(QK, dim=-1)\n",
    "        return torch.matmul(QK, Vx)\n",
    "\n",
    "    \n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, num_heads : int, dk:int, dv:int, model_dim:int, mask=None):\n",
    "        super().__init__()\n",
    "        assert num_heads * dv == model_dim, \"num_heads * dv should be equal to the model dim\"\n",
    "        self.attention_heads = nn.ModuleList([SingleHeadAttention(dk, dv, model_dim, mask) for _ in range(num_heads)])\n",
    "        self.WO = nn.Linear(in_features=num_heads*dv, out_features=model_dim)  \n",
    "        self.mask = mask\n",
    "    \n",
    "    def forward(self, x:torch.Tensor, x_encoder:torch.Tensor=None):\n",
    "        outputs = [head(x, x_encoder) for head in self.attention_heads]\n",
    "        x = torch.cat(outputs,dim=-1)\n",
    "        x = self.WO(x)\n",
    "        return x\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, num_heads, dk, dv, model_dim):\n",
    "        super().__init__()\n",
    "        self.attention = MultiHeadAttention(num_heads=num_heads, dk=dk, dv=dv, model_dim=model_dim)\n",
    "        self.layerNorm = nn.LayerNorm(normalized_shape=model_dim)\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(in_features=model_dim, out_features=2048),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=2048, out_features=model_dim)\n",
    "        )\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        attention = self.attention(x)\n",
    "        x = self.layerNorm(x + attention)\n",
    "        feedforward = self.ff(x)\n",
    "        x = self.layerNorm(x + feedforward)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, num_heads, dk, dv, model_dim, num_encoders):\n",
    "        super().__init__()\n",
    "        self.encoders_list = [EncoderBlock(num_heads, dk, dv, model_dim) for _ in range(num_encoders)]\n",
    "        self.encoders = nn.Sequential(*self.encoders_list)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoders(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, num_heads, dk, dv, model_dim, max_length):\n",
    "        super().__init__()\n",
    "        self.mask = torch.zeros(max_length, max_length) + torch.triu(torch.full((max_length, max_length), float('-inf')), diagonal=1)\n",
    "        self.masked_attention = MultiHeadAttention(num_heads=num_heads, dk=dk, dv=dv, model_dim=model_dim, mask=self.mask)\n",
    "        self.mixed_attention = MultiHeadAttention(num_heads=num_heads, dk=dk, dv=dv, model_dim=model_dim)\n",
    "        self.layerNorm = nn.LayerNorm(normalized_shape=model_dim)\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(in_features=model_dim, out_features=2048),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=2048, out_features=model_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, x_encoder):\n",
    "        attention = self.masked_attention(x)\n",
    "        x = self.layerNorm(x + attention)\n",
    "        attention = self.mixed_attention(x, x_encoder)\n",
    "        x = self.layerNorm(x + attention)\n",
    "        feedforward = self.ff(x)\n",
    "        x = self.layerNorm(x + feedforward)\n",
    "        return x\n",
    "\n",
    "class CustomSequential(nn.Module):\n",
    "    def __init__(self, *args):\n",
    "        super().__init__()\n",
    "        self.modules_list = nn.ModuleList(args)\n",
    "\n",
    "    def forward(self, x, x_encoder):\n",
    "        for module in self.modules_list:\n",
    "            x = module(x, x_encoder)\n",
    "        return x\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, num_heads, dk, dv, model_dim, max_length, num_decoders):\n",
    "        super().__init__()\n",
    "        decoders_list = [DecoderBlock(num_heads, dk, dv, model_dim, max_length) for _ in range(num_decoders)]\n",
    "        self.decoders = CustomSequential(*decoders_list)\n",
    "\n",
    "    def forward(self, x, x_encoder):\n",
    "        x = self.decoders(x, x_encoder)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, batch_size, model_dim, max_length, vocab_size, num_out, num_heads, dv, dk, num_encoders, num_decoders):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(num_heads=num_heads, dk=dk, dv=dv, model_dim=model_dim, num_encoders=num_encoders)\n",
    "        self.decoder = Decoder(num_heads=num_heads, dk=dk, dv=dv, model_dim=model_dim, num_decoders=num_decoders, max_length=max_length)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        self.linear = nn.Linear(in_features=model_dim, out_features=num_out)\n",
    "        self.embedding = Embedding(batch_size=batch_size, model_dim=model_dim, max_length=max_length, n_embedding=vocab_size)\n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x_encoder = self.encoder(x)\n",
    "        x = self.decoder(x, x_encoder)\n",
    "        x = self.linear(x)\n",
    "        x = self.softmax(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 100, 32000])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 16\n",
    "model_dim = 512\n",
    "max_length = 100\n",
    "vocab_size = 32000\n",
    "num_out = vocab_size\n",
    "num_heads = 8\n",
    "dv = 64\n",
    "dk = 64\n",
    "num_encoders = 6\n",
    "num_decoders = 6\n",
    "\n",
    "x = torch.randint(0, vocab_size, (batch_size, max_length))\n",
    "\n",
    "MyTransformer = Transformer(\n",
    "    batch_size=batch_size,\n",
    "    model_dim=model_dim,\n",
    "    max_length=max_length,\n",
    "    vocab_size=vocab_size,\n",
    "    num_out=num_out,\n",
    "    num_heads=num_heads,\n",
    "    dv=dv,\n",
    "    dk=dk,\n",
    "    num_encoders=num_encoders,\n",
    "    num_decoders=num_decoders\n",
    ")\n",
    "out = MyTransformer(x)\n",
    "out.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "too many indices for tensor of dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m MyPositionalEncoding \u001b[38;5;241m=\u001b[39m PositionalEncoding(batch_size\u001b[38;5;241m=\u001b[39mbatch_size, model_dim\u001b[38;5;241m=\u001b[39mmodel_dim, max_length\u001b[38;5;241m=\u001b[39mmax_length)\n\u001b[1;32m      3\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m10\u001b[39m))\n\u001b[0;32m----> 4\u001b[0m plt\u001b[38;5;241m.\u001b[39mimshow(MyPositionalEncoding\u001b[38;5;241m.\u001b[39mMat[\u001b[38;5;241m0\u001b[39m, :\u001b[38;5;241m100\u001b[39m, :\u001b[38;5;241m100\u001b[39m], aspect\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mIndexError\u001b[0m: too many indices for tensor of dimension 2"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "MyPositionalEncoding = PositionalEncoding(batch_size=batch_size, model_dim=model_dim, max_length=max_length)\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(MyPositionalEncoding.Mat[0, :100, :100], aspect=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder.encoders.0.attention.attention_heads.0.K.weight: 32768 parameters\n",
      "encoder.encoders.0.attention.attention_heads.0.K.bias: 64 parameters\n",
      "encoder.encoders.0.attention.attention_heads.0.Q.weight: 32768 parameters\n",
      "encoder.encoders.0.attention.attention_heads.0.Q.bias: 64 parameters\n",
      "encoder.encoders.0.attention.attention_heads.0.V.weight: 32768 parameters\n",
      "encoder.encoders.0.attention.attention_heads.0.V.bias: 64 parameters\n",
      "encoder.encoders.0.attention.attention_heads.1.K.weight: 32768 parameters\n",
      "encoder.encoders.0.attention.attention_heads.1.K.bias: 64 parameters\n",
      "encoder.encoders.0.attention.attention_heads.1.Q.weight: 32768 parameters\n",
      "encoder.encoders.0.attention.attention_heads.1.Q.bias: 64 parameters\n",
      "encoder.encoders.0.attention.attention_heads.1.V.weight: 32768 parameters\n",
      "encoder.encoders.0.attention.attention_heads.1.V.bias: 64 parameters\n",
      "encoder.encoders.0.attention.attention_heads.2.K.weight: 32768 parameters\n",
      "encoder.encoders.0.attention.attention_heads.2.K.bias: 64 parameters\n",
      "encoder.encoders.0.attention.attention_heads.2.Q.weight: 32768 parameters\n",
      "encoder.encoders.0.attention.attention_heads.2.Q.bias: 64 parameters\n",
      "encoder.encoders.0.attention.attention_heads.2.V.weight: 32768 parameters\n",
      "encoder.encoders.0.attention.attention_heads.2.V.bias: 64 parameters\n",
      "encoder.encoders.0.attention.attention_heads.3.K.weight: 32768 parameters\n",
      "encoder.encoders.0.attention.attention_heads.3.K.bias: 64 parameters\n",
      "encoder.encoders.0.attention.attention_heads.3.Q.weight: 32768 parameters\n",
      "encoder.encoders.0.attention.attention_heads.3.Q.bias: 64 parameters\n",
      "encoder.encoders.0.attention.attention_heads.3.V.weight: 32768 parameters\n",
      "encoder.encoders.0.attention.attention_heads.3.V.bias: 64 parameters\n",
      "encoder.encoders.0.attention.attention_heads.4.K.weight: 32768 parameters\n",
      "encoder.encoders.0.attention.attention_heads.4.K.bias: 64 parameters\n",
      "encoder.encoders.0.attention.attention_heads.4.Q.weight: 32768 parameters\n",
      "encoder.encoders.0.attention.attention_heads.4.Q.bias: 64 parameters\n",
      "encoder.encoders.0.attention.attention_heads.4.V.weight: 32768 parameters\n",
      "encoder.encoders.0.attention.attention_heads.4.V.bias: 64 parameters\n",
      "encoder.encoders.0.attention.attention_heads.5.K.weight: 32768 parameters\n",
      "encoder.encoders.0.attention.attention_heads.5.K.bias: 64 parameters\n",
      "encoder.encoders.0.attention.attention_heads.5.Q.weight: 32768 parameters\n",
      "encoder.encoders.0.attention.attention_heads.5.Q.bias: 64 parameters\n",
      "encoder.encoders.0.attention.attention_heads.5.V.weight: 32768 parameters\n",
      "encoder.encoders.0.attention.attention_heads.5.V.bias: 64 parameters\n",
      "encoder.encoders.0.attention.attention_heads.6.K.weight: 32768 parameters\n",
      "encoder.encoders.0.attention.attention_heads.6.K.bias: 64 parameters\n",
      "encoder.encoders.0.attention.attention_heads.6.Q.weight: 32768 parameters\n",
      "encoder.encoders.0.attention.attention_heads.6.Q.bias: 64 parameters\n",
      "encoder.encoders.0.attention.attention_heads.6.V.weight: 32768 parameters\n",
      "encoder.encoders.0.attention.attention_heads.6.V.bias: 64 parameters\n",
      "encoder.encoders.0.attention.attention_heads.7.K.weight: 32768 parameters\n",
      "encoder.encoders.0.attention.attention_heads.7.K.bias: 64 parameters\n",
      "encoder.encoders.0.attention.attention_heads.7.Q.weight: 32768 parameters\n",
      "encoder.encoders.0.attention.attention_heads.7.Q.bias: 64 parameters\n",
      "encoder.encoders.0.attention.attention_heads.7.V.weight: 32768 parameters\n",
      "encoder.encoders.0.attention.attention_heads.7.V.bias: 64 parameters\n",
      "encoder.encoders.0.attention.WO.weight: 262144 parameters\n",
      "encoder.encoders.0.attention.WO.bias: 512 parameters\n",
      "encoder.encoders.0.layerNorm.weight: 512 parameters\n",
      "encoder.encoders.0.layerNorm.bias: 512 parameters\n",
      "encoder.encoders.0.ff.0.weight: 1048576 parameters\n",
      "encoder.encoders.0.ff.0.bias: 2048 parameters\n",
      "encoder.encoders.0.ff.2.weight: 1048576 parameters\n",
      "encoder.encoders.0.ff.2.bias: 512 parameters\n",
      "encoder.encoders.1.attention.attention_heads.0.K.weight: 32768 parameters\n",
      "encoder.encoders.1.attention.attention_heads.0.K.bias: 64 parameters\n",
      "encoder.encoders.1.attention.attention_heads.0.Q.weight: 32768 parameters\n",
      "encoder.encoders.1.attention.attention_heads.0.Q.bias: 64 parameters\n",
      "encoder.encoders.1.attention.attention_heads.0.V.weight: 32768 parameters\n",
      "encoder.encoders.1.attention.attention_heads.0.V.bias: 64 parameters\n",
      "encoder.encoders.1.attention.attention_heads.1.K.weight: 32768 parameters\n",
      "encoder.encoders.1.attention.attention_heads.1.K.bias: 64 parameters\n",
      "encoder.encoders.1.attention.attention_heads.1.Q.weight: 32768 parameters\n",
      "encoder.encoders.1.attention.attention_heads.1.Q.bias: 64 parameters\n",
      "encoder.encoders.1.attention.attention_heads.1.V.weight: 32768 parameters\n",
      "encoder.encoders.1.attention.attention_heads.1.V.bias: 64 parameters\n",
      "encoder.encoders.1.attention.attention_heads.2.K.weight: 32768 parameters\n",
      "encoder.encoders.1.attention.attention_heads.2.K.bias: 64 parameters\n",
      "encoder.encoders.1.attention.attention_heads.2.Q.weight: 32768 parameters\n",
      "encoder.encoders.1.attention.attention_heads.2.Q.bias: 64 parameters\n",
      "encoder.encoders.1.attention.attention_heads.2.V.weight: 32768 parameters\n",
      "encoder.encoders.1.attention.attention_heads.2.V.bias: 64 parameters\n",
      "encoder.encoders.1.attention.attention_heads.3.K.weight: 32768 parameters\n",
      "encoder.encoders.1.attention.attention_heads.3.K.bias: 64 parameters\n",
      "encoder.encoders.1.attention.attention_heads.3.Q.weight: 32768 parameters\n",
      "encoder.encoders.1.attention.attention_heads.3.Q.bias: 64 parameters\n",
      "encoder.encoders.1.attention.attention_heads.3.V.weight: 32768 parameters\n",
      "encoder.encoders.1.attention.attention_heads.3.V.bias: 64 parameters\n",
      "encoder.encoders.1.attention.attention_heads.4.K.weight: 32768 parameters\n",
      "encoder.encoders.1.attention.attention_heads.4.K.bias: 64 parameters\n",
      "encoder.encoders.1.attention.attention_heads.4.Q.weight: 32768 parameters\n",
      "encoder.encoders.1.attention.attention_heads.4.Q.bias: 64 parameters\n",
      "encoder.encoders.1.attention.attention_heads.4.V.weight: 32768 parameters\n",
      "encoder.encoders.1.attention.attention_heads.4.V.bias: 64 parameters\n",
      "encoder.encoders.1.attention.attention_heads.5.K.weight: 32768 parameters\n",
      "encoder.encoders.1.attention.attention_heads.5.K.bias: 64 parameters\n",
      "encoder.encoders.1.attention.attention_heads.5.Q.weight: 32768 parameters\n",
      "encoder.encoders.1.attention.attention_heads.5.Q.bias: 64 parameters\n",
      "encoder.encoders.1.attention.attention_heads.5.V.weight: 32768 parameters\n",
      "encoder.encoders.1.attention.attention_heads.5.V.bias: 64 parameters\n",
      "encoder.encoders.1.attention.attention_heads.6.K.weight: 32768 parameters\n",
      "encoder.encoders.1.attention.attention_heads.6.K.bias: 64 parameters\n",
      "encoder.encoders.1.attention.attention_heads.6.Q.weight: 32768 parameters\n",
      "encoder.encoders.1.attention.attention_heads.6.Q.bias: 64 parameters\n",
      "encoder.encoders.1.attention.attention_heads.6.V.weight: 32768 parameters\n",
      "encoder.encoders.1.attention.attention_heads.6.V.bias: 64 parameters\n",
      "encoder.encoders.1.attention.attention_heads.7.K.weight: 32768 parameters\n",
      "encoder.encoders.1.attention.attention_heads.7.K.bias: 64 parameters\n",
      "encoder.encoders.1.attention.attention_heads.7.Q.weight: 32768 parameters\n",
      "encoder.encoders.1.attention.attention_heads.7.Q.bias: 64 parameters\n",
      "encoder.encoders.1.attention.attention_heads.7.V.weight: 32768 parameters\n",
      "encoder.encoders.1.attention.attention_heads.7.V.bias: 64 parameters\n",
      "encoder.encoders.1.attention.WO.weight: 262144 parameters\n",
      "encoder.encoders.1.attention.WO.bias: 512 parameters\n",
      "encoder.encoders.1.layerNorm.weight: 512 parameters\n",
      "encoder.encoders.1.layerNorm.bias: 512 parameters\n",
      "encoder.encoders.1.ff.0.weight: 1048576 parameters\n",
      "encoder.encoders.1.ff.0.bias: 2048 parameters\n",
      "encoder.encoders.1.ff.2.weight: 1048576 parameters\n",
      "encoder.encoders.1.ff.2.bias: 512 parameters\n",
      "encoder.encoders.2.attention.attention_heads.0.K.weight: 32768 parameters\n",
      "encoder.encoders.2.attention.attention_heads.0.K.bias: 64 parameters\n",
      "encoder.encoders.2.attention.attention_heads.0.Q.weight: 32768 parameters\n",
      "encoder.encoders.2.attention.attention_heads.0.Q.bias: 64 parameters\n",
      "encoder.encoders.2.attention.attention_heads.0.V.weight: 32768 parameters\n",
      "encoder.encoders.2.attention.attention_heads.0.V.bias: 64 parameters\n",
      "encoder.encoders.2.attention.attention_heads.1.K.weight: 32768 parameters\n",
      "encoder.encoders.2.attention.attention_heads.1.K.bias: 64 parameters\n",
      "encoder.encoders.2.attention.attention_heads.1.Q.weight: 32768 parameters\n",
      "encoder.encoders.2.attention.attention_heads.1.Q.bias: 64 parameters\n",
      "encoder.encoders.2.attention.attention_heads.1.V.weight: 32768 parameters\n",
      "encoder.encoders.2.attention.attention_heads.1.V.bias: 64 parameters\n",
      "encoder.encoders.2.attention.attention_heads.2.K.weight: 32768 parameters\n",
      "encoder.encoders.2.attention.attention_heads.2.K.bias: 64 parameters\n",
      "encoder.encoders.2.attention.attention_heads.2.Q.weight: 32768 parameters\n",
      "encoder.encoders.2.attention.attention_heads.2.Q.bias: 64 parameters\n",
      "encoder.encoders.2.attention.attention_heads.2.V.weight: 32768 parameters\n",
      "encoder.encoders.2.attention.attention_heads.2.V.bias: 64 parameters\n",
      "encoder.encoders.2.attention.attention_heads.3.K.weight: 32768 parameters\n",
      "encoder.encoders.2.attention.attention_heads.3.K.bias: 64 parameters\n",
      "encoder.encoders.2.attention.attention_heads.3.Q.weight: 32768 parameters\n",
      "encoder.encoders.2.attention.attention_heads.3.Q.bias: 64 parameters\n",
      "encoder.encoders.2.attention.attention_heads.3.V.weight: 32768 parameters\n",
      "encoder.encoders.2.attention.attention_heads.3.V.bias: 64 parameters\n",
      "encoder.encoders.2.attention.attention_heads.4.K.weight: 32768 parameters\n",
      "encoder.encoders.2.attention.attention_heads.4.K.bias: 64 parameters\n",
      "encoder.encoders.2.attention.attention_heads.4.Q.weight: 32768 parameters\n",
      "encoder.encoders.2.attention.attention_heads.4.Q.bias: 64 parameters\n",
      "encoder.encoders.2.attention.attention_heads.4.V.weight: 32768 parameters\n",
      "encoder.encoders.2.attention.attention_heads.4.V.bias: 64 parameters\n",
      "encoder.encoders.2.attention.attention_heads.5.K.weight: 32768 parameters\n",
      "encoder.encoders.2.attention.attention_heads.5.K.bias: 64 parameters\n",
      "encoder.encoders.2.attention.attention_heads.5.Q.weight: 32768 parameters\n",
      "encoder.encoders.2.attention.attention_heads.5.Q.bias: 64 parameters\n",
      "encoder.encoders.2.attention.attention_heads.5.V.weight: 32768 parameters\n",
      "encoder.encoders.2.attention.attention_heads.5.V.bias: 64 parameters\n",
      "encoder.encoders.2.attention.attention_heads.6.K.weight: 32768 parameters\n",
      "encoder.encoders.2.attention.attention_heads.6.K.bias: 64 parameters\n",
      "encoder.encoders.2.attention.attention_heads.6.Q.weight: 32768 parameters\n",
      "encoder.encoders.2.attention.attention_heads.6.Q.bias: 64 parameters\n",
      "encoder.encoders.2.attention.attention_heads.6.V.weight: 32768 parameters\n",
      "encoder.encoders.2.attention.attention_heads.6.V.bias: 64 parameters\n",
      "encoder.encoders.2.attention.attention_heads.7.K.weight: 32768 parameters\n",
      "encoder.encoders.2.attention.attention_heads.7.K.bias: 64 parameters\n",
      "encoder.encoders.2.attention.attention_heads.7.Q.weight: 32768 parameters\n",
      "encoder.encoders.2.attention.attention_heads.7.Q.bias: 64 parameters\n",
      "encoder.encoders.2.attention.attention_heads.7.V.weight: 32768 parameters\n",
      "encoder.encoders.2.attention.attention_heads.7.V.bias: 64 parameters\n",
      "encoder.encoders.2.attention.WO.weight: 262144 parameters\n",
      "encoder.encoders.2.attention.WO.bias: 512 parameters\n",
      "encoder.encoders.2.layerNorm.weight: 512 parameters\n",
      "encoder.encoders.2.layerNorm.bias: 512 parameters\n",
      "encoder.encoders.2.ff.0.weight: 1048576 parameters\n",
      "encoder.encoders.2.ff.0.bias: 2048 parameters\n",
      "encoder.encoders.2.ff.2.weight: 1048576 parameters\n",
      "encoder.encoders.2.ff.2.bias: 512 parameters\n",
      "encoder.encoders.3.attention.attention_heads.0.K.weight: 32768 parameters\n",
      "encoder.encoders.3.attention.attention_heads.0.K.bias: 64 parameters\n",
      "encoder.encoders.3.attention.attention_heads.0.Q.weight: 32768 parameters\n",
      "encoder.encoders.3.attention.attention_heads.0.Q.bias: 64 parameters\n",
      "encoder.encoders.3.attention.attention_heads.0.V.weight: 32768 parameters\n",
      "encoder.encoders.3.attention.attention_heads.0.V.bias: 64 parameters\n",
      "encoder.encoders.3.attention.attention_heads.1.K.weight: 32768 parameters\n",
      "encoder.encoders.3.attention.attention_heads.1.K.bias: 64 parameters\n",
      "encoder.encoders.3.attention.attention_heads.1.Q.weight: 32768 parameters\n",
      "encoder.encoders.3.attention.attention_heads.1.Q.bias: 64 parameters\n",
      "encoder.encoders.3.attention.attention_heads.1.V.weight: 32768 parameters\n",
      "encoder.encoders.3.attention.attention_heads.1.V.bias: 64 parameters\n",
      "encoder.encoders.3.attention.attention_heads.2.K.weight: 32768 parameters\n",
      "encoder.encoders.3.attention.attention_heads.2.K.bias: 64 parameters\n",
      "encoder.encoders.3.attention.attention_heads.2.Q.weight: 32768 parameters\n",
      "encoder.encoders.3.attention.attention_heads.2.Q.bias: 64 parameters\n",
      "encoder.encoders.3.attention.attention_heads.2.V.weight: 32768 parameters\n",
      "encoder.encoders.3.attention.attention_heads.2.V.bias: 64 parameters\n",
      "encoder.encoders.3.attention.attention_heads.3.K.weight: 32768 parameters\n",
      "encoder.encoders.3.attention.attention_heads.3.K.bias: 64 parameters\n",
      "encoder.encoders.3.attention.attention_heads.3.Q.weight: 32768 parameters\n",
      "encoder.encoders.3.attention.attention_heads.3.Q.bias: 64 parameters\n",
      "encoder.encoders.3.attention.attention_heads.3.V.weight: 32768 parameters\n",
      "encoder.encoders.3.attention.attention_heads.3.V.bias: 64 parameters\n",
      "encoder.encoders.3.attention.attention_heads.4.K.weight: 32768 parameters\n",
      "encoder.encoders.3.attention.attention_heads.4.K.bias: 64 parameters\n",
      "encoder.encoders.3.attention.attention_heads.4.Q.weight: 32768 parameters\n",
      "encoder.encoders.3.attention.attention_heads.4.Q.bias: 64 parameters\n",
      "encoder.encoders.3.attention.attention_heads.4.V.weight: 32768 parameters\n",
      "encoder.encoders.3.attention.attention_heads.4.V.bias: 64 parameters\n",
      "encoder.encoders.3.attention.attention_heads.5.K.weight: 32768 parameters\n",
      "encoder.encoders.3.attention.attention_heads.5.K.bias: 64 parameters\n",
      "encoder.encoders.3.attention.attention_heads.5.Q.weight: 32768 parameters\n",
      "encoder.encoders.3.attention.attention_heads.5.Q.bias: 64 parameters\n",
      "encoder.encoders.3.attention.attention_heads.5.V.weight: 32768 parameters\n",
      "encoder.encoders.3.attention.attention_heads.5.V.bias: 64 parameters\n",
      "encoder.encoders.3.attention.attention_heads.6.K.weight: 32768 parameters\n",
      "encoder.encoders.3.attention.attention_heads.6.K.bias: 64 parameters\n",
      "encoder.encoders.3.attention.attention_heads.6.Q.weight: 32768 parameters\n",
      "encoder.encoders.3.attention.attention_heads.6.Q.bias: 64 parameters\n",
      "encoder.encoders.3.attention.attention_heads.6.V.weight: 32768 parameters\n",
      "encoder.encoders.3.attention.attention_heads.6.V.bias: 64 parameters\n",
      "encoder.encoders.3.attention.attention_heads.7.K.weight: 32768 parameters\n",
      "encoder.encoders.3.attention.attention_heads.7.K.bias: 64 parameters\n",
      "encoder.encoders.3.attention.attention_heads.7.Q.weight: 32768 parameters\n",
      "encoder.encoders.3.attention.attention_heads.7.Q.bias: 64 parameters\n",
      "encoder.encoders.3.attention.attention_heads.7.V.weight: 32768 parameters\n",
      "encoder.encoders.3.attention.attention_heads.7.V.bias: 64 parameters\n",
      "encoder.encoders.3.attention.WO.weight: 262144 parameters\n",
      "encoder.encoders.3.attention.WO.bias: 512 parameters\n",
      "encoder.encoders.3.layerNorm.weight: 512 parameters\n",
      "encoder.encoders.3.layerNorm.bias: 512 parameters\n",
      "encoder.encoders.3.ff.0.weight: 1048576 parameters\n",
      "encoder.encoders.3.ff.0.bias: 2048 parameters\n",
      "encoder.encoders.3.ff.2.weight: 1048576 parameters\n",
      "encoder.encoders.3.ff.2.bias: 512 parameters\n",
      "encoder.encoders.4.attention.attention_heads.0.K.weight: 32768 parameters\n",
      "encoder.encoders.4.attention.attention_heads.0.K.bias: 64 parameters\n",
      "encoder.encoders.4.attention.attention_heads.0.Q.weight: 32768 parameters\n",
      "encoder.encoders.4.attention.attention_heads.0.Q.bias: 64 parameters\n",
      "encoder.encoders.4.attention.attention_heads.0.V.weight: 32768 parameters\n",
      "encoder.encoders.4.attention.attention_heads.0.V.bias: 64 parameters\n",
      "encoder.encoders.4.attention.attention_heads.1.K.weight: 32768 parameters\n",
      "encoder.encoders.4.attention.attention_heads.1.K.bias: 64 parameters\n",
      "encoder.encoders.4.attention.attention_heads.1.Q.weight: 32768 parameters\n",
      "encoder.encoders.4.attention.attention_heads.1.Q.bias: 64 parameters\n",
      "encoder.encoders.4.attention.attention_heads.1.V.weight: 32768 parameters\n",
      "encoder.encoders.4.attention.attention_heads.1.V.bias: 64 parameters\n",
      "encoder.encoders.4.attention.attention_heads.2.K.weight: 32768 parameters\n",
      "encoder.encoders.4.attention.attention_heads.2.K.bias: 64 parameters\n",
      "encoder.encoders.4.attention.attention_heads.2.Q.weight: 32768 parameters\n",
      "encoder.encoders.4.attention.attention_heads.2.Q.bias: 64 parameters\n",
      "encoder.encoders.4.attention.attention_heads.2.V.weight: 32768 parameters\n",
      "encoder.encoders.4.attention.attention_heads.2.V.bias: 64 parameters\n",
      "encoder.encoders.4.attention.attention_heads.3.K.weight: 32768 parameters\n",
      "encoder.encoders.4.attention.attention_heads.3.K.bias: 64 parameters\n",
      "encoder.encoders.4.attention.attention_heads.3.Q.weight: 32768 parameters\n",
      "encoder.encoders.4.attention.attention_heads.3.Q.bias: 64 parameters\n",
      "encoder.encoders.4.attention.attention_heads.3.V.weight: 32768 parameters\n",
      "encoder.encoders.4.attention.attention_heads.3.V.bias: 64 parameters\n",
      "encoder.encoders.4.attention.attention_heads.4.K.weight: 32768 parameters\n",
      "encoder.encoders.4.attention.attention_heads.4.K.bias: 64 parameters\n",
      "encoder.encoders.4.attention.attention_heads.4.Q.weight: 32768 parameters\n",
      "encoder.encoders.4.attention.attention_heads.4.Q.bias: 64 parameters\n",
      "encoder.encoders.4.attention.attention_heads.4.V.weight: 32768 parameters\n",
      "encoder.encoders.4.attention.attention_heads.4.V.bias: 64 parameters\n",
      "encoder.encoders.4.attention.attention_heads.5.K.weight: 32768 parameters\n",
      "encoder.encoders.4.attention.attention_heads.5.K.bias: 64 parameters\n",
      "encoder.encoders.4.attention.attention_heads.5.Q.weight: 32768 parameters\n",
      "encoder.encoders.4.attention.attention_heads.5.Q.bias: 64 parameters\n",
      "encoder.encoders.4.attention.attention_heads.5.V.weight: 32768 parameters\n",
      "encoder.encoders.4.attention.attention_heads.5.V.bias: 64 parameters\n",
      "encoder.encoders.4.attention.attention_heads.6.K.weight: 32768 parameters\n",
      "encoder.encoders.4.attention.attention_heads.6.K.bias: 64 parameters\n",
      "encoder.encoders.4.attention.attention_heads.6.Q.weight: 32768 parameters\n",
      "encoder.encoders.4.attention.attention_heads.6.Q.bias: 64 parameters\n",
      "encoder.encoders.4.attention.attention_heads.6.V.weight: 32768 parameters\n",
      "encoder.encoders.4.attention.attention_heads.6.V.bias: 64 parameters\n",
      "encoder.encoders.4.attention.attention_heads.7.K.weight: 32768 parameters\n",
      "encoder.encoders.4.attention.attention_heads.7.K.bias: 64 parameters\n",
      "encoder.encoders.4.attention.attention_heads.7.Q.weight: 32768 parameters\n",
      "encoder.encoders.4.attention.attention_heads.7.Q.bias: 64 parameters\n",
      "encoder.encoders.4.attention.attention_heads.7.V.weight: 32768 parameters\n",
      "encoder.encoders.4.attention.attention_heads.7.V.bias: 64 parameters\n",
      "encoder.encoders.4.attention.WO.weight: 262144 parameters\n",
      "encoder.encoders.4.attention.WO.bias: 512 parameters\n",
      "encoder.encoders.4.layerNorm.weight: 512 parameters\n",
      "encoder.encoders.4.layerNorm.bias: 512 parameters\n",
      "encoder.encoders.4.ff.0.weight: 1048576 parameters\n",
      "encoder.encoders.4.ff.0.bias: 2048 parameters\n",
      "encoder.encoders.4.ff.2.weight: 1048576 parameters\n",
      "encoder.encoders.4.ff.2.bias: 512 parameters\n",
      "encoder.encoders.5.attention.attention_heads.0.K.weight: 32768 parameters\n",
      "encoder.encoders.5.attention.attention_heads.0.K.bias: 64 parameters\n",
      "encoder.encoders.5.attention.attention_heads.0.Q.weight: 32768 parameters\n",
      "encoder.encoders.5.attention.attention_heads.0.Q.bias: 64 parameters\n",
      "encoder.encoders.5.attention.attention_heads.0.V.weight: 32768 parameters\n",
      "encoder.encoders.5.attention.attention_heads.0.V.bias: 64 parameters\n",
      "encoder.encoders.5.attention.attention_heads.1.K.weight: 32768 parameters\n",
      "encoder.encoders.5.attention.attention_heads.1.K.bias: 64 parameters\n",
      "encoder.encoders.5.attention.attention_heads.1.Q.weight: 32768 parameters\n",
      "encoder.encoders.5.attention.attention_heads.1.Q.bias: 64 parameters\n",
      "encoder.encoders.5.attention.attention_heads.1.V.weight: 32768 parameters\n",
      "encoder.encoders.5.attention.attention_heads.1.V.bias: 64 parameters\n",
      "encoder.encoders.5.attention.attention_heads.2.K.weight: 32768 parameters\n",
      "encoder.encoders.5.attention.attention_heads.2.K.bias: 64 parameters\n",
      "encoder.encoders.5.attention.attention_heads.2.Q.weight: 32768 parameters\n",
      "encoder.encoders.5.attention.attention_heads.2.Q.bias: 64 parameters\n",
      "encoder.encoders.5.attention.attention_heads.2.V.weight: 32768 parameters\n",
      "encoder.encoders.5.attention.attention_heads.2.V.bias: 64 parameters\n",
      "encoder.encoders.5.attention.attention_heads.3.K.weight: 32768 parameters\n",
      "encoder.encoders.5.attention.attention_heads.3.K.bias: 64 parameters\n",
      "encoder.encoders.5.attention.attention_heads.3.Q.weight: 32768 parameters\n",
      "encoder.encoders.5.attention.attention_heads.3.Q.bias: 64 parameters\n",
      "encoder.encoders.5.attention.attention_heads.3.V.weight: 32768 parameters\n",
      "encoder.encoders.5.attention.attention_heads.3.V.bias: 64 parameters\n",
      "encoder.encoders.5.attention.attention_heads.4.K.weight: 32768 parameters\n",
      "encoder.encoders.5.attention.attention_heads.4.K.bias: 64 parameters\n",
      "encoder.encoders.5.attention.attention_heads.4.Q.weight: 32768 parameters\n",
      "encoder.encoders.5.attention.attention_heads.4.Q.bias: 64 parameters\n",
      "encoder.encoders.5.attention.attention_heads.4.V.weight: 32768 parameters\n",
      "encoder.encoders.5.attention.attention_heads.4.V.bias: 64 parameters\n",
      "encoder.encoders.5.attention.attention_heads.5.K.weight: 32768 parameters\n",
      "encoder.encoders.5.attention.attention_heads.5.K.bias: 64 parameters\n",
      "encoder.encoders.5.attention.attention_heads.5.Q.weight: 32768 parameters\n",
      "encoder.encoders.5.attention.attention_heads.5.Q.bias: 64 parameters\n",
      "encoder.encoders.5.attention.attention_heads.5.V.weight: 32768 parameters\n",
      "encoder.encoders.5.attention.attention_heads.5.V.bias: 64 parameters\n",
      "encoder.encoders.5.attention.attention_heads.6.K.weight: 32768 parameters\n",
      "encoder.encoders.5.attention.attention_heads.6.K.bias: 64 parameters\n",
      "encoder.encoders.5.attention.attention_heads.6.Q.weight: 32768 parameters\n",
      "encoder.encoders.5.attention.attention_heads.6.Q.bias: 64 parameters\n",
      "encoder.encoders.5.attention.attention_heads.6.V.weight: 32768 parameters\n",
      "encoder.encoders.5.attention.attention_heads.6.V.bias: 64 parameters\n",
      "encoder.encoders.5.attention.attention_heads.7.K.weight: 32768 parameters\n",
      "encoder.encoders.5.attention.attention_heads.7.K.bias: 64 parameters\n",
      "encoder.encoders.5.attention.attention_heads.7.Q.weight: 32768 parameters\n",
      "encoder.encoders.5.attention.attention_heads.7.Q.bias: 64 parameters\n",
      "encoder.encoders.5.attention.attention_heads.7.V.weight: 32768 parameters\n",
      "encoder.encoders.5.attention.attention_heads.7.V.bias: 64 parameters\n",
      "encoder.encoders.5.attention.WO.weight: 262144 parameters\n",
      "encoder.encoders.5.attention.WO.bias: 512 parameters\n",
      "encoder.encoders.5.layerNorm.weight: 512 parameters\n",
      "encoder.encoders.5.layerNorm.bias: 512 parameters\n",
      "encoder.encoders.5.ff.0.weight: 1048576 parameters\n",
      "encoder.encoders.5.ff.0.bias: 2048 parameters\n",
      "encoder.encoders.5.ff.2.weight: 1048576 parameters\n",
      "encoder.encoders.5.ff.2.bias: 512 parameters\n",
      "decoder.decoders.modules_list.0.masked_attention.attention_heads.0.K.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.0.masked_attention.attention_heads.0.K.bias: 64 parameters\n",
      "decoder.decoders.modules_list.0.masked_attention.attention_heads.0.Q.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.0.masked_attention.attention_heads.0.Q.bias: 64 parameters\n",
      "decoder.decoders.modules_list.0.masked_attention.attention_heads.0.V.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.0.masked_attention.attention_heads.0.V.bias: 64 parameters\n",
      "decoder.decoders.modules_list.0.masked_attention.attention_heads.1.K.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.0.masked_attention.attention_heads.1.K.bias: 64 parameters\n",
      "decoder.decoders.modules_list.0.masked_attention.attention_heads.1.Q.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.0.masked_attention.attention_heads.1.Q.bias: 64 parameters\n",
      "decoder.decoders.modules_list.0.masked_attention.attention_heads.1.V.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.0.masked_attention.attention_heads.1.V.bias: 64 parameters\n",
      "decoder.decoders.modules_list.0.masked_attention.attention_heads.2.K.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.0.masked_attention.attention_heads.2.K.bias: 64 parameters\n",
      "decoder.decoders.modules_list.0.masked_attention.attention_heads.2.Q.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.0.masked_attention.attention_heads.2.Q.bias: 64 parameters\n",
      "decoder.decoders.modules_list.0.masked_attention.attention_heads.2.V.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.0.masked_attention.attention_heads.2.V.bias: 64 parameters\n",
      "decoder.decoders.modules_list.0.masked_attention.attention_heads.3.K.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.0.masked_attention.attention_heads.3.K.bias: 64 parameters\n",
      "decoder.decoders.modules_list.0.masked_attention.attention_heads.3.Q.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.0.masked_attention.attention_heads.3.Q.bias: 64 parameters\n",
      "decoder.decoders.modules_list.0.masked_attention.attention_heads.3.V.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.0.masked_attention.attention_heads.3.V.bias: 64 parameters\n",
      "decoder.decoders.modules_list.0.masked_attention.attention_heads.4.K.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.0.masked_attention.attention_heads.4.K.bias: 64 parameters\n",
      "decoder.decoders.modules_list.0.masked_attention.attention_heads.4.Q.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.0.masked_attention.attention_heads.4.Q.bias: 64 parameters\n",
      "decoder.decoders.modules_list.0.masked_attention.attention_heads.4.V.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.0.masked_attention.attention_heads.4.V.bias: 64 parameters\n",
      "decoder.decoders.modules_list.0.masked_attention.attention_heads.5.K.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.0.masked_attention.attention_heads.5.K.bias: 64 parameters\n",
      "decoder.decoders.modules_list.0.masked_attention.attention_heads.5.Q.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.0.masked_attention.attention_heads.5.Q.bias: 64 parameters\n",
      "decoder.decoders.modules_list.0.masked_attention.attention_heads.5.V.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.0.masked_attention.attention_heads.5.V.bias: 64 parameters\n",
      "decoder.decoders.modules_list.0.masked_attention.attention_heads.6.K.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.0.masked_attention.attention_heads.6.K.bias: 64 parameters\n",
      "decoder.decoders.modules_list.0.masked_attention.attention_heads.6.Q.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.0.masked_attention.attention_heads.6.Q.bias: 64 parameters\n",
      "decoder.decoders.modules_list.0.masked_attention.attention_heads.6.V.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.0.masked_attention.attention_heads.6.V.bias: 64 parameters\n",
      "decoder.decoders.modules_list.0.masked_attention.attention_heads.7.K.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.0.masked_attention.attention_heads.7.K.bias: 64 parameters\n",
      "decoder.decoders.modules_list.0.masked_attention.attention_heads.7.Q.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.0.masked_attention.attention_heads.7.Q.bias: 64 parameters\n",
      "decoder.decoders.modules_list.0.masked_attention.attention_heads.7.V.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.0.masked_attention.attention_heads.7.V.bias: 64 parameters\n",
      "decoder.decoders.modules_list.0.masked_attention.WO.weight: 262144 parameters\n",
      "decoder.decoders.modules_list.0.masked_attention.WO.bias: 512 parameters\n",
      "decoder.decoders.modules_list.0.mixed_attention.attention_heads.0.K.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.0.mixed_attention.attention_heads.0.K.bias: 64 parameters\n",
      "decoder.decoders.modules_list.0.mixed_attention.attention_heads.0.Q.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.0.mixed_attention.attention_heads.0.Q.bias: 64 parameters\n",
      "decoder.decoders.modules_list.0.mixed_attention.attention_heads.0.V.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.0.mixed_attention.attention_heads.0.V.bias: 64 parameters\n",
      "decoder.decoders.modules_list.0.mixed_attention.attention_heads.1.K.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.0.mixed_attention.attention_heads.1.K.bias: 64 parameters\n",
      "decoder.decoders.modules_list.0.mixed_attention.attention_heads.1.Q.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.0.mixed_attention.attention_heads.1.Q.bias: 64 parameters\n",
      "decoder.decoders.modules_list.0.mixed_attention.attention_heads.1.V.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.0.mixed_attention.attention_heads.1.V.bias: 64 parameters\n",
      "decoder.decoders.modules_list.0.mixed_attention.attention_heads.2.K.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.0.mixed_attention.attention_heads.2.K.bias: 64 parameters\n",
      "decoder.decoders.modules_list.0.mixed_attention.attention_heads.2.Q.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.0.mixed_attention.attention_heads.2.Q.bias: 64 parameters\n",
      "decoder.decoders.modules_list.0.mixed_attention.attention_heads.2.V.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.0.mixed_attention.attention_heads.2.V.bias: 64 parameters\n",
      "decoder.decoders.modules_list.0.mixed_attention.attention_heads.3.K.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.0.mixed_attention.attention_heads.3.K.bias: 64 parameters\n",
      "decoder.decoders.modules_list.0.mixed_attention.attention_heads.3.Q.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.0.mixed_attention.attention_heads.3.Q.bias: 64 parameters\n",
      "decoder.decoders.modules_list.0.mixed_attention.attention_heads.3.V.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.0.mixed_attention.attention_heads.3.V.bias: 64 parameters\n",
      "decoder.decoders.modules_list.0.mixed_attention.attention_heads.4.K.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.0.mixed_attention.attention_heads.4.K.bias: 64 parameters\n",
      "decoder.decoders.modules_list.0.mixed_attention.attention_heads.4.Q.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.0.mixed_attention.attention_heads.4.Q.bias: 64 parameters\n",
      "decoder.decoders.modules_list.0.mixed_attention.attention_heads.4.V.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.0.mixed_attention.attention_heads.4.V.bias: 64 parameters\n",
      "decoder.decoders.modules_list.0.mixed_attention.attention_heads.5.K.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.0.mixed_attention.attention_heads.5.K.bias: 64 parameters\n",
      "decoder.decoders.modules_list.0.mixed_attention.attention_heads.5.Q.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.0.mixed_attention.attention_heads.5.Q.bias: 64 parameters\n",
      "decoder.decoders.modules_list.0.mixed_attention.attention_heads.5.V.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.0.mixed_attention.attention_heads.5.V.bias: 64 parameters\n",
      "decoder.decoders.modules_list.0.mixed_attention.attention_heads.6.K.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.0.mixed_attention.attention_heads.6.K.bias: 64 parameters\n",
      "decoder.decoders.modules_list.0.mixed_attention.attention_heads.6.Q.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.0.mixed_attention.attention_heads.6.Q.bias: 64 parameters\n",
      "decoder.decoders.modules_list.0.mixed_attention.attention_heads.6.V.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.0.mixed_attention.attention_heads.6.V.bias: 64 parameters\n",
      "decoder.decoders.modules_list.0.mixed_attention.attention_heads.7.K.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.0.mixed_attention.attention_heads.7.K.bias: 64 parameters\n",
      "decoder.decoders.modules_list.0.mixed_attention.attention_heads.7.Q.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.0.mixed_attention.attention_heads.7.Q.bias: 64 parameters\n",
      "decoder.decoders.modules_list.0.mixed_attention.attention_heads.7.V.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.0.mixed_attention.attention_heads.7.V.bias: 64 parameters\n",
      "decoder.decoders.modules_list.0.mixed_attention.WO.weight: 262144 parameters\n",
      "decoder.decoders.modules_list.0.mixed_attention.WO.bias: 512 parameters\n",
      "decoder.decoders.modules_list.0.layerNorm.weight: 512 parameters\n",
      "decoder.decoders.modules_list.0.layerNorm.bias: 512 parameters\n",
      "decoder.decoders.modules_list.0.ff.0.weight: 1048576 parameters\n",
      "decoder.decoders.modules_list.0.ff.0.bias: 2048 parameters\n",
      "decoder.decoders.modules_list.0.ff.2.weight: 1048576 parameters\n",
      "decoder.decoders.modules_list.0.ff.2.bias: 512 parameters\n",
      "decoder.decoders.modules_list.1.masked_attention.attention_heads.0.K.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.1.masked_attention.attention_heads.0.K.bias: 64 parameters\n",
      "decoder.decoders.modules_list.1.masked_attention.attention_heads.0.Q.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.1.masked_attention.attention_heads.0.Q.bias: 64 parameters\n",
      "decoder.decoders.modules_list.1.masked_attention.attention_heads.0.V.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.1.masked_attention.attention_heads.0.V.bias: 64 parameters\n",
      "decoder.decoders.modules_list.1.masked_attention.attention_heads.1.K.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.1.masked_attention.attention_heads.1.K.bias: 64 parameters\n",
      "decoder.decoders.modules_list.1.masked_attention.attention_heads.1.Q.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.1.masked_attention.attention_heads.1.Q.bias: 64 parameters\n",
      "decoder.decoders.modules_list.1.masked_attention.attention_heads.1.V.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.1.masked_attention.attention_heads.1.V.bias: 64 parameters\n",
      "decoder.decoders.modules_list.1.masked_attention.attention_heads.2.K.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.1.masked_attention.attention_heads.2.K.bias: 64 parameters\n",
      "decoder.decoders.modules_list.1.masked_attention.attention_heads.2.Q.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.1.masked_attention.attention_heads.2.Q.bias: 64 parameters\n",
      "decoder.decoders.modules_list.1.masked_attention.attention_heads.2.V.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.1.masked_attention.attention_heads.2.V.bias: 64 parameters\n",
      "decoder.decoders.modules_list.1.masked_attention.attention_heads.3.K.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.1.masked_attention.attention_heads.3.K.bias: 64 parameters\n",
      "decoder.decoders.modules_list.1.masked_attention.attention_heads.3.Q.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.1.masked_attention.attention_heads.3.Q.bias: 64 parameters\n",
      "decoder.decoders.modules_list.1.masked_attention.attention_heads.3.V.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.1.masked_attention.attention_heads.3.V.bias: 64 parameters\n",
      "decoder.decoders.modules_list.1.masked_attention.attention_heads.4.K.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.1.masked_attention.attention_heads.4.K.bias: 64 parameters\n",
      "decoder.decoders.modules_list.1.masked_attention.attention_heads.4.Q.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.1.masked_attention.attention_heads.4.Q.bias: 64 parameters\n",
      "decoder.decoders.modules_list.1.masked_attention.attention_heads.4.V.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.1.masked_attention.attention_heads.4.V.bias: 64 parameters\n",
      "decoder.decoders.modules_list.1.masked_attention.attention_heads.5.K.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.1.masked_attention.attention_heads.5.K.bias: 64 parameters\n",
      "decoder.decoders.modules_list.1.masked_attention.attention_heads.5.Q.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.1.masked_attention.attention_heads.5.Q.bias: 64 parameters\n",
      "decoder.decoders.modules_list.1.masked_attention.attention_heads.5.V.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.1.masked_attention.attention_heads.5.V.bias: 64 parameters\n",
      "decoder.decoders.modules_list.1.masked_attention.attention_heads.6.K.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.1.masked_attention.attention_heads.6.K.bias: 64 parameters\n",
      "decoder.decoders.modules_list.1.masked_attention.attention_heads.6.Q.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.1.masked_attention.attention_heads.6.Q.bias: 64 parameters\n",
      "decoder.decoders.modules_list.1.masked_attention.attention_heads.6.V.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.1.masked_attention.attention_heads.6.V.bias: 64 parameters\n",
      "decoder.decoders.modules_list.1.masked_attention.attention_heads.7.K.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.1.masked_attention.attention_heads.7.K.bias: 64 parameters\n",
      "decoder.decoders.modules_list.1.masked_attention.attention_heads.7.Q.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.1.masked_attention.attention_heads.7.Q.bias: 64 parameters\n",
      "decoder.decoders.modules_list.1.masked_attention.attention_heads.7.V.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.1.masked_attention.attention_heads.7.V.bias: 64 parameters\n",
      "decoder.decoders.modules_list.1.masked_attention.WO.weight: 262144 parameters\n",
      "decoder.decoders.modules_list.1.masked_attention.WO.bias: 512 parameters\n",
      "decoder.decoders.modules_list.1.mixed_attention.attention_heads.0.K.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.1.mixed_attention.attention_heads.0.K.bias: 64 parameters\n",
      "decoder.decoders.modules_list.1.mixed_attention.attention_heads.0.Q.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.1.mixed_attention.attention_heads.0.Q.bias: 64 parameters\n",
      "decoder.decoders.modules_list.1.mixed_attention.attention_heads.0.V.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.1.mixed_attention.attention_heads.0.V.bias: 64 parameters\n",
      "decoder.decoders.modules_list.1.mixed_attention.attention_heads.1.K.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.1.mixed_attention.attention_heads.1.K.bias: 64 parameters\n",
      "decoder.decoders.modules_list.1.mixed_attention.attention_heads.1.Q.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.1.mixed_attention.attention_heads.1.Q.bias: 64 parameters\n",
      "decoder.decoders.modules_list.1.mixed_attention.attention_heads.1.V.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.1.mixed_attention.attention_heads.1.V.bias: 64 parameters\n",
      "decoder.decoders.modules_list.1.mixed_attention.attention_heads.2.K.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.1.mixed_attention.attention_heads.2.K.bias: 64 parameters\n",
      "decoder.decoders.modules_list.1.mixed_attention.attention_heads.2.Q.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.1.mixed_attention.attention_heads.2.Q.bias: 64 parameters\n",
      "decoder.decoders.modules_list.1.mixed_attention.attention_heads.2.V.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.1.mixed_attention.attention_heads.2.V.bias: 64 parameters\n",
      "decoder.decoders.modules_list.1.mixed_attention.attention_heads.3.K.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.1.mixed_attention.attention_heads.3.K.bias: 64 parameters\n",
      "decoder.decoders.modules_list.1.mixed_attention.attention_heads.3.Q.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.1.mixed_attention.attention_heads.3.Q.bias: 64 parameters\n",
      "decoder.decoders.modules_list.1.mixed_attention.attention_heads.3.V.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.1.mixed_attention.attention_heads.3.V.bias: 64 parameters\n",
      "decoder.decoders.modules_list.1.mixed_attention.attention_heads.4.K.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.1.mixed_attention.attention_heads.4.K.bias: 64 parameters\n",
      "decoder.decoders.modules_list.1.mixed_attention.attention_heads.4.Q.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.1.mixed_attention.attention_heads.4.Q.bias: 64 parameters\n",
      "decoder.decoders.modules_list.1.mixed_attention.attention_heads.4.V.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.1.mixed_attention.attention_heads.4.V.bias: 64 parameters\n",
      "decoder.decoders.modules_list.1.mixed_attention.attention_heads.5.K.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.1.mixed_attention.attention_heads.5.K.bias: 64 parameters\n",
      "decoder.decoders.modules_list.1.mixed_attention.attention_heads.5.Q.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.1.mixed_attention.attention_heads.5.Q.bias: 64 parameters\n",
      "decoder.decoders.modules_list.1.mixed_attention.attention_heads.5.V.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.1.mixed_attention.attention_heads.5.V.bias: 64 parameters\n",
      "decoder.decoders.modules_list.1.mixed_attention.attention_heads.6.K.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.1.mixed_attention.attention_heads.6.K.bias: 64 parameters\n",
      "decoder.decoders.modules_list.1.mixed_attention.attention_heads.6.Q.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.1.mixed_attention.attention_heads.6.Q.bias: 64 parameters\n",
      "decoder.decoders.modules_list.1.mixed_attention.attention_heads.6.V.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.1.mixed_attention.attention_heads.6.V.bias: 64 parameters\n",
      "decoder.decoders.modules_list.1.mixed_attention.attention_heads.7.K.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.1.mixed_attention.attention_heads.7.K.bias: 64 parameters\n",
      "decoder.decoders.modules_list.1.mixed_attention.attention_heads.7.Q.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.1.mixed_attention.attention_heads.7.Q.bias: 64 parameters\n",
      "decoder.decoders.modules_list.1.mixed_attention.attention_heads.7.V.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.1.mixed_attention.attention_heads.7.V.bias: 64 parameters\n",
      "decoder.decoders.modules_list.1.mixed_attention.WO.weight: 262144 parameters\n",
      "decoder.decoders.modules_list.1.mixed_attention.WO.bias: 512 parameters\n",
      "decoder.decoders.modules_list.1.layerNorm.weight: 512 parameters\n",
      "decoder.decoders.modules_list.1.layerNorm.bias: 512 parameters\n",
      "decoder.decoders.modules_list.1.ff.0.weight: 1048576 parameters\n",
      "decoder.decoders.modules_list.1.ff.0.bias: 2048 parameters\n",
      "decoder.decoders.modules_list.1.ff.2.weight: 1048576 parameters\n",
      "decoder.decoders.modules_list.1.ff.2.bias: 512 parameters\n",
      "decoder.decoders.modules_list.2.masked_attention.attention_heads.0.K.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.2.masked_attention.attention_heads.0.K.bias: 64 parameters\n",
      "decoder.decoders.modules_list.2.masked_attention.attention_heads.0.Q.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.2.masked_attention.attention_heads.0.Q.bias: 64 parameters\n",
      "decoder.decoders.modules_list.2.masked_attention.attention_heads.0.V.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.2.masked_attention.attention_heads.0.V.bias: 64 parameters\n",
      "decoder.decoders.modules_list.2.masked_attention.attention_heads.1.K.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.2.masked_attention.attention_heads.1.K.bias: 64 parameters\n",
      "decoder.decoders.modules_list.2.masked_attention.attention_heads.1.Q.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.2.masked_attention.attention_heads.1.Q.bias: 64 parameters\n",
      "decoder.decoders.modules_list.2.masked_attention.attention_heads.1.V.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.2.masked_attention.attention_heads.1.V.bias: 64 parameters\n",
      "decoder.decoders.modules_list.2.masked_attention.attention_heads.2.K.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.2.masked_attention.attention_heads.2.K.bias: 64 parameters\n",
      "decoder.decoders.modules_list.2.masked_attention.attention_heads.2.Q.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.2.masked_attention.attention_heads.2.Q.bias: 64 parameters\n",
      "decoder.decoders.modules_list.2.masked_attention.attention_heads.2.V.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.2.masked_attention.attention_heads.2.V.bias: 64 parameters\n",
      "decoder.decoders.modules_list.2.masked_attention.attention_heads.3.K.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.2.masked_attention.attention_heads.3.K.bias: 64 parameters\n",
      "decoder.decoders.modules_list.2.masked_attention.attention_heads.3.Q.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.2.masked_attention.attention_heads.3.Q.bias: 64 parameters\n",
      "decoder.decoders.modules_list.2.masked_attention.attention_heads.3.V.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.2.masked_attention.attention_heads.3.V.bias: 64 parameters\n",
      "decoder.decoders.modules_list.2.masked_attention.attention_heads.4.K.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.2.masked_attention.attention_heads.4.K.bias: 64 parameters\n",
      "decoder.decoders.modules_list.2.masked_attention.attention_heads.4.Q.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.2.masked_attention.attention_heads.4.Q.bias: 64 parameters\n",
      "decoder.decoders.modules_list.2.masked_attention.attention_heads.4.V.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.2.masked_attention.attention_heads.4.V.bias: 64 parameters\n",
      "decoder.decoders.modules_list.2.masked_attention.attention_heads.5.K.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.2.masked_attention.attention_heads.5.K.bias: 64 parameters\n",
      "decoder.decoders.modules_list.2.masked_attention.attention_heads.5.Q.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.2.masked_attention.attention_heads.5.Q.bias: 64 parameters\n",
      "decoder.decoders.modules_list.2.masked_attention.attention_heads.5.V.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.2.masked_attention.attention_heads.5.V.bias: 64 parameters\n",
      "decoder.decoders.modules_list.2.masked_attention.attention_heads.6.K.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.2.masked_attention.attention_heads.6.K.bias: 64 parameters\n",
      "decoder.decoders.modules_list.2.masked_attention.attention_heads.6.Q.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.2.masked_attention.attention_heads.6.Q.bias: 64 parameters\n",
      "decoder.decoders.modules_list.2.masked_attention.attention_heads.6.V.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.2.masked_attention.attention_heads.6.V.bias: 64 parameters\n",
      "decoder.decoders.modules_list.2.masked_attention.attention_heads.7.K.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.2.masked_attention.attention_heads.7.K.bias: 64 parameters\n",
      "decoder.decoders.modules_list.2.masked_attention.attention_heads.7.Q.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.2.masked_attention.attention_heads.7.Q.bias: 64 parameters\n",
      "decoder.decoders.modules_list.2.masked_attention.attention_heads.7.V.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.2.masked_attention.attention_heads.7.V.bias: 64 parameters\n",
      "decoder.decoders.modules_list.2.masked_attention.WO.weight: 262144 parameters\n",
      "decoder.decoders.modules_list.2.masked_attention.WO.bias: 512 parameters\n",
      "decoder.decoders.modules_list.2.mixed_attention.attention_heads.0.K.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.2.mixed_attention.attention_heads.0.K.bias: 64 parameters\n",
      "decoder.decoders.modules_list.2.mixed_attention.attention_heads.0.Q.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.2.mixed_attention.attention_heads.0.Q.bias: 64 parameters\n",
      "decoder.decoders.modules_list.2.mixed_attention.attention_heads.0.V.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.2.mixed_attention.attention_heads.0.V.bias: 64 parameters\n",
      "decoder.decoders.modules_list.2.mixed_attention.attention_heads.1.K.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.2.mixed_attention.attention_heads.1.K.bias: 64 parameters\n",
      "decoder.decoders.modules_list.2.mixed_attention.attention_heads.1.Q.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.2.mixed_attention.attention_heads.1.Q.bias: 64 parameters\n",
      "decoder.decoders.modules_list.2.mixed_attention.attention_heads.1.V.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.2.mixed_attention.attention_heads.1.V.bias: 64 parameters\n",
      "decoder.decoders.modules_list.2.mixed_attention.attention_heads.2.K.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.2.mixed_attention.attention_heads.2.K.bias: 64 parameters\n",
      "decoder.decoders.modules_list.2.mixed_attention.attention_heads.2.Q.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.2.mixed_attention.attention_heads.2.Q.bias: 64 parameters\n",
      "decoder.decoders.modules_list.2.mixed_attention.attention_heads.2.V.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.2.mixed_attention.attention_heads.2.V.bias: 64 parameters\n",
      "decoder.decoders.modules_list.2.mixed_attention.attention_heads.3.K.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.2.mixed_attention.attention_heads.3.K.bias: 64 parameters\n",
      "decoder.decoders.modules_list.2.mixed_attention.attention_heads.3.Q.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.2.mixed_attention.attention_heads.3.Q.bias: 64 parameters\n",
      "decoder.decoders.modules_list.2.mixed_attention.attention_heads.3.V.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.2.mixed_attention.attention_heads.3.V.bias: 64 parameters\n",
      "decoder.decoders.modules_list.2.mixed_attention.attention_heads.4.K.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.2.mixed_attention.attention_heads.4.K.bias: 64 parameters\n",
      "decoder.decoders.modules_list.2.mixed_attention.attention_heads.4.Q.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.2.mixed_attention.attention_heads.4.Q.bias: 64 parameters\n",
      "decoder.decoders.modules_list.2.mixed_attention.attention_heads.4.V.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.2.mixed_attention.attention_heads.4.V.bias: 64 parameters\n",
      "decoder.decoders.modules_list.2.mixed_attention.attention_heads.5.K.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.2.mixed_attention.attention_heads.5.K.bias: 64 parameters\n",
      "decoder.decoders.modules_list.2.mixed_attention.attention_heads.5.Q.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.2.mixed_attention.attention_heads.5.Q.bias: 64 parameters\n",
      "decoder.decoders.modules_list.2.mixed_attention.attention_heads.5.V.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.2.mixed_attention.attention_heads.5.V.bias: 64 parameters\n",
      "decoder.decoders.modules_list.2.mixed_attention.attention_heads.6.K.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.2.mixed_attention.attention_heads.6.K.bias: 64 parameters\n",
      "decoder.decoders.modules_list.2.mixed_attention.attention_heads.6.Q.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.2.mixed_attention.attention_heads.6.Q.bias: 64 parameters\n",
      "decoder.decoders.modules_list.2.mixed_attention.attention_heads.6.V.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.2.mixed_attention.attention_heads.6.V.bias: 64 parameters\n",
      "decoder.decoders.modules_list.2.mixed_attention.attention_heads.7.K.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.2.mixed_attention.attention_heads.7.K.bias: 64 parameters\n",
      "decoder.decoders.modules_list.2.mixed_attention.attention_heads.7.Q.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.2.mixed_attention.attention_heads.7.Q.bias: 64 parameters\n",
      "decoder.decoders.modules_list.2.mixed_attention.attention_heads.7.V.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.2.mixed_attention.attention_heads.7.V.bias: 64 parameters\n",
      "decoder.decoders.modules_list.2.mixed_attention.WO.weight: 262144 parameters\n",
      "decoder.decoders.modules_list.2.mixed_attention.WO.bias: 512 parameters\n",
      "decoder.decoders.modules_list.2.layerNorm.weight: 512 parameters\n",
      "decoder.decoders.modules_list.2.layerNorm.bias: 512 parameters\n",
      "decoder.decoders.modules_list.2.ff.0.weight: 1048576 parameters\n",
      "decoder.decoders.modules_list.2.ff.0.bias: 2048 parameters\n",
      "decoder.decoders.modules_list.2.ff.2.weight: 1048576 parameters\n",
      "decoder.decoders.modules_list.2.ff.2.bias: 512 parameters\n",
      "decoder.decoders.modules_list.3.masked_attention.attention_heads.0.K.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.3.masked_attention.attention_heads.0.K.bias: 64 parameters\n",
      "decoder.decoders.modules_list.3.masked_attention.attention_heads.0.Q.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.3.masked_attention.attention_heads.0.Q.bias: 64 parameters\n",
      "decoder.decoders.modules_list.3.masked_attention.attention_heads.0.V.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.3.masked_attention.attention_heads.0.V.bias: 64 parameters\n",
      "decoder.decoders.modules_list.3.masked_attention.attention_heads.1.K.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.3.masked_attention.attention_heads.1.K.bias: 64 parameters\n",
      "decoder.decoders.modules_list.3.masked_attention.attention_heads.1.Q.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.3.masked_attention.attention_heads.1.Q.bias: 64 parameters\n",
      "decoder.decoders.modules_list.3.masked_attention.attention_heads.1.V.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.3.masked_attention.attention_heads.1.V.bias: 64 parameters\n",
      "decoder.decoders.modules_list.3.masked_attention.attention_heads.2.K.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.3.masked_attention.attention_heads.2.K.bias: 64 parameters\n",
      "decoder.decoders.modules_list.3.masked_attention.attention_heads.2.Q.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.3.masked_attention.attention_heads.2.Q.bias: 64 parameters\n",
      "decoder.decoders.modules_list.3.masked_attention.attention_heads.2.V.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.3.masked_attention.attention_heads.2.V.bias: 64 parameters\n",
      "decoder.decoders.modules_list.3.masked_attention.attention_heads.3.K.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.3.masked_attention.attention_heads.3.K.bias: 64 parameters\n",
      "decoder.decoders.modules_list.3.masked_attention.attention_heads.3.Q.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.3.masked_attention.attention_heads.3.Q.bias: 64 parameters\n",
      "decoder.decoders.modules_list.3.masked_attention.attention_heads.3.V.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.3.masked_attention.attention_heads.3.V.bias: 64 parameters\n",
      "decoder.decoders.modules_list.3.masked_attention.attention_heads.4.K.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.3.masked_attention.attention_heads.4.K.bias: 64 parameters\n",
      "decoder.decoders.modules_list.3.masked_attention.attention_heads.4.Q.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.3.masked_attention.attention_heads.4.Q.bias: 64 parameters\n",
      "decoder.decoders.modules_list.3.masked_attention.attention_heads.4.V.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.3.masked_attention.attention_heads.4.V.bias: 64 parameters\n",
      "decoder.decoders.modules_list.3.masked_attention.attention_heads.5.K.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.3.masked_attention.attention_heads.5.K.bias: 64 parameters\n",
      "decoder.decoders.modules_list.3.masked_attention.attention_heads.5.Q.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.3.masked_attention.attention_heads.5.Q.bias: 64 parameters\n",
      "decoder.decoders.modules_list.3.masked_attention.attention_heads.5.V.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.3.masked_attention.attention_heads.5.V.bias: 64 parameters\n",
      "decoder.decoders.modules_list.3.masked_attention.attention_heads.6.K.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.3.masked_attention.attention_heads.6.K.bias: 64 parameters\n",
      "decoder.decoders.modules_list.3.masked_attention.attention_heads.6.Q.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.3.masked_attention.attention_heads.6.Q.bias: 64 parameters\n",
      "decoder.decoders.modules_list.3.masked_attention.attention_heads.6.V.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.3.masked_attention.attention_heads.6.V.bias: 64 parameters\n",
      "decoder.decoders.modules_list.3.masked_attention.attention_heads.7.K.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.3.masked_attention.attention_heads.7.K.bias: 64 parameters\n",
      "decoder.decoders.modules_list.3.masked_attention.attention_heads.7.Q.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.3.masked_attention.attention_heads.7.Q.bias: 64 parameters\n",
      "decoder.decoders.modules_list.3.masked_attention.attention_heads.7.V.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.3.masked_attention.attention_heads.7.V.bias: 64 parameters\n",
      "decoder.decoders.modules_list.3.masked_attention.WO.weight: 262144 parameters\n",
      "decoder.decoders.modules_list.3.masked_attention.WO.bias: 512 parameters\n",
      "decoder.decoders.modules_list.3.mixed_attention.attention_heads.0.K.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.3.mixed_attention.attention_heads.0.K.bias: 64 parameters\n",
      "decoder.decoders.modules_list.3.mixed_attention.attention_heads.0.Q.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.3.mixed_attention.attention_heads.0.Q.bias: 64 parameters\n",
      "decoder.decoders.modules_list.3.mixed_attention.attention_heads.0.V.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.3.mixed_attention.attention_heads.0.V.bias: 64 parameters\n",
      "decoder.decoders.modules_list.3.mixed_attention.attention_heads.1.K.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.3.mixed_attention.attention_heads.1.K.bias: 64 parameters\n",
      "decoder.decoders.modules_list.3.mixed_attention.attention_heads.1.Q.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.3.mixed_attention.attention_heads.1.Q.bias: 64 parameters\n",
      "decoder.decoders.modules_list.3.mixed_attention.attention_heads.1.V.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.3.mixed_attention.attention_heads.1.V.bias: 64 parameters\n",
      "decoder.decoders.modules_list.3.mixed_attention.attention_heads.2.K.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.3.mixed_attention.attention_heads.2.K.bias: 64 parameters\n",
      "decoder.decoders.modules_list.3.mixed_attention.attention_heads.2.Q.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.3.mixed_attention.attention_heads.2.Q.bias: 64 parameters\n",
      "decoder.decoders.modules_list.3.mixed_attention.attention_heads.2.V.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.3.mixed_attention.attention_heads.2.V.bias: 64 parameters\n",
      "decoder.decoders.modules_list.3.mixed_attention.attention_heads.3.K.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.3.mixed_attention.attention_heads.3.K.bias: 64 parameters\n",
      "decoder.decoders.modules_list.3.mixed_attention.attention_heads.3.Q.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.3.mixed_attention.attention_heads.3.Q.bias: 64 parameters\n",
      "decoder.decoders.modules_list.3.mixed_attention.attention_heads.3.V.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.3.mixed_attention.attention_heads.3.V.bias: 64 parameters\n",
      "decoder.decoders.modules_list.3.mixed_attention.attention_heads.4.K.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.3.mixed_attention.attention_heads.4.K.bias: 64 parameters\n",
      "decoder.decoders.modules_list.3.mixed_attention.attention_heads.4.Q.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.3.mixed_attention.attention_heads.4.Q.bias: 64 parameters\n",
      "decoder.decoders.modules_list.3.mixed_attention.attention_heads.4.V.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.3.mixed_attention.attention_heads.4.V.bias: 64 parameters\n",
      "decoder.decoders.modules_list.3.mixed_attention.attention_heads.5.K.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.3.mixed_attention.attention_heads.5.K.bias: 64 parameters\n",
      "decoder.decoders.modules_list.3.mixed_attention.attention_heads.5.Q.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.3.mixed_attention.attention_heads.5.Q.bias: 64 parameters\n",
      "decoder.decoders.modules_list.3.mixed_attention.attention_heads.5.V.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.3.mixed_attention.attention_heads.5.V.bias: 64 parameters\n",
      "decoder.decoders.modules_list.3.mixed_attention.attention_heads.6.K.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.3.mixed_attention.attention_heads.6.K.bias: 64 parameters\n",
      "decoder.decoders.modules_list.3.mixed_attention.attention_heads.6.Q.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.3.mixed_attention.attention_heads.6.Q.bias: 64 parameters\n",
      "decoder.decoders.modules_list.3.mixed_attention.attention_heads.6.V.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.3.mixed_attention.attention_heads.6.V.bias: 64 parameters\n",
      "decoder.decoders.modules_list.3.mixed_attention.attention_heads.7.K.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.3.mixed_attention.attention_heads.7.K.bias: 64 parameters\n",
      "decoder.decoders.modules_list.3.mixed_attention.attention_heads.7.Q.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.3.mixed_attention.attention_heads.7.Q.bias: 64 parameters\n",
      "decoder.decoders.modules_list.3.mixed_attention.attention_heads.7.V.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.3.mixed_attention.attention_heads.7.V.bias: 64 parameters\n",
      "decoder.decoders.modules_list.3.mixed_attention.WO.weight: 262144 parameters\n",
      "decoder.decoders.modules_list.3.mixed_attention.WO.bias: 512 parameters\n",
      "decoder.decoders.modules_list.3.layerNorm.weight: 512 parameters\n",
      "decoder.decoders.modules_list.3.layerNorm.bias: 512 parameters\n",
      "decoder.decoders.modules_list.3.ff.0.weight: 1048576 parameters\n",
      "decoder.decoders.modules_list.3.ff.0.bias: 2048 parameters\n",
      "decoder.decoders.modules_list.3.ff.2.weight: 1048576 parameters\n",
      "decoder.decoders.modules_list.3.ff.2.bias: 512 parameters\n",
      "decoder.decoders.modules_list.4.masked_attention.attention_heads.0.K.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.4.masked_attention.attention_heads.0.K.bias: 64 parameters\n",
      "decoder.decoders.modules_list.4.masked_attention.attention_heads.0.Q.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.4.masked_attention.attention_heads.0.Q.bias: 64 parameters\n",
      "decoder.decoders.modules_list.4.masked_attention.attention_heads.0.V.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.4.masked_attention.attention_heads.0.V.bias: 64 parameters\n",
      "decoder.decoders.modules_list.4.masked_attention.attention_heads.1.K.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.4.masked_attention.attention_heads.1.K.bias: 64 parameters\n",
      "decoder.decoders.modules_list.4.masked_attention.attention_heads.1.Q.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.4.masked_attention.attention_heads.1.Q.bias: 64 parameters\n",
      "decoder.decoders.modules_list.4.masked_attention.attention_heads.1.V.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.4.masked_attention.attention_heads.1.V.bias: 64 parameters\n",
      "decoder.decoders.modules_list.4.masked_attention.attention_heads.2.K.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.4.masked_attention.attention_heads.2.K.bias: 64 parameters\n",
      "decoder.decoders.modules_list.4.masked_attention.attention_heads.2.Q.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.4.masked_attention.attention_heads.2.Q.bias: 64 parameters\n",
      "decoder.decoders.modules_list.4.masked_attention.attention_heads.2.V.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.4.masked_attention.attention_heads.2.V.bias: 64 parameters\n",
      "decoder.decoders.modules_list.4.masked_attention.attention_heads.3.K.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.4.masked_attention.attention_heads.3.K.bias: 64 parameters\n",
      "decoder.decoders.modules_list.4.masked_attention.attention_heads.3.Q.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.4.masked_attention.attention_heads.3.Q.bias: 64 parameters\n",
      "decoder.decoders.modules_list.4.masked_attention.attention_heads.3.V.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.4.masked_attention.attention_heads.3.V.bias: 64 parameters\n",
      "decoder.decoders.modules_list.4.masked_attention.attention_heads.4.K.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.4.masked_attention.attention_heads.4.K.bias: 64 parameters\n",
      "decoder.decoders.modules_list.4.masked_attention.attention_heads.4.Q.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.4.masked_attention.attention_heads.4.Q.bias: 64 parameters\n",
      "decoder.decoders.modules_list.4.masked_attention.attention_heads.4.V.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.4.masked_attention.attention_heads.4.V.bias: 64 parameters\n",
      "decoder.decoders.modules_list.4.masked_attention.attention_heads.5.K.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.4.masked_attention.attention_heads.5.K.bias: 64 parameters\n",
      "decoder.decoders.modules_list.4.masked_attention.attention_heads.5.Q.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.4.masked_attention.attention_heads.5.Q.bias: 64 parameters\n",
      "decoder.decoders.modules_list.4.masked_attention.attention_heads.5.V.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.4.masked_attention.attention_heads.5.V.bias: 64 parameters\n",
      "decoder.decoders.modules_list.4.masked_attention.attention_heads.6.K.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.4.masked_attention.attention_heads.6.K.bias: 64 parameters\n",
      "decoder.decoders.modules_list.4.masked_attention.attention_heads.6.Q.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.4.masked_attention.attention_heads.6.Q.bias: 64 parameters\n",
      "decoder.decoders.modules_list.4.masked_attention.attention_heads.6.V.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.4.masked_attention.attention_heads.6.V.bias: 64 parameters\n",
      "decoder.decoders.modules_list.4.masked_attention.attention_heads.7.K.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.4.masked_attention.attention_heads.7.K.bias: 64 parameters\n",
      "decoder.decoders.modules_list.4.masked_attention.attention_heads.7.Q.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.4.masked_attention.attention_heads.7.Q.bias: 64 parameters\n",
      "decoder.decoders.modules_list.4.masked_attention.attention_heads.7.V.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.4.masked_attention.attention_heads.7.V.bias: 64 parameters\n",
      "decoder.decoders.modules_list.4.masked_attention.WO.weight: 262144 parameters\n",
      "decoder.decoders.modules_list.4.masked_attention.WO.bias: 512 parameters\n",
      "decoder.decoders.modules_list.4.mixed_attention.attention_heads.0.K.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.4.mixed_attention.attention_heads.0.K.bias: 64 parameters\n",
      "decoder.decoders.modules_list.4.mixed_attention.attention_heads.0.Q.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.4.mixed_attention.attention_heads.0.Q.bias: 64 parameters\n",
      "decoder.decoders.modules_list.4.mixed_attention.attention_heads.0.V.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.4.mixed_attention.attention_heads.0.V.bias: 64 parameters\n",
      "decoder.decoders.modules_list.4.mixed_attention.attention_heads.1.K.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.4.mixed_attention.attention_heads.1.K.bias: 64 parameters\n",
      "decoder.decoders.modules_list.4.mixed_attention.attention_heads.1.Q.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.4.mixed_attention.attention_heads.1.Q.bias: 64 parameters\n",
      "decoder.decoders.modules_list.4.mixed_attention.attention_heads.1.V.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.4.mixed_attention.attention_heads.1.V.bias: 64 parameters\n",
      "decoder.decoders.modules_list.4.mixed_attention.attention_heads.2.K.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.4.mixed_attention.attention_heads.2.K.bias: 64 parameters\n",
      "decoder.decoders.modules_list.4.mixed_attention.attention_heads.2.Q.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.4.mixed_attention.attention_heads.2.Q.bias: 64 parameters\n",
      "decoder.decoders.modules_list.4.mixed_attention.attention_heads.2.V.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.4.mixed_attention.attention_heads.2.V.bias: 64 parameters\n",
      "decoder.decoders.modules_list.4.mixed_attention.attention_heads.3.K.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.4.mixed_attention.attention_heads.3.K.bias: 64 parameters\n",
      "decoder.decoders.modules_list.4.mixed_attention.attention_heads.3.Q.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.4.mixed_attention.attention_heads.3.Q.bias: 64 parameters\n",
      "decoder.decoders.modules_list.4.mixed_attention.attention_heads.3.V.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.4.mixed_attention.attention_heads.3.V.bias: 64 parameters\n",
      "decoder.decoders.modules_list.4.mixed_attention.attention_heads.4.K.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.4.mixed_attention.attention_heads.4.K.bias: 64 parameters\n",
      "decoder.decoders.modules_list.4.mixed_attention.attention_heads.4.Q.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.4.mixed_attention.attention_heads.4.Q.bias: 64 parameters\n",
      "decoder.decoders.modules_list.4.mixed_attention.attention_heads.4.V.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.4.mixed_attention.attention_heads.4.V.bias: 64 parameters\n",
      "decoder.decoders.modules_list.4.mixed_attention.attention_heads.5.K.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.4.mixed_attention.attention_heads.5.K.bias: 64 parameters\n",
      "decoder.decoders.modules_list.4.mixed_attention.attention_heads.5.Q.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.4.mixed_attention.attention_heads.5.Q.bias: 64 parameters\n",
      "decoder.decoders.modules_list.4.mixed_attention.attention_heads.5.V.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.4.mixed_attention.attention_heads.5.V.bias: 64 parameters\n",
      "decoder.decoders.modules_list.4.mixed_attention.attention_heads.6.K.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.4.mixed_attention.attention_heads.6.K.bias: 64 parameters\n",
      "decoder.decoders.modules_list.4.mixed_attention.attention_heads.6.Q.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.4.mixed_attention.attention_heads.6.Q.bias: 64 parameters\n",
      "decoder.decoders.modules_list.4.mixed_attention.attention_heads.6.V.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.4.mixed_attention.attention_heads.6.V.bias: 64 parameters\n",
      "decoder.decoders.modules_list.4.mixed_attention.attention_heads.7.K.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.4.mixed_attention.attention_heads.7.K.bias: 64 parameters\n",
      "decoder.decoders.modules_list.4.mixed_attention.attention_heads.7.Q.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.4.mixed_attention.attention_heads.7.Q.bias: 64 parameters\n",
      "decoder.decoders.modules_list.4.mixed_attention.attention_heads.7.V.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.4.mixed_attention.attention_heads.7.V.bias: 64 parameters\n",
      "decoder.decoders.modules_list.4.mixed_attention.WO.weight: 262144 parameters\n",
      "decoder.decoders.modules_list.4.mixed_attention.WO.bias: 512 parameters\n",
      "decoder.decoders.modules_list.4.layerNorm.weight: 512 parameters\n",
      "decoder.decoders.modules_list.4.layerNorm.bias: 512 parameters\n",
      "decoder.decoders.modules_list.4.ff.0.weight: 1048576 parameters\n",
      "decoder.decoders.modules_list.4.ff.0.bias: 2048 parameters\n",
      "decoder.decoders.modules_list.4.ff.2.weight: 1048576 parameters\n",
      "decoder.decoders.modules_list.4.ff.2.bias: 512 parameters\n",
      "decoder.decoders.modules_list.5.masked_attention.attention_heads.0.K.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.5.masked_attention.attention_heads.0.K.bias: 64 parameters\n",
      "decoder.decoders.modules_list.5.masked_attention.attention_heads.0.Q.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.5.masked_attention.attention_heads.0.Q.bias: 64 parameters\n",
      "decoder.decoders.modules_list.5.masked_attention.attention_heads.0.V.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.5.masked_attention.attention_heads.0.V.bias: 64 parameters\n",
      "decoder.decoders.modules_list.5.masked_attention.attention_heads.1.K.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.5.masked_attention.attention_heads.1.K.bias: 64 parameters\n",
      "decoder.decoders.modules_list.5.masked_attention.attention_heads.1.Q.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.5.masked_attention.attention_heads.1.Q.bias: 64 parameters\n",
      "decoder.decoders.modules_list.5.masked_attention.attention_heads.1.V.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.5.masked_attention.attention_heads.1.V.bias: 64 parameters\n",
      "decoder.decoders.modules_list.5.masked_attention.attention_heads.2.K.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.5.masked_attention.attention_heads.2.K.bias: 64 parameters\n",
      "decoder.decoders.modules_list.5.masked_attention.attention_heads.2.Q.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.5.masked_attention.attention_heads.2.Q.bias: 64 parameters\n",
      "decoder.decoders.modules_list.5.masked_attention.attention_heads.2.V.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.5.masked_attention.attention_heads.2.V.bias: 64 parameters\n",
      "decoder.decoders.modules_list.5.masked_attention.attention_heads.3.K.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.5.masked_attention.attention_heads.3.K.bias: 64 parameters\n",
      "decoder.decoders.modules_list.5.masked_attention.attention_heads.3.Q.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.5.masked_attention.attention_heads.3.Q.bias: 64 parameters\n",
      "decoder.decoders.modules_list.5.masked_attention.attention_heads.3.V.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.5.masked_attention.attention_heads.3.V.bias: 64 parameters\n",
      "decoder.decoders.modules_list.5.masked_attention.attention_heads.4.K.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.5.masked_attention.attention_heads.4.K.bias: 64 parameters\n",
      "decoder.decoders.modules_list.5.masked_attention.attention_heads.4.Q.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.5.masked_attention.attention_heads.4.Q.bias: 64 parameters\n",
      "decoder.decoders.modules_list.5.masked_attention.attention_heads.4.V.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.5.masked_attention.attention_heads.4.V.bias: 64 parameters\n",
      "decoder.decoders.modules_list.5.masked_attention.attention_heads.5.K.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.5.masked_attention.attention_heads.5.K.bias: 64 parameters\n",
      "decoder.decoders.modules_list.5.masked_attention.attention_heads.5.Q.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.5.masked_attention.attention_heads.5.Q.bias: 64 parameters\n",
      "decoder.decoders.modules_list.5.masked_attention.attention_heads.5.V.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.5.masked_attention.attention_heads.5.V.bias: 64 parameters\n",
      "decoder.decoders.modules_list.5.masked_attention.attention_heads.6.K.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.5.masked_attention.attention_heads.6.K.bias: 64 parameters\n",
      "decoder.decoders.modules_list.5.masked_attention.attention_heads.6.Q.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.5.masked_attention.attention_heads.6.Q.bias: 64 parameters\n",
      "decoder.decoders.modules_list.5.masked_attention.attention_heads.6.V.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.5.masked_attention.attention_heads.6.V.bias: 64 parameters\n",
      "decoder.decoders.modules_list.5.masked_attention.attention_heads.7.K.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.5.masked_attention.attention_heads.7.K.bias: 64 parameters\n",
      "decoder.decoders.modules_list.5.masked_attention.attention_heads.7.Q.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.5.masked_attention.attention_heads.7.Q.bias: 64 parameters\n",
      "decoder.decoders.modules_list.5.masked_attention.attention_heads.7.V.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.5.masked_attention.attention_heads.7.V.bias: 64 parameters\n",
      "decoder.decoders.modules_list.5.masked_attention.WO.weight: 262144 parameters\n",
      "decoder.decoders.modules_list.5.masked_attention.WO.bias: 512 parameters\n",
      "decoder.decoders.modules_list.5.mixed_attention.attention_heads.0.K.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.5.mixed_attention.attention_heads.0.K.bias: 64 parameters\n",
      "decoder.decoders.modules_list.5.mixed_attention.attention_heads.0.Q.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.5.mixed_attention.attention_heads.0.Q.bias: 64 parameters\n",
      "decoder.decoders.modules_list.5.mixed_attention.attention_heads.0.V.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.5.mixed_attention.attention_heads.0.V.bias: 64 parameters\n",
      "decoder.decoders.modules_list.5.mixed_attention.attention_heads.1.K.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.5.mixed_attention.attention_heads.1.K.bias: 64 parameters\n",
      "decoder.decoders.modules_list.5.mixed_attention.attention_heads.1.Q.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.5.mixed_attention.attention_heads.1.Q.bias: 64 parameters\n",
      "decoder.decoders.modules_list.5.mixed_attention.attention_heads.1.V.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.5.mixed_attention.attention_heads.1.V.bias: 64 parameters\n",
      "decoder.decoders.modules_list.5.mixed_attention.attention_heads.2.K.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.5.mixed_attention.attention_heads.2.K.bias: 64 parameters\n",
      "decoder.decoders.modules_list.5.mixed_attention.attention_heads.2.Q.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.5.mixed_attention.attention_heads.2.Q.bias: 64 parameters\n",
      "decoder.decoders.modules_list.5.mixed_attention.attention_heads.2.V.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.5.mixed_attention.attention_heads.2.V.bias: 64 parameters\n",
      "decoder.decoders.modules_list.5.mixed_attention.attention_heads.3.K.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.5.mixed_attention.attention_heads.3.K.bias: 64 parameters\n",
      "decoder.decoders.modules_list.5.mixed_attention.attention_heads.3.Q.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.5.mixed_attention.attention_heads.3.Q.bias: 64 parameters\n",
      "decoder.decoders.modules_list.5.mixed_attention.attention_heads.3.V.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.5.mixed_attention.attention_heads.3.V.bias: 64 parameters\n",
      "decoder.decoders.modules_list.5.mixed_attention.attention_heads.4.K.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.5.mixed_attention.attention_heads.4.K.bias: 64 parameters\n",
      "decoder.decoders.modules_list.5.mixed_attention.attention_heads.4.Q.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.5.mixed_attention.attention_heads.4.Q.bias: 64 parameters\n",
      "decoder.decoders.modules_list.5.mixed_attention.attention_heads.4.V.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.5.mixed_attention.attention_heads.4.V.bias: 64 parameters\n",
      "decoder.decoders.modules_list.5.mixed_attention.attention_heads.5.K.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.5.mixed_attention.attention_heads.5.K.bias: 64 parameters\n",
      "decoder.decoders.modules_list.5.mixed_attention.attention_heads.5.Q.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.5.mixed_attention.attention_heads.5.Q.bias: 64 parameters\n",
      "decoder.decoders.modules_list.5.mixed_attention.attention_heads.5.V.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.5.mixed_attention.attention_heads.5.V.bias: 64 parameters\n",
      "decoder.decoders.modules_list.5.mixed_attention.attention_heads.6.K.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.5.mixed_attention.attention_heads.6.K.bias: 64 parameters\n",
      "decoder.decoders.modules_list.5.mixed_attention.attention_heads.6.Q.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.5.mixed_attention.attention_heads.6.Q.bias: 64 parameters\n",
      "decoder.decoders.modules_list.5.mixed_attention.attention_heads.6.V.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.5.mixed_attention.attention_heads.6.V.bias: 64 parameters\n",
      "decoder.decoders.modules_list.5.mixed_attention.attention_heads.7.K.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.5.mixed_attention.attention_heads.7.K.bias: 64 parameters\n",
      "decoder.decoders.modules_list.5.mixed_attention.attention_heads.7.Q.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.5.mixed_attention.attention_heads.7.Q.bias: 64 parameters\n",
      "decoder.decoders.modules_list.5.mixed_attention.attention_heads.7.V.weight: 32768 parameters\n",
      "decoder.decoders.modules_list.5.mixed_attention.attention_heads.7.V.bias: 64 parameters\n",
      "decoder.decoders.modules_list.5.mixed_attention.WO.weight: 262144 parameters\n",
      "decoder.decoders.modules_list.5.mixed_attention.WO.bias: 512 parameters\n",
      "decoder.decoders.modules_list.5.layerNorm.weight: 512 parameters\n",
      "decoder.decoders.modules_list.5.layerNorm.bias: 512 parameters\n",
      "decoder.decoders.modules_list.5.ff.0.weight: 1048576 parameters\n",
      "decoder.decoders.modules_list.5.ff.0.bias: 2048 parameters\n",
      "decoder.decoders.modules_list.5.ff.2.weight: 1048576 parameters\n",
      "decoder.decoders.modules_list.5.ff.2.bias: 512 parameters\n",
      "linear.weight: 16384000 parameters\n",
      "linear.bias: 32000 parameters\n",
      "embedding.embedding.weight: 16384000 parameters\n",
      "Total number of trainable parameters: 76920064\n",
      "Total number parameters: 76920064\n"
     ]
    }
   ],
   "source": [
    "def print_model_parameters(model):\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.requires_grad:\n",
    "            print(f'{name}: {param.numel()} parameters')\n",
    "\n",
    "print_model_parameters(MyTransformer)\n",
    "# \n",
    "# Calculate the total number of trainable parameters\n",
    "total_params = sum(p.numel() for p in MyTransformer.parameters() if p.requires_grad)\n",
    "total_params_no_grad = sum(p.numel() for p in MyTransformer.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'Total number of trainable parameters: {total_params_no_grad}')\n",
    "print(f'Total number parameters: {total_params}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output (Dot Product):\n",
      "tensor([[[ 0.8135, -0.4580,  0.8029,  0.1411],\n",
      "         [-0.3034, -0.2345,  0.2389,  0.1042],\n",
      "         [ 0.6104, -0.1512,  0.4783, -0.0657]],\n",
      "\n",
      "        [[-0.2787,  0.9954,  0.8464, -0.2801],\n",
      "         [-0.2337,  1.0036,  0.7845, -0.1865],\n",
      "         [-0.2645,  0.9221,  0.8332, -0.2380]]])\n",
      "\n",
      "Output (Element-wise):\n",
      "tensor([[[ 0.8075, -0.5717,  0.8141,  0.2876],\n",
      "         [ 0.2791, -0.0066,  0.1415, -0.0585],\n",
      "         [-0.3046, -0.0824,  0.0058,  0.0681]],\n",
      "\n",
      "        [[ 0.0791,  0.2676,  0.0383,  0.2675],\n",
      "         [-0.3081,  0.4092,  0.7134, -0.4355],\n",
      "         [-0.0153,  0.1695,  0.0602, -0.0146]]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Define dimensions\n",
    "batch_size = 2\n",
    "seq_length = 3\n",
    "model_dim = 4\n",
    "dk = 4\n",
    "dv = 4\n",
    "\n",
    "# Generate random matrices\n",
    "Qx = torch.randn(batch_size, seq_length, model_dim)\n",
    "Kx = torch.randn(batch_size, seq_length, model_dim)\n",
    "Vx = torch.randn(batch_size, seq_length, dv)\n",
    "\n",
    "# Implementation 1: Dot product attention\n",
    "QK_dot_product = torch.matmul(Qx, Kx.transpose(-2, -1)) / np.sqrt(dk)\n",
    "QK_dot_product = torch.softmax(QK_dot_product, dim=-1)\n",
    "output_dot_product = torch.matmul(QK_dot_product, Vx)\n",
    "\n",
    "# Implementation 2: Element-wise multiplication and summation\n",
    "QK_elementwise = torch.sum(Kx * Qx, dim=-1) / np.sqrt(dk)\n",
    "QK_elementwise = torch.softmax(QK_elementwise, dim=-1)\n",
    "QK_elementwise = QK_elementwise.unsqueeze(-1)\n",
    "QK_elementwise = QK_elementwise.expand(-1, -1, dv)\n",
    "output_elementwise = QK_elementwise * Vx\n",
    "\n",
    "# Print results\n",
    "print(\"Output (Dot Product):\")\n",
    "print(output_dot_product)\n",
    "print(\"\\nOutput (Element-wise):\")\n",
    "print(output_elementwise)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
